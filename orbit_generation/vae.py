"""Scripts to use Variational Autoencoders"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_vae.ipynb.

# %% auto 0
__all__ = ['BetaVAE']

# %% ../nbs/10_vae.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torchmetrics import MeanMetric

# %% ../nbs/10_vae.ipynb 3
class BetaVAE(pl.LightningModule):
    """
    beta-VAE implementation for unlabeled time series using PyTorch Lightning.
    """
    def __init__(self, encoder, decoder, beta=1.0, **kwargs):
        super().__init__(**kwargs)
        self.beta = beta
        self.encoder = encoder
        self.decoder = decoder
        self.total_loss_tracker = MeanMetric()
        self.reconstruction_loss_tracker = MeanMetric()
        self.kl_loss_tracker = MeanMetric()

    def forward(self, x):
        z_mean, z_log_var = self.encoder(x)
        z = self._reparameterize(z_mean, z_log_var)
        return self.decoder(z)

    def _reparameterize(self, mean, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mean + eps * std

    def training_step(self, batch):
        x = batch
        z_mean, z_log_var = self.encoder(x)
        z = self._reparameterize(z_mean, z_log_var)
        x_hat = self.decoder(z)
        reconst_loss = F.mse_loss(x_hat, x, reduction='mean')
        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1).mean()
        total_loss = reconst_loss + self.beta * kl_loss
        self.total_loss_tracker.update(total_loss)
        self.reconstruction_loss_tracker.update(reconst_loss)
        self.kl_loss_tracker.update(kl_loss)
        self.log_dict({"train_loss": total_loss, "recon_loss": reconst_loss, "kl_loss": kl_loss})
        return total_loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

    def generate(self, n=1):
        z = torch.randn(n, self.latent_dim, device=self.device)
        return self.decoder(z)

