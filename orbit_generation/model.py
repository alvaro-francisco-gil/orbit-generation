# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_model.ipynb.

# %% auto 0
__all__ = ['get_model', 'get_optimizer', 'extract_plot_and_return_metrics']

# %% ../nbs/06_model.ipynb 2
# from tensorflow.keras.optimizers import Adam, SGD

from tensorflow import keras
import tensorflow as tf
import tsgm.models
import matplotlib.pyplot as plt

# %% ../nbs/06_model.ipynb 5
def get_model(params):
    model_name = params['model_name']

    if model_name == 'vae_conv5':
        # Accessing model configuration from the zoo using parameters from the dictionary
        architecture = tsgm.models.zoo[model_name](
            seq_len=params['seq_len'], 
            feat_dim=params['feature_dim'], 
            latent_dim=params['latent_dim']
        )

        # Extracting encoder and decoder from the architecture
        encoder, decoder = architecture.encoder, architecture.decoder

        # Build the VAE
        # vae = BetaVAE(encoder, decoder)
        vae = tsgm.models.cvae.BetaVAE(encoder, decoder)
        return vae

    elif model_name == 'timeGAN':
        model = tsgm.models.timeGAN.TimeGAN(
            seq_len=params['seq_len'],
            module="gru",
            hidden_dim=24,
            n_features=params['feature_dim'],
            n_layers=3,
            batch_size=params['batch_size'],
            gamma=1.0,
        )
        # .compile() sets all optimizers to Adam by default
        model.compile(optimizer=params['optimizer']['name'], learning_rate=params['optimizer']['learning_rate'])
        return model

    else:
        raise ValueError(f"Unsupported model_name: {model_name}")

# %% ../nbs/06_model.ipynb 6
def get_optimizer(optimizer_config):
    name = optimizer_config['name'].lower()
    if name == 'adam':
        return keras.optimizers.Adam(learning_rate=optimizer_config.get('learning_rate', 0.001))
    elif name == 'sgd':
        return keras.optimizers.SGD(learning_rate=optimizer_config.get('learning_rate', 0.01))
    # Add additional optimizers as needed
    raise ValueError("Unsupported optimizer: {}".format(optimizer_config['name']))

# %% ../nbs/06_model.ipynb 7
def extract_plot_and_return_metrics(history, validation=True):
    """
    Extracts the metrics from the training history, plots the training and validation loss over epochs if validation is True, and returns the metrics.
    
    Parameters:
    - history: History object returned by model.fit().
    - validation: Boolean flag to control whether to extract and plot validation metrics.
    
    Returns:
    - metrics: Dictionary containing the final training and validation metrics.
    """
    metrics = {}

    # Extract training metrics from the training history and add to the metrics dictionary
    metrics['loss'] = history.history['loss'][-1]
    metrics['reconstruction_loss'] = history.history['reconstruction_loss'][-1]
    metrics['kl_loss'] = history.history['kl_loss'][-1]
    
    if validation:
        if 'val_loss' in history.history:
            metrics['val_loss'] = history.history['val_loss'][-1]
        if 'val_reconstruction_loss' in history.history:
            metrics['val_reconstruction_loss'] = history.history['val_reconstruction_loss'][-1]
        if 'val_kl_loss' in history.history:
            metrics['val_kl_loss'] = history.history['val_kl_loss'][-1]

    # Get the number of epochs
    num_epochs = len(history.history['loss'])

    # Plot metrics
    epochs = range(1, num_epochs + 1)
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, history.history['loss'], label='Training loss')
    if validation:
        plt.plot(epochs, history.history['val_loss'], label='Validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    
    return metrics

