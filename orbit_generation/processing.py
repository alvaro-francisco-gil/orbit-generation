"""Necessary scripts to read orbits from different formats"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_processing.ipynb.

# %% auto 0
__all__ = ['downsample_3d_array', 'resample_3d_array', 'average_downsample_3d_array', 'reorder_orbits', 'pad_and_convert_to_3d',
           'segment_and_convert_to_3d', 'add_time_vector_to_orbits']

# %% ../nbs/02_processing.ipynb 2
from scipy.interpolate import interp1d
import numpy as np
from typing import Tuple, Any, List, Dict
from scipy.stats import kendalltau

# %% ../nbs/02_processing.ipynb 6
def downsample_3d_array(data: np.ndarray,     # The original 3D array to be downsampled.
                        axis: int,            # The axis along which to perform the downsampling.
                        hop: int = None,      # The interval at which to keep elements.
                        target_size: int = None  # The target size for the specified axis.
                       ) -> np.ndarray:
    """
    Downsample a 3D numpy array along a specified axis by keeping only every hop-th element or 
    to a target size.
    """
    if axis not in [0, 1, 2]:  # Validate the axis to ensure it's within the correct range.
        raise ValueError("Invalid axis. Axis must be 0, 1, or 2.")

    if hop is not None and hop < 1:
        raise ValueError("Hop must be a positive integer greater than or equal to 1.")

    if target_size is not None and (target_size < 1 or target_size > data.shape[axis]):
        raise ValueError("Target size must be a positive integer and less than or equal to the size of the axis.")
    
    if hop is not None:
        # Create slices for each axis
        slices = [slice(None)] * 3
        slices[axis] = slice(None, None, hop)
        # Use the slices to downsample the array
        downsampled_data = data[tuple(slices)]
    
    elif target_size is not None:
        # Calculate the hop based on the target size
        original_size = data.shape[axis]
        hop = max(original_size // target_size, 1)
        slices = [slice(None)] * 3
        slices[axis] = slice(None, None, hop)
        downsampled_data = data[tuple(slices)]
        # Adjust if the resulting size does not match the target size due to rounding
        if downsampled_data.shape[axis] != target_size:
            indices = np.round(np.linspace(0, downsampled_data.shape[axis] - 1, target_size)).astype(int)
            downsampled_data = np.take(downsampled_data, indices, axis=axis)
    
    else:
        raise ValueError("Either hop or target_size must be specified.")
    
    return downsampled_data

# %% ../nbs/02_processing.ipynb 9
def resample_3d_array(data: np.ndarray,  # The original 3D array to be resampled.
                      axis: int,         # The axis along which to perform the interpolation.
                      target_size: int   # The new size of the axis after resampling.
                     ) -> np.ndarray:
    """
    Resample a 3D numpy array along a specified axis using linear interpolation.
    """
    if axis not in [0, 1, 2]:  # Validate the axis to ensure it's within the correct range.
        raise ValueError("Invalid axis. Axis must be 0, 1, or 2.")

    old_indices = np.linspace(0, 1, num=data.shape[axis])  # Calculate old indices for interpolation.
    new_indices = np.linspace(0, 1, num=target_size)       # New indices for the target size.

    new_shape = list(data.shape)  # Define the shape of the new data array.
    new_shape[axis] = target_size
    new_data = np.empty(new_shape, dtype=data.dtype)
    
    # Perform interpolation for each slice of the array along the specified axis.
    if axis == 0:
        for i in range(data.shape[1]):
            for j in range(data.shape[2]):
                interpolator = interp1d(old_indices, data[:, i, j], kind='linear')
                new_data[:, i, j] = interpolator(new_indices)
    elif axis == 1:
        for i in range(data.shape[0]):
            for j in range(data.shape[2]):
                interpolator = interp1d(old_indices, data[i, :, j], kind='linear')
                new_data[i, :, j] = interpolator(new_indices)
    else:  # axis == 2
        for i in range(data.shape[0]):
            for j in range(data.shape[1]):
                interpolator = interp1d(old_indices, data[i, j, :], kind='linear')
                new_data[i, j, :] = interpolator(new_indices)

    return new_data

# %% ../nbs/02_processing.ipynb 13
def average_downsample_3d_array(data: np.ndarray,  # The original 3D array to be downsampled.
                                axis: int,         # The axis along which to perform the downsampling (0, 1, or 2).
                                target_size: int   # The desired size of the specified axis after downsampling.
                               ) -> np.ndarray:
    """
    Downsample a 3D numpy array along a specified axis using averaging.
    """
    # Validate the axis to ensure it's within the correct range.
    if axis not in [0, 1, 2]:
        raise ValueError("Invalid axis. Axis must be 0, 1, or 2.")

    # Calculate the number of elements in each block that will be averaged.
    original_size = data.shape[axis]
    block_size = original_size / target_size

    # Define the shape of the new, downsampled data array.
    new_shape = list(data.shape)
    new_shape[axis] = target_size
    new_data = np.empty(new_shape, dtype=data.dtype)

    # Perform averaging along the specified axis.
    if axis == 0:
        for i in range(target_size):
            start_idx = int(i * block_size)
            end_idx = int((i + 1) * block_size)
            new_data[i, :, :] = np.mean(data[start_idx:end_idx, :, :], axis=0)  # Average blocks along the 0th axis.
    elif axis == 1:
        for i in range(target_size):
            start_idx = int(i * block_size)
            end_idx = int((i + 1) * block_size)
            new_data[:, i, :] = np.mean(data[:, start_idx:end_idx, :], axis=1)  # Average blocks along the 1st axis.
    else:  # axis == 2
        for i in range(target_size):
            start_idx = int(i * block_size)
            end_idx = int((i + 1) * block_size)
            new_data[:, :, i] = np.mean(data[:, :, start_idx:end_idx], axis=2)  # Average blocks along the 2nd axis.

    return new_data

# %% ../nbs/02_processing.ipynb 16
def reorder_orbits(orbit_dataset: np.ndarray
                  ) -> Tuple[np.ndarray,      # 3D numpy array of reordered orbits.
                             np.ndarray,        # 2D numpy array of metric values.
                             List[str]]:        # List of metric labels.
    """
    Reorders the time steps of each orbit in the dataset such that the time values are always incrementally increasing.
    Returns the reordered dataset, a 2D array of metric values for each orbit, and a list of metric labels.
    """
    num_orbits, num_scalars, num_timesteps = orbit_dataset.shape
    reordered_dataset = np.zeros_like(orbit_dataset)
    metrics_array = np.zeros((num_orbits, 4))  # Assuming four metrics
    metric_labels = ['disorder_metric', 'correct_order', 'inversions', 'kendall_tau_distance']
    
    for i in range(num_orbits):
        # Extract the time steps and corresponding data for the current orbit
        orbit_data = orbit_dataset[i]
        time_steps = orbit_data[0]
        
        # Calculate the disorder metric for the current orbit
        sorted_indices = np.argsort(time_steps)
        disorder_metric = np.sum(np.abs(sorted_indices - np.arange(len(time_steps))))
        correct_order = np.sum(np.diff(time_steps) >= 0)
        
        # Calculate the number of inversions
        inversions = sum(1 for j in range(num_timesteps) for k in range(j + 1, num_timesteps) if time_steps[j] > time_steps[k])
        
        # Calculate Kendall's tau distance
        tau, _ = kendalltau(time_steps, np.sort(time_steps))
        kendall_tau_distance = 1 - tau if not np.isnan(tau) else 1.0  # Handle NaN
        
        # Store the metrics in the array
        metrics_array[i] = [disorder_metric, correct_order, inversions, kendall_tau_distance]
        
        # Reorder the orbit data based on the sorted indices
        reordered_orbit_data = orbit_data[:, sorted_indices]
        
        # Store the reordered orbit data in the new dataset
        reordered_dataset[i] = reordered_orbit_data
    
    return reordered_dataset, metrics_array, metric_labels

# %% ../nbs/02_processing.ipynb 19
def pad_and_convert_to_3d(orbits: Dict[int, np.ndarray],     # Dictionary of orbits with numerical keys.
                          timesteps: int                     # Desired number of timesteps.
                         ) -> np.ndarray:                    # 3D numpy array of padded orbits.
    """
    Truncate and pad each orbit to a uniform length and convert to a 3D numpy array.
    """
    # Initialize a list to store the padded arrays
    padded_arrays = []

    # Iterate over each orbit in the dictionary
    for key, orbit in orbits.items():
        # Determine the number of timesteps to take from the orbit
        num_timesteps = min(timesteps, orbit.shape[1])

        # Take the first num_timesteps from the orbit
        truncated_orbit = orbit[:, :num_timesteps]

        # Pad the truncated orbit to have length timesteps in the final dimension
        padded_orbit = np.pad(truncated_orbit, ((0, 0), (0, timesteps - num_timesteps)))

        # Add the padded orbit to the list
        padded_arrays.append(padded_orbit)

    # Convert the list of padded arrays to a 3D numpy array and return it
    return np.stack(padded_arrays)

# %% ../nbs/02_processing.ipynb 20
def segment_and_convert_to_3d(orbits: Dict[int, np.ndarray],  # Dictionary of orbits with numerical keys.
                              segment_length: int             # Desired length of each segment.
                             ) -> Tuple[np.ndarray,           # 3D numpy array of segments.
                                        List[int]]:           # List of IDs representing each new segment.
    """
    Divide each orbit into segments of a given length and convert to a 3D numpy array.
    """
    
    # Initialize a list to store the segments and their corresponding IDs
    segments = []
    segment_ids = []

    # Iterate over each orbit in the dictionary
    for key, orbit in orbits.items():
        # Determine the number of complete segments that can be taken from the orbit
        num_segments = orbit.shape[1] // segment_length

        # Iterate over the number of complete segments
        for i in range(num_segments):
            # Take the segment of the desired length
            segment = orbit[:, i*segment_length:(i+1)*segment_length]

            # Add the segment to the list
            segments.append(segment)

            # Add the corresponding ID to the list
            segment_ids.append(key)

    # Convert the list of segments to a 3D numpy array
    segments_3d = np.stack(segments)

    return segments_3d, segment_ids

# %% ../nbs/02_processing.ipynb 23
def add_time_vector_to_orbits(orbits: Dict[int, np.ndarray],  # Dictionary of orbits with numerical keys.
                              propagated_periods: List[float], # List of propagated periods for each orbit.
                              periods: List[float]            # List of periods for each orbit.
                             ) -> Dict[int, np.ndarray]:      # Dictionary of updated orbits with time vectors added.
    """
    Add a time vector to each orbit in the dictionary.
    """
    # Create a new dictionary to store the updated orbits
    updated_orbits = {}

    # Iterate over each orbit in the dictionary
    for key, orbit in orbits.items():
        # Extract the propagated_periods and period for this orbit using the key as index
        propagated_period = propagated_periods[key]
        period = periods[key]

        # Compute the new time vector
        tvec = np.linspace(0, propagated_period * period, orbit.shape[1])

        # Add the time vector as the first vector in the orbit array
        updated_orbit = np.vstack([tvec, orbit])

        # Add the updated orbit to the new dictionary
        updated_orbits[key] = updated_orbit

    return updated_orbits
