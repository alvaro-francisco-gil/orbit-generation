"""Necessary scripts to read orbits from different formats"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% auto 0
__all__ = ['EPS', 'load_orbit_data', 'load_memmap_array', 'get_orbit_features', 'save_data', 'get_example_orbit_data',
           'order_labels_and_array_with_target', 'sample_orbits', 'discard_random_labels',
           'remove_duplicates_preserve_order', 'create_dataloaders', 'TSFeatureWiseScaler', 'TSGlobalScaler']

# %% ../nbs/01_data.ipynb 2
import h5py
from scipy.io import loadmat
import numpy as np
import os
import pandas as pd
from typing import Optional, Any, Union, List, Dict, Tuple
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from .path import get_data_path

# %% ../nbs/01_data.ipynb 3
from unittest.mock import patch, MagicMock
from fastcore.test import test_eq

# %% ../nbs/01_data.ipynb 5
def load_orbit_data(file_path: str,  # The path to the .mat, .h5, or .npy file.
                    variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.
                    dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.
                   ) -> Any:  # The loaded orbit data.
    """
    Load orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.
    """
    if file_path.endswith('.mat'):
        if variable_name is None:
            raise ValueError("variable_name must be provided for .mat files")
        mat = loadmat(file_path)
        if variable_name in mat:
            data = mat[variable_name]
        else:
            raise ValueError(f"{variable_name} not found in {file_path}")

    elif file_path.endswith('.h5'):
        with h5py.File(file_path, 'r') as file:
            if dataset_path is None:
                raise ValueError("dataset_path must be provided for .h5 files")
            if dataset_path in file:
                data = np.array(file[dataset_path])
            else:
                raise ValueError(f"{dataset_path} not found in {file_path}")

    elif file_path.endswith('.npy'):
        data = np.load(file_path)

    else:
        raise ValueError("Unsupported file format. Please provide a .mat, .h5, or .npy file.")
    
    return data

# %% ../nbs/01_data.ipynb 7
def load_memmap_array(file_path: str,  # The path to the .npy file as a string.
                      mode: str = 'c'  # Mode for memory-mapping ('r', 'r+', 'w+', 'c').
                     ) -> np.memmap:   # Returns a memory-mapped array.
    """
    Load a .npy file as a memory-mapped array using numpy.memmap.
    """
    
    # Check if the file exists at the specified path
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"No file found at the specified path: {file_path}")
    
    # Load the .npy file as a memmap object with the specified mode
    return np.load(file_path, mmap_mode=mode)

# %% ../nbs/01_data.ipynb 8
def get_orbit_features(file_path: str,  # The path to the file (can be .mat, .h5, or .npy).
                       variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.
                       dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.
                      ) -> pd.DataFrame:  # DataFrame with detailed orbit features.
    """
    Load orbit feature data from a specified file and convert it to a DataFrame.
    """
    # Load data using the previously defined function that supports .mat, .h5, and .npy files
    orbit_data = load_orbit_data(file_path, variable_name=variable_name, dataset_path=dataset_path)
    
    # Define column labels for the DataFrame
    column_labels = [
        'Orbit Family', 'Initial Position X', 'Initial Position Y', 'Initial Position Z',
        'Initial Velocity X', 'Initial Velocity Y', 'Initial Velocity Z',
        'Jacobi Constant', 'Period', 'Stability Index'
    ]
    
    # Create a DataFrame from the loaded data
    features = pd.DataFrame(orbit_data, columns=column_labels)

    return features

# %% ../nbs/01_data.ipynb 11
def save_data(data: np.ndarray,  # The numpy array data to save.
              file_name: str  # The name of the file to save the data in, including the extension.
             ) -> None:
    """
    Save a numpy array to a file based on the file extension specified in `file_name`.
    Supports saving to HDF5 (.hdf5) or NumPy (.npy) file formats.
    """
    # Extract file extension from file name
    _, file_extension = os.path.splitext(file_name)
    
    if file_extension == '.hdf5':
        # Open a new HDF5 file
        with h5py.File(file_name, 'w') as f:
            # Create a dataset in the file
            f.create_dataset('data', data=data, compression='gzip', compression_opts=9)
    elif file_extension == '.npy':
        # Save the array to a NumPy .npy file
        np.save(file_name, data)
    else:
        # Raise an error for unsupported file types
        raise ValueError("Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.")

# %% ../nbs/01_data.ipynb 14
def get_example_orbit_data(
    ) -> np.ndarray:  # Return type annotation added
    """
    Load example orbit data from a numpy file located in the example_data directory.
    """
    # Construct path to example data file
    data_path = get_data_path() / "example_training_data" / "example_orbits.npy"
    
    # Convert Path to string before passing to load_orbit_data
    data = load_orbit_data(str(data_path))
    
    return data

# %% ../nbs/01_data.ipynb 17
def order_labels_and_array_with_target(
    labels: np.ndarray,  # Array of labels to be ordered
    array: np.ndarray,  # Array to be ordered according to labels
    target_label: str,  # Label to order by
    place_at_end: bool = False,  # Whether to place target label at end
    ) -> tuple[np.ndarray, np.ndarray]:  # Returns ordered labels and array
    """
    Orders labels and array by placing entries with target_label either at start or end.
    """
    # Convert labels to a numpy array if it's not already
    labels = np.array(labels)
    n = len(labels)
    
    # Create index arrays to sort based on target label
    primary_indices = [i for i in range(n) if labels[i] == target_label]
    secondary_indices = [i for i in range(n) if labels[i] != target_label]

    # If place_at_end is True, reorder the indices
    if place_at_end:
        combined_indices = secondary_indices + primary_indices
    else:
        combined_indices = primary_indices + secondary_indices
    
    # Use indices to sort labels and array
    ordered_labels = labels[combined_indices]
    ordered_array = array[combined_indices]
    
    return ordered_labels, ordered_array

# %% ../nbs/01_data.ipynb 20
def sample_orbits(orbit_data: np.ndarray,  # Array of orbit data with shape (num_orbits, 6, num_time_points)
                  sample_spec: Union[dict, int],  # Number of samples per class (dict) or total samples (int)
                  labels: Optional[np.ndarray] = None,  # Array of labels for each orbit
                  ) -> tuple[np.ndarray, Optional[np.ndarray]]:
    """
    Randomly sample orbits from the provided dataset.
    """
    if labels is not None and isinstance(sample_spec, dict):
        # Sampling specified number of orbits for each class
        indices = []
        for label, count in sample_spec.items():
            class_indices = np.where(labels == label)[0]
            if len(class_indices) < count:
                raise ValueError(f"Not enough samples for class {label}. Requested {count}, available {len(class_indices)}.")
            selected_indices = np.random.choice(class_indices, size=count, replace=False)
            indices.extend(selected_indices)
        indices = np.array(indices)
    else:
        # Random sampling without considering classes
        indices = np.random.choice(orbit_data.shape[0], size=sample_spec, replace=False)
    
    # Select the sampled data and labels
    sampled_data = orbit_data[indices]
    sampled_labels = labels[indices] if labels is not None else None
    
    return sampled_data, sampled_labels

# %% ../nbs/01_data.ipynb 23
def discard_random_labels(data: np.ndarray,  # Dataset to filter
                         labels: np.ndarray,  # Labels corresponding to the data
                         discard_labels: Union[List, Dict, int],  # Labels to discard - list, dict or number
                         ) -> Tuple[List, np.ndarray, np.ndarray]:
    """
    Discards random or specified labels from the dataset.
    
    Returns tuple of (discarded labels, filtered data, filtered labels).
    """
    # Handle empty dictionary or empty list
    if isinstance(discard_labels, dict) and not discard_labels:
        return [], data, labels  # Return everything as is if dictionary is empty
    elif isinstance(discard_labels, list) and not discard_labels:
        return [], data, labels  # Return everything as is if list is empty
    
    # Check if discard_labels is a list
    if isinstance(discard_labels, list):
        # Use the provided list of labels to discard
        discarded = np.array(discard_labels)
    elif isinstance(discard_labels, dict):
        # If it's a dictionary, use its keys as labels to discard
        discarded = np.array(list(discard_labels.keys()))
    else:
        # Get unique labels
        unique_labels = np.unique(labels)
        # Randomly select labels to discard
        discarded = np.random.choice(unique_labels, size=discard_labels, replace=False)
    
    # Create a mask for samples that are not discarded
    mask = ~np.isin(labels, discarded)
    
    # Return the discarded labels and the filtered dataset
    return discarded.tolist(), data[mask], labels[mask]


# %% ../nbs/01_data.ipynb 25
def remove_duplicates_preserve_order(input_list: List,  # Input list that may contain duplicates
                                   ) -> List:  # Returns list with duplicates removed while preserving order
    """
    Removes duplicate items from a list while preserving the original order.
    """
    unique_items = []
    seen = set()
    for item in input_list:
        if item not in seen:
            seen.add(item)
            unique_items.append(item)
    return unique_items

# %% ../nbs/01_data.ipynb 27
def create_dataloaders(scaled_data: torch.Tensor,  # Input tensor of scaled data
                      val_split: float = 0.2,  # Fraction of data to use for validation
                      batch_size: int = 32,  # Batch size for dataloaders
                      ) -> Tuple[DataLoader, Optional[DataLoader]]:  # Returns train and optional val dataloaders
    """
    Creates train and validation dataloaders from input tensor data.
    """
    if val_split > 0:
        X_train, X_val = train_test_split(
            scaled_data,
            test_size=val_split,
            shuffle=True,
            random_state=42
        )
        train_dataset = TensorDataset(X_train)
        val_dataset = TensorDataset(X_val)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
    else:
        train_dataset = TensorDataset(scaled_data)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
        val_dataloader = None
    
    return train_dataloader, val_dataloader

# %% ../nbs/01_data.ipynb 29
EPS = 1e-18  # A small epsilon to prevent division by zero

# %% ../nbs/01_data.ipynb 30
class TSFeatureWiseScaler():
    """
    Scales time series data feature-wise using PyTorch tensors.

    Parameters:
    -----------
    feature_range : tuple(float, float), optional
        Tuple representing the minimum and maximum feature values (default is (0, 1)).

    Attributes:
    -----------
    _min_v : float
        Minimum feature value.
    _max_v : float
        Maximum feature value.
    """
    def __init__(self, feature_range: tuple = (0, 1)) -> None:
        assert len(feature_range) == 2
        self._min_v, self._max_v = feature_range

    def fit(self, X: torch.Tensor) -> "TSFeatureWiseScaler":
        """
        Fits the scaler to the data.
        
        :param X: Input data. Shape: (N, F, T) where N is number of data points,
                  F is number of features, and T is number of time steps.
        :type X: torch.Tensor
        
        :returns: The fitted scaler object.
        :rtype: TSFeatureWiseScaler
        """
        F = X.shape[1]
        self.mins = torch.zeros(F, device=X.device)
        self.maxs = torch.zeros(F, device=X.device)

        for i in range(F):
            self.mins[i] = torch.min(X[:, i, :])
            self.maxs[i] = torch.max(X[:, i, :])

        return self

    def transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Transforms the data.
        
        :param X: Input data. Shape: (N, F, T)
        :type X: torch.Tensor
        
        :returns: Scaled data.
        :rtype: torch.Tensor
        """
        X_scaled = X.clone()
        for i in range(X.shape[1]):
            X_scaled[:, i, :] = ((X[:, i, :] - self.mins[i]) / (self.maxs[i] - self.mins[i] + 1e-8)) * (self._max_v - self._min_v) + self._min_v
        return X_scaled

    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Inverse-transforms the data.
        
        :param X: Scaled data. Shape: (N, F, T)
        :type X: torch.Tensor
        
        :returns: Original data.
        :rtype: torch.Tensor
        """
        X_inv = X.clone()
        for i in range(X.shape[1]):
            X_inv[:, i, :] = (X[:, i, :] - self._min_v) / (self._max_v - self._min_v)
            X_inv[:, i, :] = X_inv[:, i, :] * (self.maxs[i] - self.mins[i] + 1e-8) + self.mins[i]
        return X_inv

    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Fits the scaler to the data and transforms it.
        
        :param X: Input data. Shape: (N, F, T)
        :type X: torch.Tensor
        
        :returns: Scaled data.
        :rtype: torch.Tensor
        """
        self.fit(X)
        return self.transform(X)

# %% ../nbs/01_data.ipynb 31
class TSGlobalScaler():
    """
    Scales time series data globally using PyTorch tensors.

    Attributes:
    -----------
    min : float
        Minimum value encountered in the data.
    max : float
        Maximum value encountered in the data.
    """
    def fit(self, X: torch.Tensor) -> "TSGlobalScaler":
        """
        Fits the scaler to the data.
        
        :param X: Input data.
        :type X: torch.Tensor
        
        :returns: The fitted scaler object.
        :rtype: TSGlobalScaler
        """
        self.min = torch.min(X)
        self.max = torch.max(X)
        return self

    def transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Transforms the data.
        
        :param X: Input data.
        :type X: torch.Tensor
        
        :returns: Scaled X.
        :rtype: torch.Tensor
        """
        return (X - self.min) / (self.max - self.min + EPS)

    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Inverse-transforms the data.
        
        :param X: Scaled data.
        :type X: torch.Tensor
        
        :returns: Original data.
        :rtype: torch.Tensor
        """
        X = X * (self.max - self.min + EPS)
        X = X + self.min
        return X

    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Fits the scaler to the data and transforms it.
        
        :param X: Input data.
        :type X: torch.Tensor
        
        :returns: Scaled input data X.
        :rtype: torch.Tensor
        """
        self.fit(X)
        return self.transform(X)
