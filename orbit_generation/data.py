"""Necessary scripts to read orbits from different formats"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% auto 0
__all__ = ['EPS', 'load_orbit_data', 'load_memmap_array', 'get_orbit_features', 'save_data', 'get_example_orbit_data',
           'order_labels_and_array_with_target', 'sample_orbits', 'discard_random_labels',
           'remove_duplicates_preserve_order', 'create_dataloaders', 'TSFeatureWiseScaler', 'TSGlobalScaler']

# %% ../nbs/01_data.ipynb 2
import h5py
from scipy.io import loadmat
import numpy as np
import os
import pandas as pd
from typing import Optional, Any
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

# %% ../nbs/01_data.ipynb 3
from unittest.mock import patch, MagicMock
from fastcore.test import test_eq

# %% ../nbs/01_data.ipynb 6
def load_orbit_data(file_path: str,  # The path to the .mat, .h5, or .npy file.
                    variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.
                    dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.
                   ) -> Any:  # The loaded orbit data.
    """
    Load orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.
    """
    if file_path.endswith('.mat'):
        if variable_name is None:
            raise ValueError("variable_name must be provided for .mat files")
        mat = loadmat(file_path)
        if variable_name in mat:
            data = mat[variable_name]
        else:
            raise ValueError(f"{variable_name} not found in {file_path}")

    elif file_path.endswith('.h5'):
        with h5py.File(file_path, 'r') as file:
            if dataset_path is None:
                raise ValueError("dataset_path must be provided for .h5 files")
            if dataset_path in file:
                data = np.array(file[dataset_path])
            else:
                raise ValueError(f"{dataset_path} not found in {file_path}")

    elif file_path.endswith('.npy'):
        data = np.load(file_path)

    else:
        raise ValueError("Unsupported file format. Please provide a .mat, .h5, or .npy file.")
    
    return data

# %% ../nbs/01_data.ipynb 8
def load_memmap_array(file_path: str,  # The path to the .npy file as a string.
                      mode: str = 'c'  # Mode for memory-mapping ('r', 'r+', 'w+', 'c').
                     ) -> np.memmap:   # Returns a memory-mapped array.
    """
    Load a .npy file as a memory-mapped array using numpy.memmap.
    
    Args:
    file_path: A string representing the path to the .npy file.
    mode: The mode in which the file is to be opened. Valid options are:
          - 'r'  : Read-only, no data can be modified.
          - 'r+' : Read/write, modifications to the data are written to the file.
          - 'w+' : Read/write, file is created if it does not exist, overwritten if it does.
          - 'c'  : Copy-on-write, data can be modified in memory but changes are not saved to the file.

    Returns:
    A numpy.memmap object that behaves like a numpy array but with data stored on disk instead of in memory.
    """
    
    # Check if the file exists at the specified path
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"No file found at the specified path: {file_path}")
    
    # Load the .npy file as a memmap object with the specified mode
    return np.load(file_path, mmap_mode=mode)

# %% ../nbs/01_data.ipynb 9
def get_orbit_features(file_path: str,  # The path to the file (can be .mat, .h5, or .npy).
                       variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.
                       dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.
                      ) -> pd.DataFrame:  # DataFrame with detailed orbit features.
    """
    Load orbit feature data from a specified file and convert it to a DataFrame.
    """
    # Load data using the previously defined function that supports .mat, .h5, and .npy files
    orbit_data = load_orbit_data(file_path, variable_name=variable_name, dataset_path=dataset_path)
    
    # Define column labels for the DataFrame
    column_labels = [
        'Orbit Family', 'Initial Position X', 'Initial Position Y', 'Initial Position Z',
        'Initial Velocity X', 'Initial Velocity Y', 'Initial Velocity Z',
        'Jacobi Constant', 'Period', 'Stability Index'
    ]
    
    # Create a DataFrame from the loaded data
    features = pd.DataFrame(orbit_data, columns=column_labels)

    return features

# %% ../nbs/01_data.ipynb 12
def save_data(data: np.ndarray,  # The numpy array data to save.
              file_name: str  # The name of the file to save the data in, including the extension.
             ) -> None:
    """
    Save a numpy array to a file based on the file extension specified in `file_name`.
    Supports saving to HDF5 (.hdf5) or NumPy (.npy) file formats.
    """
    # Extract file extension from file name
    _, file_extension = os.path.splitext(file_name)
    
    if file_extension == '.hdf5':
        # Open a new HDF5 file
        with h5py.File(file_name, 'w') as f:
            # Create a dataset in the file
            f.create_dataset('data', data=data, compression='gzip', compression_opts=9)
    elif file_extension == '.npy':
        # Save the array to a NumPy .npy file
        np.save(file_name, data)
    else:
        # Raise an error for unsupported file types
        raise ValueError("Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.")

# %% ../nbs/01_data.ipynb 15
def get_example_orbit_data():
    """
    Load orbit data from a hardcoded MAT file located in the `data` directory.
    
    The function is specifically designed to load the 'Xarray' variable 
    from the '1_L2_S_200_EM_CR3BP.mat' file. This setup is intended for 
    demonstration or testing purposes, where the data file and the variable 
    of interest are known ahead of time.

    :return: A numpy.ndarray containing the transposed data from the MAT file.
    """
    # Hardcoded file name and variable name
    filename = "example_orbits_1_L2_S_200_EM_CR3BP.mat"
    variable_name = 'Xarray'
    
    # Assuming the notebook or script is executed in a directory at the same level as the `data` folder
    matlab_file_path = '..' + "/data/example_data/" + filename
    
    # Assuming `load_orbit_data` is a predefined function that loads and returns data from the .mat file
    data = load_orbit_data(str(matlab_file_path), variable_name=variable_name)
    # Transpose the data for further use
    data = np.transpose(data, (2, 1, 0))
    
    return data

# %% ../nbs/01_data.ipynb 18
def order_labels_and_array_with_target(labels, array, target_label, place_at_end=False):
    # Convert labels to a numpy array if it's not already
    labels = np.array(labels)
    n = len(labels)
    
    # Create index arrays to sort based on target label
    primary_indices = [i for i in range(n) if labels[i] == target_label]
    secondary_indices = [i for i in range(n) if labels[i] != target_label]

    # If place_at_end is True, reorder the indices
    if place_at_end:
        combined_indices = secondary_indices + primary_indices
    else:
        combined_indices = primary_indices + secondary_indices
    
    # Use indices to sort labels and array
    ordered_labels = labels[combined_indices]
    ordered_array = array[combined_indices]
    
    return ordered_labels, ordered_array

# %% ../nbs/01_data.ipynb 21
def sample_orbits(orbit_data: np.ndarray,  # Orbit data array
                  sample_spec: dict or int, # Number of samples per class (dict) or total number of samples (int)
                  labels: np.ndarray = None # Optional: Array of labels corresponding to each orbit
                 ) -> (np.ndarray, np.ndarray):
    """
    Randomly sample orbits from the provided dataset.
    
    Parameters:
        orbit_data (np.ndarray): Array of orbit data with shape (num_orbits, 6, num_time_points).
        sample_spec (dict or int): If int, it is the total number of orbits to sample.
                                   If dict, it specifies the number of samples for each class.
        labels (np.ndarray, optional): Array of labels for each orbit.
    
    Returns:
        tuple: A tuple containing the sampled orbit data and corresponding labels (if provided).
    """
    if labels is not None and isinstance(sample_spec, dict):
        # Sampling specified number of orbits for each class
        indices = []
        for label, count in sample_spec.items():
            class_indices = np.where(labels == label)[0]
            if len(class_indices) < count:
                raise ValueError(f"Not enough samples for class {label}. Requested {count}, available {len(class_indices)}.")
            selected_indices = np.random.choice(class_indices, size=count, replace=False)
            indices.extend(selected_indices)
        indices = np.array(indices)
    else:
        # Random sampling without considering classes
        indices = np.random.choice(orbit_data.shape[0], size=sample_spec, replace=False)
    
    # Select the sampled data and labels
    sampled_data = orbit_data[indices]
    sampled_labels = labels[indices] if labels is not None else None
    
    return sampled_data, sampled_labels

# %% ../nbs/01_data.ipynb 24
def discard_random_labels(data, labels, discard_labels):
    # Check if discard_labels is a list
    if isinstance(discard_labels, list):
        # Use the provided list of labels to discard
        discarded = np.array(discard_labels)
    else:
        # Get unique labels
        unique_labels = np.unique(labels)
        # Randomly select labels to discard
        discarded = np.random.choice(unique_labels, size=discard_labels, replace=False)
    
    # Create a mask for samples that are not discarded
    mask = ~np.isin(labels, discarded)
    
    # Return the discarded labels and the filtered dataset
    return discarded.tolist(), data[mask], labels[mask]

# %% ../nbs/01_data.ipynb 26
def remove_duplicates_preserve_order(input_list):
    unique_items = []
    seen = set()
    for item in input_list:
        if item not in seen:
            seen.add(item)
            unique_items.append(item)
    return unique_items

# %% ../nbs/01_data.ipynb 28
def create_dataloaders(scaled_data, val_split=0.2, batch_size=32):
    if val_split > 0:
        X_train, X_val = train_test_split(
            scaled_data,
            test_size=val_split,
            shuffle=True,
            random_state=42
        )
        train_dataset = TensorDataset(X_train)
        val_dataset = TensorDataset(X_val)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
    else:
        train_dataset = TensorDataset(scaled_data)
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
        val_dataloader = None
    
    return train_dataloader, val_dataloader

# %% ../nbs/01_data.ipynb 30
EPS = 1e-18  # A small epsilon to prevent division by zero

# %% ../nbs/01_data.ipynb 31
class TSFeatureWiseScaler():
    """
    Scales time series data feature-wise using PyTorch tensors.

    Parameters:
    -----------
    feature_range : tuple(float, float), optional
        Tuple representing the minimum and maximum feature values (default is (0, 1)).

    Attributes:
    -----------
    _min_v : float
        Minimum feature value.
    _max_v : float
        Maximum feature value.
    """
    def __init__(self, feature_range: tuple = (0, 1)) -> None:
        assert len(feature_range) == 2
        self._min_v, self._max_v = feature_range

    def fit(self, X: torch.Tensor) -> "TSFeatureWiseScaler":
        """
        Fits the scaler to the data.
        
        :param X: Input data. Shape: (N, F, T) where N is number of data points,
                  F is number of features, and T is number of time steps.
        :type X: torch.Tensor
        
        :returns: The fitted scaler object.
        :rtype: TSFeatureWiseScaler
        """
        F = X.shape[1]
        self.mins = torch.zeros(F, device=X.device)
        self.maxs = torch.zeros(F, device=X.device)

        for i in range(F):
            self.mins[i] = torch.min(X[:, i, :])
            self.maxs[i] = torch.max(X[:, i, :])

        return self

    def transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Transforms the data.
        
        :param X: Input data. Shape: (N, F, T)
        :type X: torch.Tensor
        
        :returns: Scaled data.
        :rtype: torch.Tensor
        """
        X_scaled = X.clone()
        for i in range(X.shape[1]):
            X_scaled[:, i, :] = ((X[:, i, :] - self.mins[i]) / (self.maxs[i] - self.mins[i] + 1e-8)) * (self._max_v - self._min_v) + self._min_v
        return X_scaled

    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Inverse-transforms the data.
        
        :param X: Scaled data. Shape: (N, F, T)
        :type X: torch.Tensor
        
        :returns: Original data.
        :rtype: torch.Tensor
        """
        X_inv = X.clone()
        for i in range(X.shape[1]):
            X_inv[:, i, :] = (X[:, i, :] - self._min_v) / (self._max_v - self._min_v)
            X_inv[:, i, :] = X_inv[:, i, :] * (self.maxs[i] - self.mins[i] + 1e-8) + self.mins[i]
        return X_inv

    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Fits the scaler to the data and transforms it.
        
        :param X: Input data. Shape: (N, F, T)
        :type X: torch.Tensor
        
        :returns: Scaled data.
        :rtype: torch.Tensor
        """
        self.fit(X)
        return self.transform(X)

# %% ../nbs/01_data.ipynb 32
class TSGlobalScaler():
    """
    Scales time series data globally using PyTorch tensors.

    Attributes:
    -----------
    min : float
        Minimum value encountered in the data.
    max : float
        Maximum value encountered in the data.
    """
    def fit(self, X: torch.Tensor) -> "TSGlobalScaler":
        """
        Fits the scaler to the data.
        
        :param X: Input data.
        :type X: torch.Tensor
        
        :returns: The fitted scaler object.
        :rtype: TSGlobalScaler
        """
        self.min = torch.min(X)
        self.max = torch.max(X)
        return self

    def transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Transforms the data.
        
        :param X: Input data.
        :type X: torch.Tensor
        
        :returns: Scaled X.
        :rtype: torch.Tensor
        """
        return (X - self.min) / (self.max - self.min + EPS)

    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Inverse-transforms the data.
        
        :param X: Scaled data.
        :type X: torch.Tensor
        
        :returns: Original data.
        :rtype: torch.Tensor
        """
        X = X * (self.max - self.min + EPS)
        X = X + self.min
        return X

    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:
        """
        Fits the scaler to the data and transforms it.
        
        :param X: Input data.
        :type X: torch.Tensor
        
        :returns: Scaled input data X.
        :rtype: torch.Tensor
        """
        self.fit(X)
        return self.transform(X)
