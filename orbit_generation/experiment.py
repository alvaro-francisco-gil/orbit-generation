"""Scripts to perform the experiments"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_experiment.ipynb.

# %% auto 0
__all__ = ['setup_new_experiment', 'create_experiments_json', 'convert_numpy_types', 'add_experiment_metrics',
           'get_experiment_parameters', 'get_experiment_data', 'concatenate_orbits_from_experiment_folder',
           'convert_notebook', 'read_json_to_dataframe', 'generate_parameter_sets', 'create_experiment_image_grid',
           'plot_corr_matrix', 'execute_parameter_notebook', 'paralelize_notebook_experiment']

# %% ../nbs/08_experiment.ipynb 2
import os
import pandas as pd
import subprocess
import numpy as np
import shutil
import seaborn as sns
import json
from typing import Dict, Any, Optional
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import torch
import itertools
import nbformat
import papermill as pm
import re
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed

# %% ../nbs/08_experiment.ipynb 5
def setup_new_experiment(params: Dict[str, Any],              # Dictionary of parameters for the new experiment.
                         experiments_folder: str,             # Path to the folder containing all experiments.
                         json_file: Optional[str] = None      # Optional path to the JSON file tracking experiment parameters.
                        ) -> str:                             # The path to the newly created experiment folder.
    """
    Sets up a new experiment by creating a new folder and updating the JSON file with experiment parameters.
    """
    # Ensure the experiments folder exists
    if not os.path.exists(experiments_folder):
        os.makedirs(experiments_folder)

    # Default JSON file to 'experiments.json' in the experiments_folder if not provided
    if json_file is None:
        json_file = os.path.join(experiments_folder, 'experiments.json')

    # Load existing experiments from the JSON file if it exists
    if os.path.isfile(json_file):
        with open(json_file, mode='r') as file:
            experiments = json.load(file)
    else:
        experiments = []

    # Check if the parameters already exist in the JSON file
    for experiment in experiments:
        if all(experiment['parameters'].get(key) == value for key, value in params.items()):
            candidate_folder = os.path.join(experiments_folder, f"experiment_{experiment['id']}")
            if os.path.exists(candidate_folder):
                print(f'Parameters already exist for experiment: {candidate_folder}')
                return candidate_folder

    # Determine the next experiment number
    next_experiment_number = max((experiment['id'] for experiment in experiments), default=0) + 1

    # Create a new folder for the next experiment
    new_experiment_folder = os.path.join(experiments_folder, f'experiment_{next_experiment_number}')
    os.makedirs(new_experiment_folder, exist_ok=True)

    # Add the new experiment to the list and save to JSON file
    new_experiment = {
        'id': next_experiment_number,
        'parameters': params
    }
    experiments.append(new_experiment)
    with open(json_file, mode='w') as file:
        json.dump(experiments, file, indent=4)

    print(f'New experiment setup complete: {new_experiment_folder}')
    print(f'Parameters saved to {json_file}.')

    return new_experiment_folder

# %% ../nbs/08_experiment.ipynb 6
def create_experiments_json(parameter_sets, output_file='experiments.json'):
    """
    Create an experiments.json file from given parameter sets.
    
    Args:
    parameter_sets (list): List of dictionaries containing parameters for each experiment.
    output_file (str): Name of the output JSON file. Defaults to 'experiments.json'.
    
    Returns:
    None
    """
    experiments = []
    
    for i, params in enumerate(parameter_sets, start=1):
        experiment = {
            "id": i,
            "parameters": params
        }
        experiments.append(experiment)
    
    with open(output_file, 'w') as f:
        json.dump(experiments, f, indent=4)
    
    print(f"Experiments JSON file created: {output_file}")

# %% ../nbs/08_experiment.ipynb 8
def convert_numpy_types(obj):
    """
    Recursively convert numpy types and tensors to native Python types for JSON serialization.
    """
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, torch.Tensor):
        return obj.item() if obj.numel() == 1 else obj.tolist()
    else:
        return obj

# %% ../nbs/08_experiment.ipynb 9
def add_experiment_metrics(experiments_folder: str,                    # Path to the folder containing all experiments.
                           params: Optional[Dict[str, Any]] = None,    # Optional dictionary of parameters identifying the experiment.
                           experiment_id: Optional[int] = None,        # Optional ID to identify the experiment.
                           metrics: Optional[Dict[str, Any]] = None,   # Optional dictionary of metrics to be added to the experiment.
                           json_file: Optional[str] = None             # Optional path to the JSON file tracking experiment parameters and metrics.
                          ) -> None:
    """
    Adds metrics to an existing experiment in the JSON file based on the given parameters or ID.
    """
    if not os.path.exists(experiments_folder):
        raise FileNotFoundError(f"The experiments folder '{experiments_folder}' does not exist.")

    if json_file is None:
        json_file = os.path.join(experiments_folder, 'experiments.json')

    if not os.path.isfile(json_file):
        raise FileNotFoundError(f"The JSON file '{json_file}' does not exist.")

    if params is None and experiment_id is None:
        raise ValueError("Either 'params' or 'experiment_id' must be provided to identify the experiment.")

    if metrics is None:
        metrics = {}

    with open(json_file, mode='r') as file:
        try:
            experiments = json.load(file)
        except json.JSONDecodeError as e:
            raise ValueError(f"Error reading JSON file: {e}")

    found_experiment = False

    for experiment in experiments:
        if (experiment_id is not None and experiment['id'] == experiment_id) or \
           (params is not None and all(experiment['parameters'].get(key) == value for key, value in params.items())):
            experiment.update(convert_numpy_types(metrics))
            found_experiment = True
            break

    if not found_experiment:
        if experiment_id is not None:
            raise ValueError(f"Experiment with the specified ID {experiment_id} does not exist.")
        else:
            raise ValueError("Experiment with the specified parameters does not exist.")

    # Convert the entire experiments list to ensure all nested objects are serializable
    serializable_experiments = convert_numpy_types(experiments)

    with open(json_file, mode='w') as file:
        json.dump(serializable_experiments, file, indent=4)

    if experiment_id is not None:
        print(f'Metrics added to experiment with ID {experiment_id} in {json_file}.')
    else:
        experiment_id = experiment['id']
        print(f'Metrics added to experiment with ID {experiment_id} in {json_file}.')

# %% ../nbs/08_experiment.ipynb 11
def get_experiment_parameters(experiments_folder: str,                    # Path to the folder containing all experiments.
                              experiment_id: int,                         # ID to identify the experiment.
                              json_file: Optional[str] = None             # Optional path to the JSON file tracking experiment parameters and metrics.
                             ) -> Dict[str, Any]:
    """
    Retrieves the parameters of an experiment from the JSON file based on the given ID.
    """
    # Ensure the experiments folder exists
    if not os.path.exists(experiments_folder):
        raise FileNotFoundError(f"The experiments folder '{experiments_folder}' does not exist.")

    # Default JSON file to 'experiments.json' in the experiments_folder if not provided
    if json_file is None:
        json_file = os.path.join(experiments_folder, 'experiments.json')

    if not os.path.isfile(json_file):
        raise FileNotFoundError(f"The JSON file '{json_file}' does not exist.")

    # Load existing experiments from the JSON file
    with open(json_file, mode='r') as file:
        experiments = json.load(file)

    # Find the matching experiment and return its parameters
    for experiment in experiments:
        if experiment['id'] == experiment_id:
            return experiment.get('parameters', {})

    # If the experiment is not found, raise an error
    raise ValueError(f"Experiment with the specified ID {experiment_id} does not exist.")

# %% ../nbs/08_experiment.ipynb 13
def get_experiment_data(experiments_folder: str,
                        experiment_id: int,
                        json_file: Optional[str] = None
                       ) -> Dict[str, Any]:
    """
    Retrieves all data for an experiment from the JSON file based on the given ID.
    
    Args:
    experiments_folder (str): Path to the folder containing all experiments.
    experiment_id (int): ID to identify the experiment.
    json_file (Optional[str]): Optional path to the JSON file tracking experiment data.
    
    Returns:
    Dict[str, Any]: A dictionary containing all data for the specified experiment.
    
    Raises:
    FileNotFoundError: If the experiments folder or JSON file doesn't exist.
    ValueError: If the experiment with the specified ID is not found.
    """
    # Ensure the experiments folder exists
    if not os.path.exists(experiments_folder):
        raise FileNotFoundError(f"The experiments folder '{experiments_folder}' does not exist.")

    # Default JSON file to 'experiments.json' in the experiments_folder if not provided
    if json_file is None:
        json_file = os.path.join(experiments_folder, 'experiments.json')

    if not os.path.isfile(json_file):
        raise FileNotFoundError(f"The JSON file '{json_file}' does not exist.")

    # Load existing experiments from the JSON file
    with open(json_file, mode='r') as file:
        experiments = json.load(file)

    # Find the matching experiment and return all its data
    for experiment in experiments:
        if experiment['id'] == int(experiment_id):
            return experiment

    # If the experiment is not found, raise an error
    raise ValueError(f"Experiment with the specified ID {experiment_id} does not exist.")

# %% ../nbs/08_experiment.ipynb 14
def concatenate_orbits_from_experiment_folder(experiments_folder, seq_len):
    arrays = []
    
    for folder in os.listdir(experiments_folder):
        if folder.startswith('experiment_') and os.path.isdir(os.path.join(experiments_folder, folder)):
            # Extract the experiment number using regex
            match = re.search(r'experiment_(\d+)', folder)
            if match:
                experiment_id = match.group(1)
                generated_data_path = os.path.join(experiments_folder, folder, f'exp{experiment_id}_generated_orbits.npy')
                
                if os.path.isfile(generated_data_path):
                    generated_orbit = np.load(generated_data_path)
                    
                    if generated_orbit.shape[-1] == seq_len:
                        arrays.append(generated_orbit)
    
    if arrays:
        return np.concatenate(arrays, axis=0)
    else:
        return np.array([])

# %% ../nbs/08_experiment.ipynb 16
def convert_notebook(notebook_path: str,                # The path to the notebook to convert.
                     output_folder: str,                # The folder to save the converted file.
                     output_filename: str,              # The name of the output file.
                     format: str = 'html'               # The format to convert the notebook to ('html' or 'pdf').
                    ) -> None:                          # This function does not return a value.
    """
    Convert the specified Jupyter notebook to HTML or PDF.

    :param notebook_path: The path to the notebook to convert.
    :param output_folder: The folder to save the converted file.
    :param output_filename: The name of the output file.
    :param format: The format to convert the notebook to ('html' or 'pdf').
    """
    if format == 'pdf' and shutil.which('pandoc') is None:
        raise RuntimeError("Pandoc is required for PDF conversion but was not found. Please install Pandoc: https://pandoc.org/installing.html")

    # Create the full path for the output file
    os.makedirs(output_folder, exist_ok=True)
    output_path = os.path.join(output_folder, f"{output_filename}.{format}")

    # Convert the notebook using nbconvert
    command = f"jupyter nbconvert --to {format} \"{notebook_path}\" --output \"{output_path}\""
    try:
        subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        print(f"Notebook converted to {format.upper()} and saved at {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"An error occurred while converting the notebook to {format.upper()}:")
        print(e.stderr)
        raise

# %% ../nbs/08_experiment.ipynb 17
def read_json_to_dataframe(json_path: str) -> pd.DataFrame:
    """
    Reads a JSON file containing experiment results and returns a DataFrame.

    Args:
    - json_path (str): The path to the JSON file.

    Returns:
    - pd.DataFrame: A DataFrame containing the experiment results.
    """
    with open(json_path, 'r') as file:
        data = json.load(file)
    
    # Extract relevant information
    records = []
    for item in data:
        # Assuming each item is a dictionary with an 'id' field and other details
        record = {'id_experiment': item['id']}
        record.update(item)
        records.append(record)
    
    # Create DataFrame
    df = pd.DataFrame(records)
    return df

# %% ../nbs/08_experiment.ipynb 19
def generate_parameter_sets(params, model_specific_params):
    keys, values = zip(*params.items())
    combinations = [dict(zip(keys, v)) for v in itertools.product(*[
        value if isinstance(value, list) else [value] for value in values
    ])]
    
    final_combinations = []
    for combo in combinations:
        model_name = combo['model_name']
        if model_name in model_specific_params:
            model_kwargs = model_specific_params[model_name].copy()
            combo['model_kwargs'] = model_kwargs
            combo['model_kwargs']['beta'] = combo.pop('beta')
            final_combinations.append(combo)
    
    return final_combinations


# %% ../nbs/08_experiment.ipynb 21
def create_experiment_image_grid(experiments_folder, image_suffix, crop_length, font_size=12, save_path=None, grid_size=(3, 2), experiment_indices=None, hspace=-0.37):
    if experiment_indices is None:
        experiment_indices = [1, 2, 3, 4, 5, 6]  # Default set of indices

    # Set the directory for experiments
    experiments = [d for d in os.listdir(experiments_folder) if os.path.isdir(os.path.join(experiments_folder, d)) and 'experiment_' in d]
    experiments.sort()  # Sorting to maintain numerical order
    
    # Set up the plot with dynamic grid sizing
    fig, axes = plt.subplots(*grid_size, figsize=(5 * grid_size[1], 5 * grid_size[0]))  # Adjusted figsize dynamically
    axes = axes.flatten()
    
    for ax in axes:
        ax.axis('off')  # Hide axes

    # Process each experiment folder
    max_images = grid_size[0] * grid_size[1]
    processed_images = 0
    for idx in experiment_indices:
        if processed_images >= max_images:
            break
        experiment_name = f'experiment_{idx}'
        image_path = os.path.join(experiments_folder, experiment_name, 'images', f'exp{idx}_{image_suffix}')
        if os.path.exists(image_path):
            # Load and crop the image
            img = Image.open(image_path)
            img = img.crop((crop_length, crop_length, img.width - crop_length, img.height - crop_length))
            
            # Draw label
            draw = ImageDraw.Draw(img)
            try:
                font = ImageFont.truetype("arial.ttf", font_size)
            except IOError:
                font = ImageFont.load_default()
            text = f"Experiment {idx}"
            draw.text((10, 10), text, font=font, fill=(255, 255, 255))

            # Show image in grid
            axes[processed_images].imshow(img)
            axes[processed_images].set_title(text)
            processed_images += 1
        else:
            axes[processed_images].text(0.5, 0.5, 'Image not found', horizontalalignment='center', verticalalignment='center')
            processed_images += 1

    # Adjust layout
    plt.tight_layout()
    plt.subplots_adjust(hspace=hspace)  # Reduce vertical spacing

    # Save the grid if save_path is provided
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')

    # Display the grid
    plt.show()

# %% ../nbs/08_experiment.ipynb 22
def plot_corr_matrix(dataframe: pd.DataFrame, figsize=(14, 10), cmap='coolwarm', save_path: Optional[str] = None):
    """
    Plots a correlation matrix heatmap with annotations.
    
    Parameters:
    dataframe (pd.DataFrame): The DataFrame containing the data to be analyzed.
    figsize (tuple): The size of the figure (width, height).
    cmap (str): The color map to be used for the heatmap.
    save_path (Optional[str]): The path to save the plot image. If None, the plot is not saved.
    
    Returns:
    None: Displays the correlation matrix heatmap.
    """
    # Calculate the correlation matrix
    corr_matrix = dataframe.corr()

    # Set up the matplotlib figure
    plt.figure(figsize=figsize)

    # Draw the heatmap with the correlation numbers
    sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap=cmap, cbar=True, linewidths=.5, square=True)

    # Set the title
    plt.title('Correlation Matrix of Metrics')

    # Save the plot if a save path is provided
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
        print(f"Plot saved to {save_path}")

    # Show the plot
    plt.show()

# %% ../nbs/08_experiment.ipynb 24
def execute_parameter_notebook(notebook_to_execute, output_dir, i, params, checkpoint_file):
    try:
        # Mark as started
        with open(checkpoint_file, 'r+') as f:
            checkpoint = json.load(f)
            if i not in checkpoint['started']:
                checkpoint['started'].append(i)
                f.seek(0)
                json.dump(checkpoint, f)
                f.truncate()
        
        logging.info(f"Starting execution {i}")

        # Generate output filenames
        base_name = os.path.splitext(os.path.basename(notebook_to_execute))[0]
        output_notebook = os.path.join(output_dir, f"{base_name}_execution_{i}.ipynb")

        # Read the notebook
        with open(notebook_to_execute, 'r', encoding='utf-8') as f:
            nb = nbformat.read(f, as_version=4)
        
        nb = pm.execute_notebook(
            nb,
            output_notebook,
            parameters=params,
            kernel_name='pytorch',
            timeout=100000,
            log_output=True
        )
        
        logging.info(f"Completed execution {i}")
        return i

    except Exception as e:
        logging.error(f"Error in execution {i}: {str(e)}")
        logging.error(f"Parameters used: {params}")
        import traceback
        logging.error(f"Traceback: {traceback.format_exc()}")
        return None

# %% ../nbs/08_experiment.ipynb 25
def paralelize_notebook_experiment(parameter_sets, notebook_to_execute, output_dir, checkpoint_file, max_workers=3):
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            checkpoint = json.load(f)
    else:
        checkpoint = {'completed': [], 'started': []}
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f)

    # Ensure checkpoint is a dictionary with 'completed' and 'started' keys
    if not isinstance(checkpoint, dict) or 'completed' not in checkpoint or 'started' not in checkpoint:
        checkpoint = {'completed': [], 'started': []}
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f)
    
    # Filter out already completed executions
    remaining_executions = [i for i in range(1, len(parameter_sets) + 1) if i not in checkpoint['completed']]
    
    logging.info(f"Starting execution. {len(remaining_executions)} executions remaining.")
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for i in remaining_executions:
            future = executor.submit(
                execute_parameter_notebook,
                notebook_to_execute=notebook_to_execute,
                output_dir=output_dir,
                i=i,
                params=parameter_sets[i-1],
                checkpoint_file=checkpoint_file
            )
            futures.append(future)
        
        for future in as_completed(futures):
            result = future.result()
            if result is not None:
                logging.info(f"Execution {result} completed successfully.")
                # Update checkpoint
                with open(checkpoint_file, 'r+') as f:
                    checkpoint = json.load(f)
                    checkpoint['completed'].append(result)
                    f.seek(0)
                    json.dump(checkpoint, f)
                    f.truncate()
            else:
                logging.warning("An execution failed.")
    
    logging.info("All executions completed.")
