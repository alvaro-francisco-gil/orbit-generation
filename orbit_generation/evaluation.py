"""Scripts to perform evaluation on the data"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_evaluation.ipynb.

# %% auto 0
__all__ = ['plot_metric_heatmaps', 'plot_comparison', 'calculate_closest_feature_distances', 'find_non_matching_elements',
           'evaluate_clustering_multiple_labels', 'euclidean_distance_pair', 'manhattan_distance_pair',
           'cosine_distance_pair', 'dtw_distance_pair', 'distance_array', 'find_nearest_orbits',
           'find_nearest_orbits_batch', 'calculate_pairwise_distances', 'evaluate_distance_metrics_and_clustering']

# %% ../nbs/09_evaluation.ipynb 2
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, fowlkes_mallows_score, silhouette_score, jaccard_score, confusion_matrix
from sklearn.metrics.cluster import contingency_matrix
from scipy.optimize import linear_sum_assignment
from sklearn.preprocessing import LabelEncoder
from scipy.spatial.distance import squareform
from fastdtw import fastdtw
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial import cKDTree

# %% ../nbs/09_evaluation.ipynb 4
def plot_metric_heatmaps(results, distance_metrics, clustering_algorithms, evaluation_metrics):
    """
    Plot heatmaps for each evaluation metric.
    """
    n_metrics = len(evaluation_metrics)
    n_cols = 3
    n_rows = (n_metrics + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
    axes = axes.flatten()
    
    for i, metric in enumerate(evaluation_metrics):
        data = np.zeros((len(distance_metrics), len(clustering_algorithms)))
        for d, distance in enumerate(distance_metrics):
            for c, clustering in enumerate(clustering_algorithms):
                data[d, c] = results[f"{distance}_{clustering}"][metric]
        
        sns.heatmap(data, annot=True, fmt=".2f", cmap="YlGnBu", 
                    xticklabels=clustering_algorithms, yticklabels=distance_metrics, ax=axes[i])
        axes[i].set_title(f"{metric} Scores")
        axes[i].set_xlabel("Clustering Algorithms")
        axes[i].set_ylabel("Distance Metrics")
    
    # Remove any unused subplots
    for i in range(n_metrics, len(axes)):
        fig.delaxes(axes[i])
    
    plt.tight_layout()
    plt.show()

# %% ../nbs/09_evaluation.ipynb 5
def plot_comparison(orbit_df, synthetic_orbit_df):
    """
    Function to create a scatter plot comparing 'period' and 'calculated_jacobi'
    between two DataFrames and plot the index of points in synthetic_orbit_df.
    
    Parameters:
    - orbit_df: DataFrame containing 'period' and 'calculated_jacobi' columns
    - synthetic_orbit_df: DataFrame containing 'period' and 'calculated_jacobi' columns
    """
    plt.figure(figsize=(10, 6))

    # Plot for orbit_df
    plt.scatter(orbit_df['period'], orbit_df['calculated_jacobi'], color='blue', label='orbit_df', marker='o')

    # Plot for synthetic_orbit_df
    plt.scatter(synthetic_orbit_df['period'], synthetic_orbit_df['calculated_jacobi'], color='red', label='synthetic_orbit_df', marker='x')

    # Annotate the points in synthetic_orbit_df with their indices
    for i in synthetic_orbit_df.index:
        plt.text(synthetic_orbit_df['period'].loc[i], 
                 synthetic_orbit_df['calculated_jacobi'].loc[i], 
                 str(int(i)), 
                 fontsize=9, color='black', ha='right')

    # Add labels and title
    plt.xlabel('Period')
    plt.ylabel('Calculated Jacobi')
    plt.title('Comparison of Features Between Two DataFrames')
    plt.legend()  # Add legend to distinguish between the groups

    # Display the plot
    plt.grid(True)
    plt.show()

def calculate_closest_feature_distances(orbit_df, synthetic_orbit_df, features, display_comparison=True):
    """
    Calculate the distance from each point in synthetic_orbit_df to the closest point in orbit_df
    based on specified features.
    
    Parameters:
    - orbit_df: DataFrame containing the training data.
    - synthetic_orbit_df: DataFrame containing the synthetic data.
    - features: List of features (columns) to use for calculating the distances.
    
    Returns:
    - distances: A NumPy array of the minimum distances from each synthetic point to the nearest orbit point.
    """
    # Extract the relevant features from both DataFrames
    orbit_points = orbit_df[features].values
    synthetic_points = synthetic_orbit_df[features].values

    # Create a KDTree for efficient nearest-neighbor search in orbit_df
    tree = cKDTree(orbit_points)

    # Query the KDTree with the synthetic points to find the distance to the nearest orbit point
    distances, _ = tree.query(synthetic_points, k=1)

    if display_comparison:
        plot_comparison(orbit_df, synthetic_orbit_df)

    return distances

# %% ../nbs/09_evaluation.ipynb 6
def find_non_matching_elements(main_array, check_array):
    """
    Finds elements in check_array that are not present in main_array.

    Parameters:
    main_array (numpy.ndarray): The main array with larger set of elements.
    check_array (numpy.ndarray): The array with elements to check against the main array.

    Returns:
    numpy.ndarray: Elements in check_array that are not in main_array.
    """
    # Convert arrays to tuples to enable comparison
    main_set = set(map(tuple, main_array))
    check_set = set(map(tuple, check_array))

    # Find elements in check_set that are not in main_set
    non_matching_elements = check_set - main_set

    # Convert the result back to a numpy array
    non_matching_elements_array = np.array(list(non_matching_elements))

    return non_matching_elements_array

# %% ../nbs/09_evaluation.ipynb 8
def evaluate_clustering_multiple_labels(latent_representations: np.ndarray,  # The latent space data.
                                        list_of_labels: list,                # List of true labels or a single true labels array.
                                        clustering_method: str = 'kmeans',   # The clustering algorithm to use ('kmeans', 'gmm', 'dbscan').
                                        label_names: list = None,            # Optional names for the label sets.
                                        **kwargs                             # Additional arguments for the clustering algorithm.
                                       ) -> dict:                            # Returns a dictionary with clustering metrics.
    """
    Evaluates the clustering quality of the latent representations for one or multiple sets of labels.
    """
    
    def calculate_clustering_accuracy(true_labels, pred_labels):
        contingency = contingency_matrix(true_labels, pred_labels)
        row_ind, col_ind = linear_sum_assignment(-contingency)
        return contingency[row_ind, col_ind].sum() / np.sum(contingency)

    # Ensure list_of_labels is a list of arrays
    if isinstance(list_of_labels, np.ndarray):
        list_of_labels = [list_of_labels]
    
    # Use default names if label_names are not provided
    if label_names is None:
        label_names = [f'Set_{i+1}' for i in range(len(list_of_labels))]
    
    combined_metrics = {}
    average_metrics = {
        'ARI': 0,
        'NMI': 0,
        'Homogeneity': 0,
        'Completeness': 0,
        'V-Measure': 0,
        'FMI': 0,
        'Purity': 0,
        'Silhouette Score': 0,
        'Jaccard': 0,
        'Accuracy': 0
    }
    num_label_sets = len(list_of_labels)
    
    label_encoders = [LabelEncoder() for _ in range(num_label_sets)]
    encoded_labels_list = [label_encoders[i].fit_transform(list_of_labels[i]) for i in range(num_label_sets)]
    
    for i, true_labels in enumerate(encoded_labels_list):
        # Determine the number of clusters
        n_clusters = len(np.unique(true_labels))
        
        # Apply the selected clustering algorithm
        if clustering_method == 'kmeans':
            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, **kwargs)
            pred_labels = clusterer.fit_predict(latent_representations)
        elif clustering_method == 'gmm':
            clusterer = GaussianMixture(n_components=n_clusters, random_state=42, **kwargs)
            pred_labels = clusterer.fit_predict(latent_representations)
        elif clustering_method == 'dbscan':
            clusterer = DBSCAN(**kwargs)
            pred_labels = clusterer.fit_predict(latent_representations)
        else:
            raise ValueError("Unsupported clustering method. Choose from 'kmeans', 'gmm', 'dbscan'.")
        
        # Calculate clustering metrics
        ari = adjusted_rand_score(true_labels, pred_labels)
        nmi = normalized_mutual_info_score(true_labels, pred_labels)
        homogeneity = homogeneity_score(true_labels, pred_labels)
        completeness = completeness_score(true_labels, pred_labels)
        v_measure = v_measure_score(true_labels, pred_labels)
        fmi = fowlkes_mallows_score(true_labels, pred_labels)
        
        # Purity
        cont_matrix = contingency_matrix(true_labels, pred_labels)
        purity = np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)
        
        # Silhouette Score
        silhouette = silhouette_score(latent_representations, pred_labels)
        
        # Jaccard Coefficient and Accuracy
        jaccard = jaccard_score(true_labels, pred_labels, average='macro')
        accuracy = calculate_clustering_accuracy(true_labels, pred_labels)
        
        # Store the results for this set of labels
        combined_metrics.update({
            f'{label_names[i]}_ari': ari,
            f'{label_names[i]}_nmi': nmi,
            f'{label_names[i]}_homogeneity': homogeneity,
            f'{label_names[i]}_completeness': completeness,
            f'{label_names[i]}_v-measure': v_measure,
            f'{label_names[i]}_fmi': fmi,
            f'{label_names[i]}_purity': purity,
            f'{label_names[i]}_silhouette_score': silhouette,
            f'{label_names[i]}_jaccard': jaccard,
            f'{label_names[i]}_accuracy': accuracy
        })
        
        # Accumulate the results for averaging
        average_metrics['ARI'] += ari
        average_metrics['NMI'] += nmi
        average_metrics['Homogeneity'] += homogeneity
        average_metrics['Completeness'] += completeness
        average_metrics['V-Measure'] += v_measure
        average_metrics['FMI'] += fmi
        average_metrics['Purity'] += purity
        average_metrics['Silhouette Score'] += silhouette
        average_metrics['Jaccard'] += jaccard
        average_metrics['Accuracy'] += accuracy
    
    # Compute the average metrics if there are multiple sets of labels
    if num_label_sets > 1:
        for key in average_metrics:
            combined_metrics[f'average_{key}'] = average_metrics[key] / num_label_sets
    
    return combined_metrics

# %% ../nbs/09_evaluation.ipynb 11
def euclidean_distance_pair(orbit1: np.ndarray, orbit2: np.ndarray) -> float:
    """
    Calculates the Euclidean distance between two orbits.
    
    :param orbit1: First orbit array (shape: [n_features, n_time_steps]).
    :param orbit2: Second orbit array (shape: [n_features, n_time_steps]).
    :return: Euclidean distance as a float.
    """
    return np.sqrt(np.sum((orbit1 - orbit2) ** 2))

# %% ../nbs/09_evaluation.ipynb 13
def manhattan_distance_pair(orbit1: np.ndarray, orbit2: np.ndarray) -> float:
    """
    Calculates the Manhattan distance between two orbits.
    
    :param orbit1: First orbit array (shape: [n_features, n_time_steps]).
    :param orbit2: Second orbit array (shape: [n_features, n_time_steps]).
    :return: Manhattan distance as a float.
    """
    return np.sum(np.abs(orbit1 - orbit2))

# %% ../nbs/09_evaluation.ipynb 15
def cosine_distance_pair(orbit1: np.ndarray, orbit2: np.ndarray) -> float:
    """
    Calculates the Cosine distance between two orbits.
    
    :param orbit1: First orbit array (shape: [n_features, n_time_steps]).
    :param orbit2: Second orbit array (shape: [n_features, n_time_steps]).
    :return: Cosine distance as a float.
    """
    dot_product = np.sum(orbit1 * orbit2)
    norm1 = np.linalg.norm(orbit1)
    norm2 = np.linalg.norm(orbit2)
    if norm1 == 0.0 or norm2 == 0.0:
        return 1.0  # Maximum distance if one of the vectors is zero
    cosine_similarity = dot_product / (norm1 * norm2)
    return 1.0 - cosine_similarity

# %% ../nbs/09_evaluation.ipynb 17
def dtw_distance_pair(orbit1: np.ndarray, orbit2: np.ndarray) -> float:
    """
    Calculates the Dynamic Time Warping (DTW) distance between two orbits.
    
    :param orbit1: First orbit array (shape: [n_features, n_time_steps]).
    :param orbit2: Second orbit array (shape: [n_features, n_time_steps]).
    :return: DTW distance as a float.
    """
    # Transpose to shape [n_time_steps, n_features] as required by fastdtw
    distance, _ = fastdtw(orbit1.T, orbit2.T)
    return distance

# %% ../nbs/09_evaluation.ipynb 19
def distance_array(
    orbit_data1: np.ndarray,                # Shape: [n_samples1, n_features, n_time_steps] or [n_features, n_time_steps]
    orbit_data2: np.ndarray,                # Shape: [n_samples2, n_features, n_time_steps] or [n_features, n_time_steps]
    distance_metric: str                     # String representing the distance metric ('euclidean', 'manhattan', 'cosine', 'dtw')
) -> np.ndarray:
    """
    Calculates distances between orbits in two datasets using a specified distance metric.

    This function is robust to input shapes. If an input is a 2D array (representing a single orbit),
    it is automatically converted to a 3D array with one sample. This allows for flexible comparisons
    between single or multiple orbits.

    :param orbit_data1: First set of orbits (shape: [n_samples1, n_features, n_time_steps] or [n_features, n_time_steps]).
    :param orbit_data2: Second set of orbits or a single orbit.
                        Shape: [n_samples2, n_features, n_time_steps] or [n_features, n_time_steps].
    :param distance_metric: A string representing the distance metric to use ('euclidean', 'manhattan', 'cosine', 'dtw').

    :return: NumPy array of distances.
             - If one input is single and the other is multiple:
                 - Shape: [n_samples1] or [n_samples2]
             - If both inputs are multiple:
                 - Shape: [n_samples1, n_samples2]
    """
    
    def ensure_3d(array: np.ndarray) -> np.ndarray:
        """
        Ensures the input array is 3D. If it's 2D, adds an extra dimension.
        
        :param array: Input NumPy array.
        :return: 3D NumPy array.
        """
        if array.ndim == 2:
            return array[np.newaxis, ...]
        elif array.ndim == 3:
            return array
        else:
            raise ValueError(f"Input array must be either 2D or 3D, got {array.ndim}D array instead.")

    # Mapping of distance metrics to their corresponding functions
    distance_functions = {
        'euclidean': euclidean_distance_pair
    }

    # Ensure inputs are 3D
    orbit_data1 = ensure_3d(orbit_data1)
    orbit_data2 = ensure_3d(orbit_data2)

    n_samples1 = orbit_data1.shape[0]
    n_samples2 = orbit_data2.shape[0]

    # Verify that feature and time step dimensions match
    if orbit_data1.shape[1:] != orbit_data2.shape[1:]:
        raise ValueError("Feature and time step dimensions must match between orbit_data1 and orbit_data2.")

    # Check if the specified distance metric is supported
    if distance_metric not in distance_functions:
        raise ValueError(f"Unsupported distance metric: {distance_metric}. Choose from {list(distance_functions.keys())}.")

    # Select the appropriate distance function
    selected_distance_func = distance_functions[distance_metric]

    # Initialize the distance matrix
    if n_samples1 == 1 and n_samples2 == 1:
        # Both are single orbits
        distances = np.array([[selected_distance_func(orbit_data1[0], orbit_data2[0])]])
    elif n_samples1 == 1:
        # orbit_data1 is single, orbit_data2 is multiple
        distances = np.array([[selected_distance_func(orbit_data1[0], orbit_data2[j]) for j in range(n_samples2)]])
    elif n_samples2 == 1:
        # orbit_data2 is single, orbit_data1 is multiple
        distances = np.array([[selected_distance_func(orbit_data1[i], orbit_data2[0])] for i in range(n_samples1)])
    else:
        # Both are multiple
        distances = np.zeros((n_samples1, n_samples2), dtype=float)
        for i in range(n_samples1):
            for j in range(n_samples2):
                distances[i, j] = selected_distance_func(orbit_data1[i], orbit_data2[j])

    return distances

# %% ../nbs/09_evaluation.ipynb 21
def find_nearest_orbits(
    single_orbit: np.ndarray,
    orbit_data: np.ndarray,
    n: int,
    distance_metric: str = 'euclidean'
) -> tuple:
    """
    Finds the n closest orbits in orbit_data to the single_orbit based on the specified distance metric.

    :param single_orbit: The reference orbit (shape: [n_features, n_time_steps]).
    :param orbit_data: The dataset of orbits (shape: [n_samples, n_features, n_time_steps]).
    :param n: The number of closest orbits to return.
    :param distance_metric: The distance metric to use ('euclidean', 'manhattan', 'cosine', 'dtw').
                            Defaults to 'euclidean'.
    :return: A tuple containing:
             - Indices of the n closest orbits in orbit_data.
             - Distances of the n closest orbits.
    """
    # Calculate distance matrix
    distance_matrix = distance_array(orbit_data, single_orbit, distance_metric)
    
    # Flatten if single_orbit was 2D (now treated as 3D with shape [1, ...])
    if distance_matrix.ndim == 2 and distance_matrix.shape[0] == 1:
        distances = distance_matrix.flatten()
    else:
        distances = distance_matrix
    
    # Get the indices of the n smallest distances
    nearest_indices = np.argsort(distances)[:n]
    
    # Get the corresponding distances
    nearest_distances = distances[nearest_indices]
    
    return nearest_indices, nearest_distances

# %% ../nbs/09_evaluation.ipynb 22
def find_nearest_orbits_batch(
    single_orbits: np.ndarray,       # Shape: [num_single_orbits, n_features, n_time_steps]
    orbit_data: np.ndarray,          # Shape: [n_samples, n_features, n_time_steps]
    n: int,                           # Number of nearest orbits to find
    distance_metric: str = 'euclidean'  # Distance metric
) -> tuple:
    """
    Iteratively finds the n closest orbits in orbit_data for each orbit in single_orbits.

    :param single_orbits: The reference orbits (shape: [num_single_orbits, n_features, n_time_steps]).
    :param orbit_data: The dataset of orbits to search within (shape: [n_samples, n_features, n_time_steps]).
    :param n: The number of closest orbits to return for each single_orbit.
    :param distance_metric: The distance metric to use ('euclidean', 'manhattan', 'cosine', 'dtw').
                            Defaults to 'euclidean'.
    :return: A tuple containing:
             - A 2D array of shape [num_single_orbits, n] with indices of the n closest orbits.
             - A 2D array of shape [num_single_orbits, n] with distances of the n closest orbits.
    """
    # Validate input dimensions
    if single_orbits.ndim != 3:
        raise ValueError(f"single_orbits must be a 3D array, got {single_orbits.ndim}D array instead.")
    if orbit_data.ndim != 3:
        raise ValueError(f"orbit_data must be a 3D array, got {orbit_data.ndim}D array instead.")
    if single_orbits.shape[1:] != orbit_data.shape[1:]:
        raise ValueError("Each single_orbit must have the same shape as the orbits in orbit_data.")
    if n <= 0:
        raise ValueError("Parameter n must be a positive integer.")
    if distance_metric not in ['euclidean', 'manhattan', 'cosine', 'dtw']:
        raise ValueError(f"Unsupported distance metric: {distance_metric}. Choose from 'euclidean', 'manhattan', 'cosine', 'dtw'.")

    num_single_orbits = single_orbits.shape[0]
    
    nearest_indices_all = np.empty((num_single_orbits, n), dtype=int)
    nearest_distances_all = np.empty((num_single_orbits, n), dtype=float)

    for i in range(num_single_orbits):
        single_orbit = single_orbits[i]
        nearest_indices, nearest_distances = find_nearest_orbits(
            single_orbit=single_orbit,
            orbit_data=orbit_data,
            n=n,
            distance_metric=distance_metric
        )
        nearest_indices_all[i] = nearest_indices
        nearest_distances_all[i] = nearest_distances

    return nearest_indices_all, nearest_distances_all

# %% ../nbs/09_evaluation.ipynb 24
def calculate_pairwise_distances(
    orbit_data1: np.ndarray,       # Shape: [n_samples, n_features, n_time_steps]
    orbit_data2: np.ndarray,       # Shape: [n_samples, n_features, n_time_steps]
    distance_metric: str = 'euclidean'  # Distance metric
) -> np.ndarray:
    """
    Calculates the distance between corresponding orbits in two orbit datasets.
    
    :param orbit_data1: The first set of orbits (shape: [n_samples, n_features, n_time_steps]).
    :param orbit_data2: The second set of orbits (shape: [n_samples, n_features, n_time_steps]).
    :param distance_metric: The distance metric to use ('euclidean', 'manhattan', 'cosine', 'dtw').
                            Defaults to 'euclidean'.
    :return: An array of distances with shape [n_samples].
    """
    
    # Inline validation
    if orbit_data1.shape != orbit_data2.shape:
        raise ValueError("Both orbit datasets must have the same shape.")
    if orbit_data1.ndim != 3:
        raise ValueError(f"orbit_data1 must be a 3D array, got {orbit_data1.ndim}D array instead.")
    
    # Mapping of distance metrics to functions
    DISTANCE_FUNCTIONS = {
        'euclidean': euclidean_distance_pair,
        'manhattan': manhattan_distance_pair,
        'cosine': cosine_distance_pair,
        'dtw': dtw_distance_pair
    }
    
    if distance_metric not in DISTANCE_FUNCTIONS:
        raise ValueError(f"Unsupported distance metric: {distance_metric}. Choose from {list(DISTANCE_FUNCTIONS.keys())}.")
    
    n_samples = orbit_data1.shape[0]
    distances = np.empty(n_samples, dtype=float)
    
    distance_func = DISTANCE_FUNCTIONS[distance_metric]
    
    for i in range(n_samples):
        distances[i] = distance_func(orbit_data1[i], orbit_data2[i])
    
    return distances

# %% ../nbs/09_evaluation.ipynb 26
def evaluate_distance_metrics_and_clustering(orbit_data: np.ndarray,
                                    true_labels: np.ndarray,
                                    distance_metrics: list = None,
                                    clustering_algorithms: list = None,
                                    evaluation_metrics: list = None,
                                    n_clusters: int = None,
                                    plot_results: bool = True):
    """
    Evaluates specified distance metrics and clustering algorithms on the given orbit data.
    
    :param orbit_data: The orbit data as a multivariate time series (shape: [n_samples, n_features, n_time_steps]).
    :param true_labels: Array of true labels for the orbit data.
    :param distance_metrics: List of strings specifying distance metrics to use. If None, all available metrics are used.
    :param clustering_algorithms: List of strings specifying clustering algorithms to use. If None, all available algorithms are used.
    :param evaluation_metrics: List of strings specifying evaluation metrics to use. If None, all available metrics are used.
    :param n_clusters: Number of clusters for algorithms that require it. If None, it will be inferred from labels.
    :param plot_results: If True, plot heatmaps of the results.
    :return: A dictionary containing results for all combinations of metrics and clustering algorithms.
    """

    def calculate_clustering_accuracy(true_labels, pred_labels):
        cm = confusion_matrix(true_labels, pred_labels)
        row_ind, col_ind = linear_sum_assignment(-cm)
        return cm[row_ind, col_ind].sum() / np.sum(cm)

    # Define available distance metrics
    available_distance_metrics = {
        'euclidean': euclidean_distance_array,
        'manhattan': manhattan_distance_array,
        'cosine': cosine_distance_array,
        'dtw': dtw_distance_array
    }
    
    # Define available clustering algorithms
    available_clustering_algorithms = {
        'kmeans': KMeans,
        'gmm': GaussianMixture,
        'dbscan': DBSCAN
    }
    
    # Define available evaluation metrics
    available_evaluation_metrics = {
        'ARI': adjusted_rand_score,
        'NMI': normalized_mutual_info_score,
        'Homogeneity': homogeneity_score,
        'Completeness': completeness_score,
        'V-Measure': v_measure_score,
        'Purity': lambda true, pred: np.sum(np.amax(confusion_matrix(true, pred), axis=0)) / np.sum(confusion_matrix(true, pred)),
        'Accuracy': calculate_clustering_accuracy
    }
    
    # If no metrics specified, use all available
    if distance_metrics is None:
        distance_metrics = list(available_distance_metrics.keys())
    
    # If no algorithms specified, use all available
    if clustering_algorithms is None:
        clustering_algorithms = list(available_clustering_algorithms.keys())
    
    # If no evaluation metrics specified, use all available
    if evaluation_metrics is None:
        evaluation_metrics = list(available_evaluation_metrics.keys())
    
    results = {}
    
    # If n_clusters is not provided, infer it from the labels
    if n_clusters is None:
        n_clusters = len(np.unique(true_labels))
    
    for distance_metric in distance_metrics:
        if distance_metric not in available_distance_metrics:
            print(f"Warning: {distance_metric} is not an available distance metric. Skipping.")
            continue
        
        print(f"Computing {distance_metric} distances...")
        metric_func = available_distance_metrics[distance_metric]
        distance_matrix = metric_func(orbit_data)

        if np.any(distance_matrix < 0):
            distance_matrix = distance_matrix - np.min(distance_matrix)
            np.fill_diagonal(distance_matrix, 0)
        
        for algo_name in clustering_algorithms:
            if algo_name not in available_clustering_algorithms:
                print(f"Warning: {algo_name} is not an available clustering algorithm. Skipping.")
                continue
            
            print(f"Clustering with {algo_name}...")
            algo_class = available_clustering_algorithms[algo_name]
            
            if algo_name == 'dbscan':
                # For DBSCAN, we need to estimate eps
                distances = squareform(distance_matrix)
                eps = np.percentile(distances, 10)  # Use the 10th percentile of distances as eps
                clusterer = algo_class(eps=eps, min_samples=5, metric='precomputed')
                labels = clusterer.fit_predict(distance_matrix)
            elif algo_name == 'kmeans':
                clusterer = algo_class(n_clusters=n_clusters, random_state=42)
                labels = clusterer.fit_predict(distance_matrix)
            elif algo_name == 'gmm':
                clusterer = algo_class(n_components=n_clusters, random_state=42)
                labels = clusterer.fit_predict(distance_matrix)
            else:
                clusterer = algo_class()  # Use default parameters for other algorithms
                labels = clusterer.fit_predict(distance_matrix)
            
            # Evaluate clustering
            eval_results = {}
            for eval_metric in evaluation_metrics:
                if eval_metric not in available_evaluation_metrics:
                    print(f"Warning: {eval_metric} is not an available evaluation metric. Skipping.")
                    continue
                else:
                    eval_results[eval_metric] = available_evaluation_metrics[eval_metric](true_labels, labels)
            
            # Store results
            results[f"{distance_metric}_{algo_name}"] = eval_results
    
    if plot_results:
        plot_metric_heatmaps(results, distance_metrics, clustering_algorithms, evaluation_metrics)
    
    return results


