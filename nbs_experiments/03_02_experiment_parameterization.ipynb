{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from nbconvert import PDFExporter\n",
    "import nbformat\n",
    "\n",
    "from orbit_generation.experiment import generate_parameter_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Data\n",
    "    'data_used': 'EM_N_fix_1500',\n",
    "    'families_to_discard': [0, 2, 4, 10, 20],\n",
    "    'seq_len': 100,\n",
    "    'feature_dim': 7,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 50,\n",
    "    'val_split': 0.05,\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.001,\n",
    "    \n",
    "    # Model\n",
    "    'model_name': ['vae_conv5_legit', 'inception_time_wp_vae'],\n",
    "    'latent_dim': [2, 6],\n",
    "    'beta': [0.001, 0.2, 0.5, 1, 1.5, 2, 10],\n",
    "    \n",
    "    # Convergence\n",
    "    'max_iter_convergence': 20,\n",
    "    'input_seq_len_convergence': 1, ### make experiment first\n",
    "    \n",
    "    # Evaluation\n",
    "    'samples_to_generate': 100,\n",
    "    'distance_metric': 'euclidean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specific_params = {\n",
    "    'vae_conv5_legit': {\n",
    "        'dropout_rate': 0.2\n",
    "    },\n",
    "    'inception_time_wp_vae': {\n",
    "        'n_filters': 32,\n",
    "        'kernel_sizes': [3, 7, 13],\n",
    "        'bottleneck_channels': 32\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_sets = generate_parameter_sets(params, model_specific_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameter_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import papermill as pm\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(notebook_to_execute, output_dir, i, **kwargs):\n",
    "    try:\n",
    "        # Generate output filenames\n",
    "        base_name = os.path.splitext(os.path.basename(notebook_to_execute))[0]\n",
    "        output_notebook = os.path.join(output_dir, f\"{base_name}_execution_{i}.ipynb\")\n",
    "        \n",
    "        # Check if output notebook already exists\n",
    "        if os.path.exists(output_notebook):\n",
    "            logging.info(f\"Skipping execution {i}, output already exists.\")\n",
    "            return i\n",
    "        \n",
    "        # Read the notebook\n",
    "        with open(notebook_to_execute, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "        # Find the parameters cell and update it\n",
    "        params_cell_found = False\n",
    "        for index, cell in enumerate(nb.cells):\n",
    "            if cell.cell_type == 'code' and cell.source.startswith('# parameters'):\n",
    "                # Update the cell source with new parameter values\n",
    "                new_source = \"# parameters\\n\"\n",
    "                for key, value in kwargs.items():\n",
    "                    new_source += f\"{key} = {repr(value)}\\n\"\n",
    "                cell.source = new_source\n",
    "                params_cell_found = True\n",
    "                \n",
    "                # Add a cell to print out and verify the parameters\n",
    "                verify_params_cell = nbformat.v4.new_code_cell(\n",
    "                    source=\"print('Injected parameters:', {\" + \n",
    "                           \", \".join(f\"'{k}': {k}\" for k in kwargs.keys()) + \n",
    "                           \"})\"\n",
    "                )\n",
    "                nb.cells.insert(index + 1, verify_params_cell)\n",
    "                break\n",
    "\n",
    "        if not params_cell_found:\n",
    "            raise ValueError(\"Parameters cell not found in the notebook\")\n",
    "\n",
    "        # Execute only the parameters cell and the verification cell\n",
    "        ep = ExecutePreprocessor(timeout=600, kernel_name='pytorch')\n",
    "        ep.preprocess(nb, {'metadata': {'path': os.path.dirname(notebook_to_execute)}})\n",
    "\n",
    "        # Check if parameters were properly injected\n",
    "        if len(nb.cells) > index + 1 and nb.cells[index + 1].outputs:\n",
    "            injected_params = nb.cells[index + 1].outputs[0].text\n",
    "            logging.info(f\"Injected parameters for execution {i}: {injected_params}\")\n",
    "            \n",
    "            # Verify that all parameters are present\n",
    "            for key in kwargs.keys():\n",
    "                if key not in injected_params:\n",
    "                    raise ValueError(f\"Parameter '{key}' was not properly injected\")\n",
    "        else:\n",
    "            raise ValueError(\"Failed to verify injected parameters\")\n",
    "\n",
    "        # Now execute the rest of the notebook\n",
    "        nb = pm.execute_notebook(\n",
    "            nb,\n",
    "            output_notebook,\n",
    "            parameters={},\n",
    "            kernel_name='pytorch',\n",
    "            timeout=10000\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Completed execution {i}\")\n",
    "        return i\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in execution {i}: {str(e)}\")\n",
    "        logging.error(f\"Parameters used: {kwargs}\")\n",
    "        import traceback\n",
    "        logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def process_parameter_sets(parameter_sets, notebook_to_execute, output_dir, checkpoint_file, max_workers=4):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize or load checkpoint\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "    else:\n",
    "        checkpoint = {'completed': []}\n",
    "    \n",
    "    # Ensure checkpoint is a dictionary with a 'completed' key\n",
    "    if not isinstance(checkpoint, dict) or 'completed' not in checkpoint:\n",
    "        checkpoint = {'completed': []}\n",
    "    \n",
    "    # Filter out already completed executions\n",
    "    remaining_executions = [i for i in range(len(parameter_sets)) if i not in checkpoint['completed']]\n",
    "    \n",
    "    logging.info(f\"Starting execution. {len(remaining_executions)} executions remaining.\")\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for i in remaining_executions:\n",
    "            future = executor.submit(\n",
    "                execute_notebook,\n",
    "                notebook_to_execute=notebook_to_execute,\n",
    "                output_dir=output_dir,\n",
    "                i=i,\n",
    "                **parameter_sets[i]\n",
    "            )\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                logging.info(f\"Execution {result} completed successfully.\")\n",
    "                # Update checkpoint\n",
    "                checkpoint['completed'].append(result)\n",
    "                with open(checkpoint_file, 'w') as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "            else:\n",
    "                logging.warning(\"An execution failed.\")\n",
    "    \n",
    "    logging.info(\"All executions completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data_used': 'EM_N_fix_1500',\n",
       "  'families_to_discard': 2,\n",
       "  'seq_len': 100,\n",
       "  'feature_dim': 7,\n",
       "  'epochs': 50,\n",
       "  'val_split': 0.05,\n",
       "  'batch_size': 32,\n",
       "  'lr': 0.001,\n",
       "  'model_name': 'inception_time_wp_vae',\n",
       "  'latent_dim': 6,\n",
       "  'max_iter_convergence': 20,\n",
       "  'input_seq_len_convergence': 1,\n",
       "  'samples_to_generate': 100,\n",
       "  'distance_metric': 'euclidean',\n",
       "  'model_kwargs': {'n_filters': 32,\n",
       "   'kernel_sizes': [3, 7, 13],\n",
       "   'bottleneck_channels': 32,\n",
       "   'beta': 0.5}},\n",
       " {'data_used': 'EM_N_fix_1500',\n",
       "  'families_to_discard': 0,\n",
       "  'seq_len': 100,\n",
       "  'feature_dim': 7,\n",
       "  'epochs': 50,\n",
       "  'val_split': 0.05,\n",
       "  'batch_size': 32,\n",
       "  'lr': 0.001,\n",
       "  'model_name': 'inception_time_wp_vae',\n",
       "  'latent_dim': 2,\n",
       "  'max_iter_convergence': 20,\n",
       "  'input_seq_len_convergence': 1,\n",
       "  'samples_to_generate': 100,\n",
       "  'distance_metric': 'euclidean',\n",
       "  'model_kwargs': {'n_filters': 32,\n",
       "   'kernel_sizes': [3, 7, 13],\n",
       "   'bottleneck_channels': 32,\n",
       "   'beta': 10}},\n",
       " {'data_used': 'EM_N_fix_1500',\n",
       "  'families_to_discard': 2,\n",
       "  'seq_len': 100,\n",
       "  'feature_dim': 7,\n",
       "  'epochs': 50,\n",
       "  'val_split': 0.05,\n",
       "  'batch_size': 32,\n",
       "  'lr': 0.001,\n",
       "  'model_name': 'vae_conv5_legit',\n",
       "  'latent_dim': 6,\n",
       "  'max_iter_convergence': 20,\n",
       "  'input_seq_len_convergence': 1,\n",
       "  'samples_to_generate': 100,\n",
       "  'distance_metric': 'euclidean',\n",
       "  'model_kwargs': {'dropout_rate': 0.2, 'beta': 2}},\n",
       " {'data_used': 'EM_N_fix_1500',\n",
       "  'families_to_discard': 20,\n",
       "  'seq_len': 100,\n",
       "  'feature_dim': 7,\n",
       "  'epochs': 50,\n",
       "  'val_split': 0.05,\n",
       "  'batch_size': 32,\n",
       "  'lr': 0.001,\n",
       "  'model_name': 'inception_time_wp_vae',\n",
       "  'latent_dim': 6,\n",
       "  'max_iter_convergence': 20,\n",
       "  'input_seq_len_convergence': 1,\n",
       "  'samples_to_generate': 100,\n",
       "  'distance_metric': 'euclidean',\n",
       "  'model_kwargs': {'n_filters': 32,\n",
       "   'kernel_sizes': [3, 7, 13],\n",
       "   'bottleneck_channels': 32,\n",
       "   'beta': 0.2}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select 4 random elements from parameter_sets\n",
    "random_parameter_sets = random.sample(parameter_sets, 4)\n",
    "\n",
    "random_parameter_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 12:33:25,378 - INFO - Starting execution. 140 executions remaining.\n"
     ]
    }
   ],
   "source": [
    "notebook_to_execute = '03_01_generative_discovery.ipynb'\n",
    "output_dir = \"../experiments/03_01_generative_discovery\"\n",
    "checkpoint_file = '../experiments/experiment_checkpoint.json'\n",
    "\n",
    "process_parameter_sets(\n",
    "    parameter_sets,\n",
    "    notebook_to_execute=notebook_to_execute,\n",
    "    output_dir=output_dir,\n",
    "    checkpoint_file=checkpoint_file,\n",
    "    max_workers=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
