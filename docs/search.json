[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Utils",
    "section": "",
    "text": "source\n\n\n\n load_orbit_data (file_path:str, variable_name:Optional[str]=None,\n                  dataset_path:Optional[str]=None)\n\nLoad orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .mat, .h5, or .npy file.\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nAny\n\nThe loaded orbit data.\n\n\n\n\nsource\n\n\n\n\n load_memmap_array (file_path:str, mode:str='c')\n\nLoad a .npy file as a memory-mapped array using numpy.memmap.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .npy file as a string.\n\n\nmode\nstr\nc\nMode for memory-mapping (‘r’, ‘r+’, ‘w+’, ‘c’).\n\n\nReturns\nmemmap\n\nReturns a memory-mapped array.\n\n\n\n\nsource\n\n\n\n\n get_orbit_features (file_path:str, variable_name:Optional[str]=None,\n                     dataset_path:Optional[str]=None)\n\nLoad orbit feature data from a specified file and convert it to a DataFrame.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the file (can be .mat, .h5, or .npy).\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nDataFrame\n\nDataFrame with detailed orbit features.",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#loading-data",
    "href": "data.html#loading-data",
    "title": "Data Utils",
    "section": "",
    "text": "source\n\n\n\n load_orbit_data (file_path:str, variable_name:Optional[str]=None,\n                  dataset_path:Optional[str]=None)\n\nLoad orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .mat, .h5, or .npy file.\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nAny\n\nThe loaded orbit data.\n\n\n\n\nsource\n\n\n\n\n load_memmap_array (file_path:str, mode:str='c')\n\nLoad a .npy file as a memory-mapped array using numpy.memmap.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .npy file as a string.\n\n\nmode\nstr\nc\nMode for memory-mapping (‘r’, ‘r+’, ‘w+’, ‘c’).\n\n\nReturns\nmemmap\n\nReturns a memory-mapped array.\n\n\n\n\nsource\n\n\n\n\n get_orbit_features (file_path:str, variable_name:Optional[str]=None,\n                     dataset_path:Optional[str]=None)\n\nLoad orbit feature data from a specified file and convert it to a DataFrame.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the file (can be .mat, .h5, or .npy).\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nDataFrame\n\nDataFrame with detailed orbit features.",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#save-data",
    "href": "data.html#save-data",
    "title": "Data Utils",
    "section": "Save Data",
    "text": "Save Data\n\nsource\n\nsave_data\n\n save_data (data:numpy.ndarray, file_name:str)\n\nSave a numpy array to a file based on the file extension specified in file_name. Supports saving to HDF5 (.hdf5) or NumPy (.npy) file formats.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe numpy array data to save.\n\n\nfile_name\nstr\nThe name of the file to save the data in, including the extension.\n\n\nReturns\nNone",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#get-example-data",
    "href": "data.html#get-example-data",
    "title": "Data Utils",
    "section": "Get Example Data",
    "text": "Get Example Data\n\nsource\n\nget_example_orbit_data\n\n get_example_orbit_data ()\n\nLoad example orbit data from a numpy file located in the example_data directory.\n\ndata = get_example_orbit_data()\ndata.shape\n\n(400, 7, 100)",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#order-labels-and-array-given-target",
    "href": "data.html#order-labels-and-array-given-target",
    "title": "Data Utils",
    "section": "Order labels and array given target",
    "text": "Order labels and array given target\n\nsource\n\norder_labels_and_array_with_target\n\n order_labels_and_array_with_target (labels:numpy.ndarray,\n                                     array:numpy.ndarray,\n                                     target_label:str,\n                                     place_at_end:bool=False)\n\nOrders labels and array by placing entries with target_label either at start or end.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlabels\nndarray\n\nArray of labels to be ordered\n\n\narray\nndarray\n\nArray to be ordered according to labels\n\n\ntarget_label\nstr\n\nLabel to order by\n\n\nplace_at_end\nbool\nFalse\nWhether to place target label at end\n\n\nReturns\ntuple\n\nReturns ordered labels and array\n\n\n\n\n# Sample labels and a sample 3D array\nlabels = np.array(['apple', 'banana', 'apple', 'orange', 'banana', 'grape'])\narray = np.array([[[1, 2], [3, 4]], \n                  [[5, 6], [7, 8]], \n                  [[9, 10], [11, 12]], \n                  [[13, 14], [15, 16]], \n                  [[17, 18], [19, 20]], \n                  [[21, 22], [23, 24]]])\ntarget_label = 'apple'\n\nordered_labels, ordered_array = order_labels_and_array_with_target(labels, array, target_label)\n\nprint(ordered_labels)\nprint(ordered_array)\n\n['apple' 'apple' 'banana' 'orange' 'banana' 'grape']\n[[[ 1  2]\n  [ 3  4]]\n\n [[ 9 10]\n  [11 12]]\n\n [[ 5  6]\n  [ 7  8]]\n\n [[13 14]\n  [15 16]]\n\n [[17 18]\n  [19 20]]\n\n [[21 22]\n  [23 24]]]",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#random-sampler",
    "href": "data.html#random-sampler",
    "title": "Data Utils",
    "section": "Random Sampler",
    "text": "Random Sampler\n\nsource\n\nsample_orbits\n\n sample_orbits (orbit_data:numpy.ndarray, sample_spec:Union[dict,int],\n                labels:Optional[numpy.ndarray]=None)\n\nRandomly sample orbits from the provided dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\nArray of orbit data with shape (num_orbits, 6, num_time_points)\n\n\nsample_spec\nUnion\n\nNumber of samples per class (dict) or total samples (int)\n\n\nlabels\nOptional\nNone\nArray of labels for each orbit\n\n\nReturns\ntuple",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#random-discarder",
    "href": "data.html#random-discarder",
    "title": "Data Utils",
    "section": "Random Discarder",
    "text": "Random Discarder\n\nsource\n\ndiscard_random_labels\n\n discard_random_labels (data:numpy.ndarray, labels:numpy.ndarray,\n                        discard_labels:Union[List,Dict,int])\n\n*Discards random or specified labels from the dataset.\nReturns tuple of (discarded labels, filtered data, filtered labels).*\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nDataset to filter\n\n\nlabels\nndarray\nLabels corresponding to the data\n\n\ndiscard_labels\nUnion\nLabels to discard - list, dict or number\n\n\nReturns\nTuple",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#remove-duplicates-preserve-order",
    "href": "data.html#remove-duplicates-preserve-order",
    "title": "Data Utils",
    "section": "Remove Duplicates preserve Order",
    "text": "Remove Duplicates preserve Order\n\nsource\n\nremove_duplicates_preserve_order\n\n remove_duplicates_preserve_order (input_list:List)\n\nRemoves duplicate items from a list while preserving the original order.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_list\nList\nInput list that may contain duplicates\n\n\nReturns\nList\nReturns list with duplicates removed while preserving order",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#dataloaders",
    "href": "data.html#dataloaders",
    "title": "Data Utils",
    "section": "Dataloaders",
    "text": "Dataloaders\n\nsource\n\ncreate_dataloaders\n\n create_dataloaders (scaled_data:torch.Tensor, val_split:float=0.2,\n                     batch_size:int=32)\n\nCreates train and validation dataloaders from input tensor data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscaled_data\nTensor\n\nInput tensor of scaled data\n\n\nval_split\nfloat\n0.2\nFraction of data to use for validation\n\n\nbatch_size\nint\n32\nBatch size for dataloaders\n\n\nReturns\nTuple\n\nReturns train and optional val dataloaders",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#scaler",
    "href": "data.html#scaler",
    "title": "Data Utils",
    "section": "Scaler",
    "text": "Scaler\n/usr/local/lib/python3.10/dist-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Parameters:\n  else: warn(msg)\n/usr/local/lib/python3.10/dist-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Attributes:\n  else: warn(msg)\n\nsource\n\nTSFeatureWiseScaler\n\n TSFeatureWiseScaler (feature_range:tuple=(0, 1))\n\nScales time series data feature-wise using PyTorch tensors.\n\nsource\n\n\nTSGlobalScaler\n\n TSGlobalScaler ()\n\nScales time series data globally using PyTorch tensors.",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "source\n\n\n\n get_orbit_data_from_hdf5 (file_path:str)\n\nLoad orbit data from an HDF5 file.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nTuple\nDictionary of orbits with numerical keys.",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#read-data",
    "href": "dataset.html#read-data",
    "title": "Dataset",
    "section": "",
    "text": "source\n\n\n\n get_orbit_data_from_hdf5 (file_path:str)\n\nLoad orbit data from an HDF5 file.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nTuple\nDictionary of orbits with numerical keys.",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-features",
    "href": "dataset.html#get-features",
    "title": "Dataset",
    "section": "Get Features",
    "text": "Get Features\n\nOrbit Features\n\nsource\n\n\nget_orbit_features_from_hdf5\n\n get_orbit_features_from_hdf5 (file_path:str)\n\nLoad orbit DataFrame from an HDF5 file.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nDataFrame\nDataFrame containing orbit features.\n\n\n\n\nsource\n\n\nget_orbit_features_from_folder\n\n get_orbit_features_from_folder (folder_path:str)\n\nConcatenate orbit DataFrames from all HDF5 files in a folder, preserving original index and adding system column.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfolder_path\nstr\nPath to the folder\n\n\nReturns\nDataFrame\nDataFrame containing concatenated orbit features.\n\n\n\n\n\nSystem Features\n\ndef get_system_data_from_hdf5(file_path: str              # Path to the HDF5 file.\n                             ) -&gt; Dict[str, float]:       # Dictionary containing system features.\n    \"\"\"\n    Load system data from an HDF5 file.\n    \"\"\"\n    with h5py.File(file_path, 'r') as file:\n        # Extract system features and labels\n        system_features = file['system_features'][:]\n        system_labels = file['system_labels'][:].astype(str)\n        \n        # Create a dictionary for system\n        system_dict = {label: feature[0] for label, feature in zip(system_labels.flatten().tolist(), system_features)}\n        \n    return system_dict\n\n\ndef get_system_features_from_folder(folder_path: str    # Path to the folder\n                                   ) -&gt; pd.DataFrame:   # DataFrame containing concatenated system features.\n    \"\"\"\n    Concatenate system DataFrames from all HDF5 files in a folder, preserving original index and adding system column.\n    \"\"\"\n    all_systems = []  # List to store individual system dictionaries\n\n    # Iterate over all files in the folder\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n\n        # Check if the file is an HDF5 file\n        if file_name.endswith('.h5') or file_name.endswith('.hdf5'):\n            # Get the system dictionary from the HDF5 file\n            system_dict = get_system_data_from_hdf5(file_path)\n            \n            # Add a new entry to the dictionary for the system name\n            system_dict['system'] = os.path.splitext(file_name)[0].split('_')[0]\n            \n            # Append the dictionary to the list\n            all_systems.append(system_dict)\n\n    # Convert the list of dictionaries to a DataFrame\n    concatenated_df = pd.DataFrame(all_systems)\n    \n    return concatenated_df",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-classes",
    "href": "dataset.html#get-classes",
    "title": "Dataset",
    "section": "Get Classes",
    "text": "Get Classes\n\nsource\n\nsubstitute_values_from_df\n\n substitute_values_from_df (values:List[Any],\n                            df:pandas.core.frame.DataFrame,\n                            goal_column:str, id_column:str='Id')\n\n*Substitute values in the given list based on the mapping from a DataFrame’s id column to goal column.\nParameters: values (List[Any]): List of values to be substituted. df (pd.DataFrame): DataFrame containing the mapping from id_column to goal_column. goal_column (str): Column in the DataFrame to get the substitution values from. id_column (str, optional): Column in the DataFrame to match the values with. Default is ‘Id’.\nReturns: List[Any]: A list with substituted values from the DataFrame’s goal_column.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalues\nList\n\nList of values to be substituted.\n\n\ndf\nDataFrame\n\nDataFrame containing the mapping.\n\n\ngoal_column\nstr\n\nColumn in the DataFrame to get the substitution values from.\n\n\nid_column\nstr\nId\nColumn in the DataFrame to match the values with. Default is ‘Id’.\n\n\nReturns\nList\n\n\n\n\n\n\nsource\n\n\nget_orbit_classes\n\n get_orbit_classes (values:List[Any])\n\nGet orbit classes by substituting values with their corresponding Label, Type, Subtype and Direction.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nvalues\nList\nList of values to be substituted with orbit classifications\n\n\nReturns\nTuple\n\n\n\n\n\nvalues = [1,7,23]\nget_orbit_classes(values)\n\n(['S_BN', 'S_L1_A', 'S_L4_LP'],\n ['System-wide', 'L1', 'L4'],\n ['Butterfly', 'Axial', 'Long Period'],\n ['North', 'No specification', 'No specification'])",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-periods",
    "href": "dataset.html#get-periods",
    "title": "Dataset",
    "section": "Get Periods",
    "text": "Get Periods\n\nsource\n\nget_periods_of_orbit_dict\n\n get_periods_of_orbit_dict (orbits:Dict[int,numpy.ndarray],\n                            propagated_periods:Dict[int,int],\n                            desired_periods:int)\n\nProcess the orbits to extract the desired periods and print the percentage of the dataset returned.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\npropagated_periods\nDict\nDictionary of propagated periods for each orbit.\n\n\ndesired_periods\nint\nDesired number of periods.\n\n\nReturns\nDict\nProcessed dictionary of orbits.",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-dataset",
    "href": "dataset.html#get-dataset",
    "title": "Dataset",
    "section": "Get Dataset",
    "text": "Get Dataset\n\nFixed Period\n\nsource\n\n\nget_first_period_of_fixed_period_dataset\n\n get_first_period_of_fixed_period_dataset (file_path:str)\n\nLoad and process orbit data from an HDF5 file for the first period.\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nTuple\n3D numpy array of padded orbits.\n\n\n\n\n\nFixed Step\n\nsource\n\n\nget_full_fixed_step_dataset\n\n get_full_fixed_step_dataset (file_path:str, segment_length:int)\n\nLoad and process orbit data from an HDF5 file, segmenting each orbit into specified length.\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nsegment_length\nint\nDesired length of each segment.\n\n\nReturns\nTuple\n3D numpy array of segmented orbits.\n\n\n\n\nsource\n\n\nget_first_period_fixed_step_dataset\n\n get_first_period_fixed_step_dataset (file_path:str, segment_length:int)\n\nLoad and process orbit data from an HDF5 file, segmenting each orbit into specified length.\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nsegment_length\nint\nDesired length of each segment.\n\n\nReturns\nTuple\n3D numpy array of segmented orbits.\n\n\n\n\n\nFirst Period\n\nsource\n\n\nget_first_period_dataset\n\n get_first_period_dataset (file_path:str,\n                           segment_length:Optional[int]=100)\n\nLoad orbit data based on the file path. Calls the appropriate function depending on the name of the file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nPath to the HDF5 file.\n\n\nsegment_length\nOptional\n100\nDesired length of each segment, optional.\n\n\nReturns\nTuple\n\nMemmap of segmented orbits.\n\n\n\n\nsource\n\n\nget_first_period_dataset_all_systems\n\n get_first_period_dataset_all_systems (folder_path:str,\n                                       segment_length:Optional[int]=100)\n\nProcesses all system files in a folder, concatenates their data while maintaining order.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfolder_path\nstr\n\nPath to the folder containing system files\n\n\nsegment_length\nOptional\n100\nDesired length of each segment\n\n\nReturns\nTuple\n\nConcatenated orbits as a 3D NumPy memmap",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-constants",
    "href": "dataset.html#get-constants",
    "title": "Dataset",
    "section": "Get Constants",
    "text": "Get Constants\n\nsource\n\nget_system_constants\n\n get_system_constants (system_dict:Dict[str,float],\n                       system_labels:numpy.ndarray, constant:str)\n\nExtracts values for a specified constant from a system dictionary based on system labels.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsystem_dict\nDict\nDictionary containing system constants for different systems\n\n\nsystem_labels\nndarray\nArray of system labels\n\n\nconstant\nstr\nThe constant to extract (e.g., ‘mu’, ‘LU’, etc.)\n\n\nReturns\nndarray",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "experiment.html",
    "href": "experiment.html",
    "title": "Experiment",
    "section": "",
    "text": "source\n\n\n\n\n setup_new_experiment (params:Dict[str,Any], experiments_folder:str,\n                       json_file:Optional[str]=None)\n\nSets up a new experiment by creating a new folder and updating the JSON file with experiment parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nDict\n\nDictionary of parameters for the new experiment.\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters.\n\n\nReturns\nstr\n\nThe path to the newly created experiment folder.\n\n\n\n\nsource\n\n\n\n\n create_experiments_json (parameter_sets, output_file='experiments.json')\n\n*Create an experiments.json file from given parameter sets.\nArgs: parameter_sets (list): List of dictionaries containing parameters for each experiment. output_file (str): Name of the output JSON file. Defaults to ‘experiments.json’.\nReturns: None*\n\n\n\n\nsource\n\n\n\n\n convert_numpy_types (obj)\n\nRecursively convert numpy types and tensors to native Python types for JSON serialization.\n\nsource\n\n\n\n\n add_experiment_metrics (experiments_folder:str,\n                         params:Optional[Dict[str,Any]]=None,\n                         experiment_id:Optional[int]=None,\n                         metrics:Optional[Dict[str,Any]]=None,\n                         json_file:Optional[str]=None)\n\nAdds metrics to an existing experiment in the JSON file based on the given parameters or ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nparams\nOptional\nNone\nOptional dictionary of parameters identifying the experiment.\n\n\nexperiment_id\nOptional\nNone\nOptional ID to identify the experiment.\n\n\nmetrics\nOptional\nNone\nOptional dictionary of metrics to be added to the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_parameters (experiments_folder:str, experiment_id:int,\n                            json_file:Optional[str]=None)\n\nRetrieves the parameters of an experiment from the JSON file based on the given ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nexperiment_id\nint\n\nID to identify the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nDict\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_data (experiments_folder:str, experiment_id:int,\n                      json_file:Optional[str]=None)\n\nRetrieves all data for an experiment from the JSON file based on the given ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments\n\n\nexperiment_id\nint\n\nID to identify the experiment\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment data\n\n\nReturns\nDict\n\n\n\n\n\n\nsource\n\n\n\n\n read_json_to_dataframe (json_path:str)\n\nReads a JSON file containing experiment results and returns a DataFrame.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\njson_path\nstr\nPath to the JSON file containing experiment results\n\n\nReturns\nDataFrame",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#json-management",
    "href": "experiment.html#json-management",
    "title": "Experiment",
    "section": "",
    "text": "source\n\n\n\n\n setup_new_experiment (params:Dict[str,Any], experiments_folder:str,\n                       json_file:Optional[str]=None)\n\nSets up a new experiment by creating a new folder and updating the JSON file with experiment parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nDict\n\nDictionary of parameters for the new experiment.\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters.\n\n\nReturns\nstr\n\nThe path to the newly created experiment folder.\n\n\n\n\nsource\n\n\n\n\n create_experiments_json (parameter_sets, output_file='experiments.json')\n\n*Create an experiments.json file from given parameter sets.\nArgs: parameter_sets (list): List of dictionaries containing parameters for each experiment. output_file (str): Name of the output JSON file. Defaults to ‘experiments.json’.\nReturns: None*\n\n\n\n\nsource\n\n\n\n\n convert_numpy_types (obj)\n\nRecursively convert numpy types and tensors to native Python types for JSON serialization.\n\nsource\n\n\n\n\n add_experiment_metrics (experiments_folder:str,\n                         params:Optional[Dict[str,Any]]=None,\n                         experiment_id:Optional[int]=None,\n                         metrics:Optional[Dict[str,Any]]=None,\n                         json_file:Optional[str]=None)\n\nAdds metrics to an existing experiment in the JSON file based on the given parameters or ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nparams\nOptional\nNone\nOptional dictionary of parameters identifying the experiment.\n\n\nexperiment_id\nOptional\nNone\nOptional ID to identify the experiment.\n\n\nmetrics\nOptional\nNone\nOptional dictionary of metrics to be added to the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_parameters (experiments_folder:str, experiment_id:int,\n                            json_file:Optional[str]=None)\n\nRetrieves the parameters of an experiment from the JSON file based on the given ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nexperiment_id\nint\n\nID to identify the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nDict\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_data (experiments_folder:str, experiment_id:int,\n                      json_file:Optional[str]=None)\n\nRetrieves all data for an experiment from the JSON file based on the given ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments\n\n\nexperiment_id\nint\n\nID to identify the experiment\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment data\n\n\nReturns\nDict\n\n\n\n\n\n\nsource\n\n\n\n\n read_json_to_dataframe (json_path:str)\n\nReads a JSON file containing experiment results and returns a DataFrame.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\njson_path\nstr\nPath to the JSON file containing experiment results\n\n\nReturns\nDataFrame",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#data-management",
    "href": "experiment.html#data-management",
    "title": "Experiment",
    "section": "Data Management",
    "text": "Data Management\n\nGet Paths\n\nsource\n\n\ngenerate_image_paths\n\n generate_image_paths (folder_prefix:str, unique_ids:list[int],\n                       file_suffix:str)\n\nGenerates a list of image file paths based on experiment IDs and folder structure.\n\n\n\n\nType\nDetails\n\n\n\n\nfolder_prefix\nstr\nBase folder path prefix for each experiment\n\n\nunique_ids\nlist\nList of experiment IDs\n\n\nfile_suffix\nstr\nSuffix to append to the generated filenames\n\n\nReturns\nlist\n\n\n\n\n\n\nGet Orbits\n\nsource\n\n\nconcatenate_orbits_from_experiment_folder\n\n concatenate_orbits_from_experiment_folder (experiments_folder:str,\n                                            seq_len:int, file_suffix:str='\n                                            _generated_orbits')\n\nConcatenates orbit data from multiple experiment folders into a single array.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nRoot folder containing experiment subfolders\n\n\nseq_len\nint\n\nExpected sequence length of orbit data\n\n\nfile_suffix\nstr\n_generated_orbits\nSuffix for orbit data files\n\n\nReturns\nndarray\n\n\n\n\n\n\n\nGet Tables\n\nsource\n\n\nconcatenate_csvs_from_experiment_folder\n\n concatenate_csvs_from_experiment_folder (experiments_folder:str,\n                                          file_suffix:str)\n\nConcatenates CSV files from multiple experiment folders into a single DataFrame.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nexperiments_folder\nstr\nRoot folder containing experiment subfolders\n\n\nfile_suffix\nstr\nSuffix for CSV files to concatenate\n\n\nReturns\nDataFrame\n\n\n\n\n\n\nGet Orbit and Table\n\nsource\n\n\nconcatenate_and_check_orbits_from_experiment_folder\n\n concatenate_and_check_orbits_from_experiment_folder\n                                                      (experiments_folder:\n                                                      str, csv_file_name:s\n                                                      tr='_refined_orbits_\n                                                      df.csv', np_file_nam\n                                                      e:str='_refined_orbi\n                                                      ts')\n\nConcatenates orbit data from multiple experiment folders and checks for consistency between generated and refined orbits.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nRoot folder containing experiment subfolders\n\n\ncsv_file_name\nstr\n_refined_orbits_df.csv\nSuffix for CSV files containing refined orbits\n\n\nnp_file_name\nstr\n_refined_orbits\nSuffix for numpy files containing generated orbits\n\n\nReturns\nTuple",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#parameters",
    "href": "experiment.html#parameters",
    "title": "Experiment",
    "section": "Parameters",
    "text": "Parameters\n\nsource\n\ngenerate_parameter_sets\n\n generate_parameter_sets (params:Dict[str,Union[Any,List[Any]]],\n                          model_specific_params:Dict[str,Dict])\n\nGenerates all possible parameter combinations from the given parameter sets and merges them with model-specific parameters.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nparams\nDict\nDictionary of parameter names and their values/value lists\n\n\nmodel_specific_params\nDict\nDictionary mapping model names to their specific parameters\n\n\nReturns\nList",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#experiment-execution",
    "href": "experiment.html#experiment-execution",
    "title": "Experiment",
    "section": "Experiment Execution",
    "text": "Experiment Execution\n\nsource\n\nexecute_parameter_notebook\n\n execute_parameter_notebook (notebook_to_execute:str, output_dir:str,\n                             i:int, params:Dict,\n                             extra_parameters:Optional[Dict]=None,\n                             checkpoint_file:Optional[str]=None)\n\nExecutes a Jupyter notebook with given parameters and saves the output. Returns the execution index if successful, None if failed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnotebook_to_execute\nstr\n\nPath to the notebook file to execute\n\n\noutput_dir\nstr\n\nDirectory to save output notebook\n\n\ni\nint\n\nExecution index\n\n\nparams\nDict\n\nParameters to pass to the notebook\n\n\nextra_parameters\nOptional\nNone\nAdditional parameters to merge with params\n\n\ncheckpoint_file\nOptional\nNone\nPath to checkpoint file\n\n\nReturns\nOptional\n\n\n\n\n\n\nsource\n\n\nparalelize_notebook_experiment\n\n paralelize_notebook_experiment (parameter_sets:List[Dict],\n                                 notebook_to_execute:str, output_dir:str,\n                                 checkpoint_file:str, max_workers:int=3,\n                                 extra_parameters:Optional[Dict]=None)\n\nExecutes a Jupyter notebook multiple times in parallel with different parameter sets, tracking progress with checkpoints.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparameter_sets\nList\n\nList of parameter dictionaries to execute\n\n\nnotebook_to_execute\nstr\n\nPath to the notebook file to execute\n\n\noutput_dir\nstr\n\nDirectory to save execution outputs\n\n\ncheckpoint_file\nstr\n\nPath to checkpoint file tracking execution progress\n\n\nmax_workers\nint\n3\nMaximum number of parallel workers\n\n\nextra_parameters\nOptional\nNone\nAdditional parameters to pass to each execution\n\n\nReturns\nNone",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#functions",
    "href": "experiment.html#functions",
    "title": "Experiment",
    "section": "Functions",
    "text": "Functions\n\nsource\n\ngenerate_file_paths\n\n generate_file_paths (experiment_id:Union[int,str], images_folder:str,\n                      experiment_folder:str)\n\nGenerate a dictionary of file paths for an experiment.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nexperiment_id\nUnion\nUnique ID of the experiment\n\n\nimages_folder\nstr\nFolder path where image files are stored\n\n\nexperiment_folder\nstr\nFolder path where experiment-related files are stored\n\n\nReturns\nDict\n\n\n\n\n\nsource\n\n\nprepare_experiment_data\n\n prepare_experiment_data (params:Dict[str,Any], experiments_folder:str,\n                          data_path:str, want_to_discover:bool)\n\nPrepare the experiment data based on the provided parameters and configurations.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nparams\nDict\nDictionary containing experiment parameters\n\n\nexperiments_folder\nstr\nFolder where experiments are stored\n\n\ndata_path\nstr\nPath to the dataset file\n\n\nwant_to_discover\nbool\nFlag indicating whether to discover new families or use existing ones\n\n\nReturns\nTuple\n\n\n\n\n\nsource\n\n\nprepare_and_train_model\n\n prepare_and_train_model (params:Dict[str,Any], scaled_data:torch.Tensor,\n                          experiments_folder:str, experiment_id:int,\n                          file_paths:Dict[str,str], want_to_train:bool)\n\nPrepare the model and either train it or load a pre-trained version based on the provided parameters.\n\n\n\n\nType\nDetails\n\n\n\n\nparams\nDict\nDictionary containing all experiment parameters\n\n\nscaled_data\nTensor\nScaled data tensor for training/validation\n\n\nexperiments_folder\nstr\nFolder where experiments are stored\n\n\nexperiment_id\nint\nUnique ID of current experiment\n\n\nfile_paths\nDict\nDictionary of paths for model/metrics files\n\n\nwant_to_train\nbool\nWhether to train model or load pre-trained\n\n\nReturns\nModule",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "model_factory.html",
    "href": "model_factory.html",
    "title": "Model Factory",
    "section": "",
    "text": "source\n\nget_model\n\n get_model (params)",
    "crumbs": [
      "Model Factory"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "orbit-generation",
    "section": "",
    "text": "This library has been built using nbdev, which means that the source code of the library is stored in Jupyter notebooks inside the nbs folder. These notebooks are then automatically converted into Python files inside the orbit_generation folder.\nApart from the library, we have included research experiments using the library in the nbs_experiments folder.\nFirst, we will review the library structure, functions, and finally, we will explain the experiments conducted.",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "orbit-generation",
    "section": "Publications",
    "text": "Publications\n\nSPAICE 2024: Generative Design of Periodic Orbits in the Restricted Three-Body Problem\nAAS 2025: Generative Astrodynamics: Trajectory Analysis and Design in the Restricted Three-Body Problem",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "orbit-generation",
    "section": "Structure",
    "text": "Structure\n.\n|-- .devcontainer: Configuration for the development container.  \n|  \n|-- .github: workflows for continuous integration in Git.  \n|  \n|-- data: The folder where datasets are stored, in GitHub only example data is included. \n|   |-- example_data\n|   |-- orbits_fix_1500 (needs to be downloaded)\n|\n|-- docs: Documentation files for the project. \n|\n|-- experiments: Results about the experiments performed. \n|\n|-- index_files: Auto-generated images to be used in the README.md. \n| \n|-- julia: Scripts written in Julia for specific computations. \n|   `-- convergence_algorithm.jl\n|\n|-- models: Some Machine Learning models used in the experiments.\n|   |-- family_classificators\n|   |-- orbit_generators\n|\n|-- nbs: Jupyter notebooks containing the source code for the library.  \n|   |-- 00_constants.ipynb\n|   |-- 01_data.ipynb\n|   |-- 02_orbit_processing.ipynb\n|   |-- 03_visualization.ipynb\n|   |-- 04_orbit_statistics.ipynb\n|   |-- 05_dataset.ipynb\n|   |-- 06_architectures.ipynb\n|   |-- 07_propagation.ipynb\n|   |-- 08_experiment.ipynb\n|   |-- 09_evaluation.ipynb\n|   |-- 10_vae.ipynb\n|   |-- 11_model_factory.ipynb\n|   |-- 12_convergence.ipynb\n|   |-- 13_latent_space.ipynb\n|   |-- 14_paper_specific.ipynb\n|   |-- index.ipynb\n|\n|-- nbs_experiments: Notebooks with research experiments using the library.  \n|   |-- 01_generative_discovery_em\n|\n|-- orbit_generation: Auto-generated Python package containing the processed library code. \n|  \n|-- .gitignore: Defines files Git should ignore.\n|  \n|-- LICENSE: Project license file.  \n|  \n|-- MANIFEST.in: Specifies which files to include in the package distribution.   \n|  \n|-- README.md: Documentation for the project, auto-generated from index.ipynb.  \n|  \n|-- settings.ini: Configuration file for `nbdev`.  \n|  \n|-- setup.py: Script for installing the package.  \n\n## Library Modules\n\n```sh\npip install orbit_generation\n\n0. Constants\nThis module contains physical constants and orbit labels.\n\nfrom orbit_generation.constants import MU_BY_SYSTEM, EM_POINTS, EM_MU\n\n\nMU_BY_SYSTEM\n\n{'SaE': 1.901109735892602e-07,\n 'MP': 1.611081404409632e-08,\n 'SaT': 0.0002366393158331484,\n 'EM': 0.01215058560962404,\n 'JE': 2.52801752854e-05,\n 'SE': 3.0542e-06,\n 'SM': 3.227154996101724e-07}\n\n\n\nEM_POINTS\n\n{'Moon': (0.987849414390376, 0, 0),\n 'Earth': (-0.01215058560962404, 0, 0),\n 'Lagrange 1': (0.8369, 0, 0),\n 'Lagrange 2': (1.1557, 0, 0),\n 'Lagrange 3': (-1.0051, 0, 0),\n 'Lagrange 4': (0.4879, 0.866, 0),\n 'Lagrange 5': (0.4879, -0.866, 0)}\n\n\n\n\n1. Data\nThis module provides utilities for handling orbit data.\n\nfrom orbit_generation.data import get_example_orbit_data\n\n\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n(400, 7, 100)\n\n\n\nNumber of orbits: 400\nTime instants: 100\n\nEvery orbit dataset is organized within a three-dimensional NumPy array with the following structure:\n\ndata.shape = (num_orbits, 7, num_time_points)\n\n\nnum_orbits: Total number of distinct orbits in the dataset.\n\n7: Represents the seven scalar values for each orbit at each time point, typically including:\n\ntime: The time corresponding to each recorded state.\n\nposX, posY, posZ: Position components in the X, Y, and Z dimensions, respectively.\n\nvelX, velY, velZ: Velocity components in the X, Y, and Z dimensions, respectively.\n\n\nnum_time_points: Number of time instants at which the data for each orbit is recorded.\n\n\n\n2. Processing\nThis module performs various processing tasks on the orbit data described above, including downsampling, interpolation, and reshaping.\n\nfrom orbit_generation.processing import resample_3d_array\n\n\nresampled_orbit_data = resample_3d_array(data=orbit_data, axis=2, target_size= 50)\nresampled_orbit_data.shape\n\n(400, 7, 50)\n\n\n\nInitial time instants: 100\nTime instants after Resampling: 50\n\n\n\n3. Visualization\nThis module handles the visualization of orbit trajectories and their features.\n\nOrbits\n\nfrom orbit_generation.visualize import visualize_static_orbits, export_dynamic_orbits_html\n\n\nvisualize_static_orbits(orbit_data, show_legend=False)\n\n\n\n\n\n\n\n\n\nvisualize_static_orbits(data= orbit_data, orbit_indices=[315,120,70,180,190], point_dict=EM_POINTS)\n\n\n\n\n\n\n\n\n\nvisualize_static_orbits(data= orbit_data, time_instants=[0,50], orbit_indices=[40], plot_reference_box=False)\n\n\n\n\n\n\n\n\n\nexport_dynamic_orbits_html(data=orbit_data, filename='../data/example_training_data/example_orbits_visualization.html')\n\nVisualization saved to ../data/example_training_data/example_orbits_visualization.html\n\n\nSee the dynamic orbit visualziation here\n\n\nFeatures\nExample plots performed with this module:\n\n\n\ndistribution_rediscovered_discarded_fams\n\n\n\n\n\nproportions_em\n\n\n\n\n\n4. Statistics\nThis module analyzes the orbital data using descriptive statistics.\n\nfrom orbit_generation.stats import plot_histograms_position, plot_histograms_comparison\n\n\nplot_histograms_position(orbit_data)\n\n\n\n\n\n\n\n\n\nplot_histograms_comparison(orbit_data[0:200,1:], orbit_data[200:400,1:], normalize=True)\n\n\n\n\n\n\n\n\n\n\n5. Dataset\nScripts to build the different datasets used for modeling.\nThe following function was used to obtain the datasets for the research:\n\nfrom orbit_generation.dataset import get_first_period_dataset\n\nThe function takes: - file_path: Path to HDF5 file containing orbit data - segment_length: Optional length to segment orbits into (default 100)\nReturns: - Segmented orbits as memory-mapped array - DataFrame with orbit features - Array of segment IDs - Dictionary of system features (mass ratios, etc)\n\n\n6. Architectures\nModule where different model architectures are defined—such as losses, decoders, and callbacks—designed to be modular and reusable.\n\nfrom orbit_generation.architectures import Conv5Encoder\n\n\nmodel = Conv5Encoder(\n    seq_len=100,      # Sequence length for input data\n    feat_dim=7,       # Feature dimension (number of features per timestep)\n    latent_dim=32,    # Dimension of latent space\n    dropout_rate=0.1  # Dropout rate for regularization\n)\nprint(\"Conv5Encoder Summary:\")\nprint(model._modules) # Access modules dictionary since it extends nn.Module\n\nConv5Encoder Summary:\n{'convo_layers': Sequential(\n  (0): Conv1d(7, 64, kernel_size=(3,), stride=(1,), padding=same)\n  (1): ReLU()\n  (2): Dropout(p=0.1, inplace=False)\n  (3): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=same)\n  (4): ReLU()\n  (5): Dropout(p=0.1, inplace=False)\n  (6): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=same)\n  (7): ReLU()\n  (8): Dropout(p=0.1, inplace=False)\n  (9): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=same)\n  (10): ReLU()\n  (11): Dropout(p=0.1, inplace=False)\n  (12): Conv1d(64, 64, kernel_size=(13,), stride=(1,), padding=same)\n  (13): ReLU()\n  (14): Dropout(p=0.1, inplace=False)\n  (15): Flatten(start_dim=1, end_dim=-1)\n), 'dense_layers': Sequential(\n  (0): Linear(in_features=6400, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=512, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=64, bias=True)\n)}\n\n\n\n\n7. Propagation\nThis module enables orbit propagation and measures physical consistency.\n\nfrom orbit_generation.propagation import calculate_errors\n\n\nerrors = calculate_errors(orbit_data, EM_MU, orbit_indices = [0])\n\nCumulative position error for selected orbits: 5.095048112547387e-05\nAverage position error per time step: 5.14651324499736e-07\n\n\n\n\n\n\n\n\n\nCumulative velocity error for selected orbits: 0.0001793751728135454\nAverage velocity error per time step: 1.8118704324600546e-06\n\n\n\n\n\n\n\n\n\nCumulative energy error for selected orbits: 0.0003757476806640625\nAverage energy error per time step: 3.7954312119836686e-06\n\n\n\n\n\n\n\n\n\n\n\n8. Experiment Tools\nScripts to manage data and results and parallelize experiments used in the nbs_experiments folder.\n\n\n9. Evaluation\nModule to evaluate distances, clustering metrics and vanilla machine learning models.\n\nfrom orbit_generation.evaluation import calculate_pairwise_orbit_distances\n\n\ncalculate_pairwise_orbit_distances(orbit_data[0:1], orbit_data[1:2], distance_metric='manhattan')\n\narray([85.13618469])\n\n\n\n\n10. Variational Auto Encoder (VAE)\nThis module builds VAE models using the architecures from 6. Architectures\n\nfrom orbit_generation.vae import BetaVAE\n\n\nprint(\"BetaVAE properties:\")\nfor key, value in BetaVAE.__dict__.items():\n    if not key.startswith('_'):\n        print(f\"- {key}\")\n\nBetaVAE properties:\n- setup\n- encode\n- decode\n- forward\n- reconstruction_loss_by_axis\n- default_loss_fn\n- training_step\n- validation_step\n- configure_optimizers\n- sample\n- on_train_epoch_start\n- on_validation_epoch_start\n\n\n\n\n11. Model Factory\nModule to create model instances based on configuration files.\n\nfrom orbit_generation.model_factory import get_model\nimport torch\n\n\nparams = {\n    'model_name': 'inception_time_vae',\n    'seq_len': 100,\n    'feature_dim': 6,\n    'latent_dim': 32,\n    'model_kwargs': {\n        'beta': 1.0,\n        'lr': 0.001,\n        'optimizer_cls': torch.optim.Adam\n    }\n}\n\nmodel = get_model(params)\n\n\n\n12. Convergence\nScript to perform a Multiple Shooting algorithm on the synthetic orbits to process differential corrections until they become physically valid.\nExample of synthetic orbits (left) and its refined versions (right):\n\n\n\nSampling Generations\n\n\n\n\n13. Latent Space\nModule to explore a model’s latent space, visualize it, apply dimensionality reduction techniques, and perform sampling.\nExamples of plots produced with this module:\n\n\n\nexp3_latent_space\n\n\n\n\n\ncombined_latent_spaces\n\n\n\n\n\nexp3_regressions",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "index.html#generative-discovery-experiment",
    "href": "index.html#generative-discovery-experiment",
    "title": "orbit-generation",
    "section": "Generative Discovery Experiment",
    "text": "Generative Discovery Experiment\n\n\n\nexp_diagram\n\n\n\n0. Figures\nExported images of the experiments below.\n\n\n1. Exploratory Data Analysis of Earth-Moon Periodic Orbits\nThis notebook explores the dataset by visualizing the orbits, the proportions within families, the initial conditions, and the distribution of features.\n\n\n2. Orbit Family Analysis\nThis notebook searches for the best properties of the orbits that define their family through clustering and machine learning techniques.\n\n\n3. Generative Discovery\nThese notebooks (03_01_generative_discovery, 03_02_experiment_parameterization, 03_03_discovery_results, 03_04_ml_classification_discovery, 03_05_distance_discovery) perform the “Generative Discovery” experiment by training a VAE on a subset of the dataset, generating orbits and refining them, obtaining metrics and visual explorations throughout the entire process. The first notebooks are used for running the experiments and the later ones for analyzing the results.\n\n\n4. Convergence Analysis\nThis notebook performs a convergence analysis of the synthetic orbits generated by the VAE, studying the effect of certain parameters for the multiple shooting algorithm.\n\n\n5. Model Analysis\nThis notebook performs an analysis of a specific model to deepen the understanding of the model’s behavior.",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "convergence.html",
    "href": "convergence.html",
    "title": "Convergence",
    "section": "",
    "text": "source\n\n\n\n differential_correction (orbit:numpy.ndarray, μ:float,\n                          variable_time:bool=True,\n                          time_flight:Optional[float]=None,\n                          jacobi_constant:Optional[float]=None,\n                          X_end:Optional[numpy.ndarray]=None,\n                          tol:float=1e-09, max_iter:int=20,\n                          printout:bool=False,\n                          DX_0:Optional[numpy.ndarray]=None,\n                          X_big_0:Optional[numpy.ndarray]=None,\n                          δ:Optional[float]=None)\n\nPerforms differential correction on an orbit using Julia implementation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit\nndarray\n\nOrbit data with shape [num_timesteps, 7]\n\n\nμ\nfloat\n\nGravitational parameter\n\n\nvariable_time\nbool\nTrue\nWhether to use variable time nodes\n\n\ntime_flight\nOptional\nNone\nTotal time of flight\n\n\njacobi_constant\nOptional\nNone\nJacobi constant\n\n\nX_end\nOptional\nNone\nTerminal state vector (shape: [6])\n\n\ntol\nfloat\n1e-09\nTolerance for convergence\n\n\nmax_iter\nint\n20\nMaximum number of iterations\n\n\nprintout\nbool\nFalse\nWhether to print iteration logs\n\n\nDX_0\nOptional\nNone\nInitial guess for state vector correction\n\n\nX_big_0\nOptional\nNone\nAuxiliary initial guess\n\n\nδ\nOptional\nNone\nStep size or perturbation parameter\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\n\n\n create_converged_orbits_df (converged_indices:List[int],\n                             orbit_array:numpy.ndarray,\n                             converged_orbits:numpy.ndarray,\n                             errors:numpy.ndarray,\n                             iterations:numpy.ndarray)\n\nCreates a DataFrame containing detailed information about converged orbits.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nconverged_indices\nList\nList of orbit indices that have converged\n\n\norbit_array\nndarray\nOriginal array containing all orbit data\n\n\nconverged_orbits\nndarray\nArray containing corrected converged orbits\n\n\nerrors\nndarray\nArray of norm values for each converged orbit\n\n\niterations\nndarray\nArray of iteration counts for each converged orbit\n\n\nReturns\nDataFrame\n\n\n\n\n\nsource\n\n\n\n\n process_diferential_correction_orbits (orbit_array:numpy.ndarray,\n                                        μ:float, variable_time:bool=True,\n                                        tol:float=1e-09, max_iter:int=20,\n                                        printout:bool=False)\n\nProcesses a set of orbits by providing the orbit array directly to differential correction.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_array\nndarray\n\nArray containing orbit data with shape [num_orbits, num_timesteps, 7]. First element in last dimension is time\n\n\nμ\nfloat\n\nGravitational parameter\n\n\nvariable_time\nbool\nTrue\nWhether to use variable time nodes for correction\n\n\ntol\nfloat\n1e-09\nTolerance for convergence in differential correction\n\n\nmax_iter\nint\n20\nMaximum number of iterations for differential correction\n\n\nprintout\nbool\nFalse\nWhether to print iteration logs\n\n\nReturns\ntuple",
    "crumbs": [
      "Convergence"
    ]
  },
  {
    "objectID": "convergence.html#julia-wrapper",
    "href": "convergence.html#julia-wrapper",
    "title": "Convergence",
    "section": "",
    "text": "source\n\n\n\n differential_correction (orbit:numpy.ndarray, μ:float,\n                          variable_time:bool=True,\n                          time_flight:Optional[float]=None,\n                          jacobi_constant:Optional[float]=None,\n                          X_end:Optional[numpy.ndarray]=None,\n                          tol:float=1e-09, max_iter:int=20,\n                          printout:bool=False,\n                          DX_0:Optional[numpy.ndarray]=None,\n                          X_big_0:Optional[numpy.ndarray]=None,\n                          δ:Optional[float]=None)\n\nPerforms differential correction on an orbit using Julia implementation.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit\nndarray\n\nOrbit data with shape [num_timesteps, 7]\n\n\nμ\nfloat\n\nGravitational parameter\n\n\nvariable_time\nbool\nTrue\nWhether to use variable time nodes\n\n\ntime_flight\nOptional\nNone\nTotal time of flight\n\n\njacobi_constant\nOptional\nNone\nJacobi constant\n\n\nX_end\nOptional\nNone\nTerminal state vector (shape: [6])\n\n\ntol\nfloat\n1e-09\nTolerance for convergence\n\n\nmax_iter\nint\n20\nMaximum number of iterations\n\n\nprintout\nbool\nFalse\nWhether to print iteration logs\n\n\nDX_0\nOptional\nNone\nInitial guess for state vector correction\n\n\nX_big_0\nOptional\nNone\nAuxiliary initial guess\n\n\nδ\nOptional\nNone\nStep size or perturbation parameter\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\n\n\n create_converged_orbits_df (converged_indices:List[int],\n                             orbit_array:numpy.ndarray,\n                             converged_orbits:numpy.ndarray,\n                             errors:numpy.ndarray,\n                             iterations:numpy.ndarray)\n\nCreates a DataFrame containing detailed information about converged orbits.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nconverged_indices\nList\nList of orbit indices that have converged\n\n\norbit_array\nndarray\nOriginal array containing all orbit data\n\n\nconverged_orbits\nndarray\nArray containing corrected converged orbits\n\n\nerrors\nndarray\nArray of norm values for each converged orbit\n\n\niterations\nndarray\nArray of iteration counts for each converged orbit\n\n\nReturns\nDataFrame\n\n\n\n\n\nsource\n\n\n\n\n process_diferential_correction_orbits (orbit_array:numpy.ndarray,\n                                        μ:float, variable_time:bool=True,\n                                        tol:float=1e-09, max_iter:int=20,\n                                        printout:bool=False)\n\nProcesses a set of orbits by providing the orbit array directly to differential correction.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_array\nndarray\n\nArray containing orbit data with shape [num_orbits, num_timesteps, 7]. First element in last dimension is time\n\n\nμ\nfloat\n\nGravitational parameter\n\n\nvariable_time\nbool\nTrue\nWhether to use variable time nodes for correction\n\n\ntol\nfloat\n1e-09\nTolerance for convergence in differential correction\n\n\nmax_iter\nint\n20\nMaximum number of iterations for differential correction\n\n\nprintout\nbool\nFalse\nWhether to print iteration logs\n\n\nReturns\ntuple",
    "crumbs": [
      "Convergence"
    ]
  },
  {
    "objectID": "architectures.html",
    "href": "architectures.html",
    "title": "Architectures",
    "section": "",
    "text": "source\n\n\n\n Sampling (*args, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#sampling",
    "href": "architectures.html#sampling",
    "title": "Architectures",
    "section": "",
    "text": "source\n\n\n\n Sampling (*args, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#callback",
    "href": "architectures.html#callback",
    "title": "Architectures",
    "section": "Callback",
    "text": "Callback\n\nsource\n\nVAELossHistory\n\n VAELossHistory ()\n\n*Abstract base class used to build new callbacks.\nSubclass this class and override any of the relevant hooks*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#vaes-encoders-and-decoders",
    "href": "architectures.html#vaes-encoders-and-decoders",
    "title": "Architectures",
    "section": "VAEs Encoders and Decoders",
    "text": "VAEs Encoders and Decoders\n\nsource\n\nVAEEncoder\n\n VAEEncoder (latent_dim:int)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nVAEDecoder\n\n VAEDecoder (latent_dim:int)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\nSimple Convolutional Architecture\n\nKernel Sizes: 5, 7, 9, 13\n\nEncoder\n\nsource\n\n\n\n\nConv5Encoder\n\n Conv5Encoder (seq_len:int, feat_dim:int, latent_dim:int,\n               dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nDecoder\n\nsource\n\n\n\nConv5Decoder\n\n Conv5Decoder (seq_len:int, feat_dim:int, latent_dim:int,\n               dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nGetter\n\nsource\n\n\n\nget_conv5_vae_components\n\n get_conv5_vae_components (seq_len:int, feat_dim:int, latent_dim:int,\n                           dropout_rate:float=0.2)\n\nCreates and returns encoder and decoder components for a convolutional VAE architecture.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseq_len\nint\n\nLength of input sequence\n\n\nfeat_dim\nint\n\nDimensionality of input features\n\n\nlatent_dim\nint\n\nDimensionality of the latent space\n\n\ndropout_rate\nfloat\n0.2\nDropout rate for regularization\n\n\nReturns\ntuple\n\n\n\n\n\n\nLegit Tsgm\n\nEncoder\n\nsource\n\n\n\n\nConv5EncoderLegitTsgm\n\n Conv5EncoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                        dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nDecoder\n\nsource\n\n\n\nConv5DecoderLegitTsgm\n\n Conv5DecoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                        dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nGetter\n\nsource\n\n\n\nget_conv5_legit_tsgm_vae_components\n\n get_conv5_legit_tsgm_vae_components (seq_len:int, feat_dim:int,\n                                      latent_dim:int,\n                                      dropout_rate:float=0.2)\n\nCreates and returns encoder and decoder components for a Conv5 VAE model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseq_len\nint\n\nLength of input sequence\n\n\nfeat_dim\nint\n\nDimensionality of input features\n\n\nlatent_dim\nint\n\nDimensionality of the latent space\n\n\ndropout_rate\nfloat\n0.2\nDropout rate for regularization\n\n\nReturns\ntuple\n\n\n\n\n\n\n\nInception Time\nThe implemetation in the following cell is taken from https://github.com/TheMrGhostman/InceptionTime-Pytorch/blob/master/inception.py , the next cell is an adjustment for our problem\n\nsource\n\n\nInceptionBlock\n\n InceptionBlock (in_channels, n_filters=32, kernel_sizes=[9, 19, 39],\n                 bottleneck_channels=32, use_residual=True,\n                 activation=ReLU(), return_indices=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInception\n\n Inception (in_channels, n_filters, kernel_sizes=[9, 19, 39],\n            bottleneck_channels=32, activation=ReLU(),\n            return_indices=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\npass_through\n\n pass_through (X)\n\n\nsource\n\n\ncorrect_sizes\n\n correct_sizes (sizes)\n\n\nsource\n\n\nInceptionTransposeBlockWithoutPool\n\n InceptionTransposeBlockWithoutPool (in_channels, out_channels=32,\n                                     kernel_sizes=[9, 19, 39],\n                                     bottleneck_channels=32,\n                                     use_residual=True, activation=ReLU())\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInceptionTransposeWithoutPool\n\n InceptionTransposeWithoutPool (in_channels, out_channels,\n                                kernel_sizes=[9, 19, 39],\n                                bottleneck_channels=32, activation=ReLU())\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInceptionBlockWithoutPool\n\n InceptionBlockWithoutPool (in_channels, n_filters=32, kernel_sizes=[9,\n                            19, 39], bottleneck_channels=32,\n                            use_residual=True, activation=ReLU(),\n                            return_indices=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInceptionWithoutPool\n\n InceptionWithoutPool (in_channels, n_filters, kernel_sizes=[9, 19, 39],\n                       bottleneck_channels=32, activation=ReLU())\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nEncoder\n\nsource\n\n\n\nInceptionTimeVAEEncoder\n\n InceptionTimeVAEEncoder (feat_dim=7, seq_len=100, n_filters=32,\n                          kernel_sizes=[5, 11, 23],\n                          bottleneck_channels=32, latent_dim=2)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nWPInceptionTimeVAEEncoder\n\n WPInceptionTimeVAEEncoder (feat_dim=7, seq_len=100, n_filters=32,\n                            kernel_sizes=[5, 11, 23],\n                            bottleneck_channels=32, latent_dim=2)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nDecoder\n\nsource\n\n\n\nWPInceptionTimeVAEDecoder\n\n WPInceptionTimeVAEDecoder (feat_dim=7, seq_len=100, n_filters=32,\n                            kernel_sizes=[5, 11, 23],\n                            bottleneck_channels=32, latent_dim=2)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nGetter\n\nsource\n\n\n\nget_inception_time_vae_components\n\n get_inception_time_vae_components (seq_len:int, feat_dim:int,\n                                    latent_dim:int,\n                                    without_pooling:bool=True,\n                                    **model_kwargs:dict)\n\nReturns encoder and decoder components for an InceptionTime-based VAE architecture.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseq_len\nint\n\nLength of input sequence\n\n\nfeat_dim\nint\n\nDimensionality of input features\n\n\nlatent_dim\nint\n\nDimensionality of the latent space\n\n\nwithout_pooling\nbool\nTrue\nIf True, returns WPInceptionTimeVAEEncoder instead of InceptionTimeVAEEncoder\n\n\nmodel_kwargs\ndict\n\n\n\n\nReturns\ntuple\n\nDictionary containing model-specific keyword arguments",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#cvaes-encoders-and-decoders",
    "href": "architectures.html#cvaes-encoders-and-decoders",
    "title": "Architectures",
    "section": "cVAEs Encoders and Decoders",
    "text": "cVAEs Encoders and Decoders\n\nsource\n\ncVAEEncoder\n\n cVAEEncoder (latent_dim:int)\n\nAbstract base for a conditional VAE encoder: Encodes data + condition into z_mean, z_log_var\n\nsource\n\n\ncVAEDecoder\n\n cVAEDecoder (latent_dim:int)\n\nAbstract base for a conditional VAE decoder: Decodes z + condition into reconstructed data\n\n\nSimple Convolutions\n\nsource\n\n\ncConv5EncoderLegitTsgm\n\n cConv5EncoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                         cond_dim:int, dropout_rate:float)\n\nAbstract base for a conditional VAE encoder: Encodes data + condition into z_mean, z_log_var\n\nsource\n\n\ncConv5DecoderLegitTsgm\n\n cConv5DecoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                         cond_dim:int, dropout_rate:float)\n\nAbstract base for a conditional VAE decoder: Decodes z + condition into reconstructed data\n\nsource\n\n\nget_conditional_conv5_legit_tsgm_vae_components\n\n get_conditional_conv5_legit_tsgm_vae_components (seq_len:int,\n                                                  feat_dim:int,\n                                                  latent_dim:int,\n                                                  dropout_rate:float=0.2,\n                                                  cond_dim:int=1)\n\nCreates encoder and decoder components for a conditional convolutional VAE.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseq_len\nint\n\nLength of input sequence\n\n\nfeat_dim\nint\n\nDimensionality of input features\n\n\nlatent_dim\nint\n\nDimensionality of the latent space\n\n\ndropout_rate\nfloat\n0.2\nDropout rate for regularization\n\n\ncond_dim\nint\n1\nDimensionality of conditional input\n\n\nReturns\ntuple",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "source\n\n\n\n plot_3d_points (data:numpy.ndarray, labels:Optional[List[str]]=None,\n                 plot_velocity:bool=True, arrow_width:float=0.005,\n                 show_legend:bool=True, figsize:tuple=(10, 8))\n\nPlots each point in space with a 3D arrow based on the first 3 coordinates (position) and optionally the next 3 coordinates (velocity).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nArray of shape (samples, 3) for positions or (samples, 6) for positions and velocities.\n\n\nlabels\nOptional\nNone\nOptional list of labels for color coding the points.\n\n\nplot_velocity\nbool\nTrue\nIf True and velocities are provided, plot arrows representing velocity vectors.\n\n\narrow_width\nfloat\n0.005\nWidth of the arrows.\n\n\nshow_legend\nbool\nTrue\nIf True, show the legend for color coding.\n\n\nfigsize\ntuple\n(10, 8)\nSize of the figure in inches (width, height).\n\n\nReturns\nNone",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#points",
    "href": "visualization.html#points",
    "title": "Visualization",
    "section": "",
    "text": "source\n\n\n\n plot_3d_points (data:numpy.ndarray, labels:Optional[List[str]]=None,\n                 plot_velocity:bool=True, arrow_width:float=0.005,\n                 show_legend:bool=True, figsize:tuple=(10, 8))\n\nPlots each point in space with a 3D arrow based on the first 3 coordinates (position) and optionally the next 3 coordinates (velocity).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nArray of shape (samples, 3) for positions or (samples, 6) for positions and velocities.\n\n\nlabels\nOptional\nNone\nOptional list of labels for color coding the points.\n\n\nplot_velocity\nbool\nTrue\nIf True and velocities are provided, plot arrows representing velocity vectors.\n\n\narrow_width\nfloat\n0.005\nWidth of the arrows.\n\n\nshow_legend\nbool\nTrue\nIf True, show the legend for color coding.\n\n\nfigsize\ntuple\n(10, 8)\nSize of the figure in inches (width, height).\n\n\nReturns\nNone",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#orbits",
    "href": "visualization.html#orbits",
    "title": "Visualization",
    "section": "Orbits",
    "text": "Orbits\n\nStatic\n\nsource\n\n\nvisualize_static_orbits\n\n visualize_static_orbits (data:numpy.ndarray,\n                          time_instants:Optional[List[int]]=None,\n                          orbit_indices:Optional[List[int]]=None,\n                          point_dict:Optional[Dict[str,tuple]]=None,\n                          show_legend:bool=True,\n                          save_path:Optional[str]=None,\n                          plot_reference_box:bool=True,\n                          title:Optional[str]=None,\n                          orbit_names:Optional[List[str]]=None,\n                          equal_aspect:bool=False)\n\nVisualizes orbits in 3D space and highlights specified time instants for each selected orbit. If data has 7 scalars instead of 6, the first scalar (assumed to be time) is removed.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe orbit data with shape (num_orbits, 6 or 7, num_time_points).\n\n\ntime_instants\nOptional\nNone\nTime points to highlight; defaults to None.\n\n\norbit_indices\nOptional\nNone\nIndices of orbits to visualize; defaults to all.\n\n\npoint_dict\nOptional\nNone\nDictionary of extra points to plot.\n\n\nshow_legend\nbool\nTrue\nFlag to indicate whether to show a legend.\n\n\nsave_path\nOptional\nNone\nPath to save the figure; defaults to None.\n\n\nplot_reference_box\nbool\nTrue\nFlag to indicate whether to plot the reference box.\n\n\ntitle\nOptional\nNone\nCustom title for the plot.\n\n\norbit_names\nOptional\nNone\nCustom names for orbits; defaults to “Orbit {index}”.\n\n\nequal_aspect\nbool\nFalse\nFlag to enforce equal scaling for all axes.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nvisualize_orbits_minimal\n\n visualize_orbits_minimal (data:numpy.ndarray,\n                           orbit_indices:Optional[List[int]]=None,\n                           time_instants:Optional[List[int]]=None,\n                           save_path:Optional[str]=None)\n\nVisualizes orbits in 3D space with a completely blank background (no axes, no labels, no grid).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe orbit data with shape (num_orbits, 6, num_time_points).\n\n\norbit_indices\nOptional\nNone\nIndices of orbits to visualize; defaults to all.\n\n\ntime_instants\nOptional\nNone\nTime points to highlight; defaults to None.\n\n\nsave_path\nOptional\nNone\nPath to save the figure; defaults to None.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nvisualize_orbits_comparison\n\n visualize_orbits_comparison (data1:numpy.ndarray, data2:numpy.ndarray,\n                              title1:Optional[str]='Set 1',\n                              title2:Optional[str]='Set 2',\n                              equal_aspect:bool=False, title_size:int=18,\n                              title_pad:float=20.0,\n                              shared_scale:bool=False, wspace:float=0.3)\n\nVisualizes two sets of orbits side by side in 3D space.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata1\nndarray\n\nFirst set of orbit data with shape (num_orbits, 6, num_time_points)\n\n\ndata2\nndarray\n\nSecond set of orbit data with shape (num_orbits, 6, num_time_points)\n\n\ntitle1\nOptional\nSet 1\nTitle for the first plot\n\n\ntitle2\nOptional\nSet 2\nTitle for the second plot\n\n\nequal_aspect\nbool\nFalse\nFlag to enforce equal scaling for all axes\n\n\ntitle_size\nint\n18\nFont size for the plot titles\n\n\ntitle_pad\nfloat\n20.0\nPadding between plot and title in points\n\n\nshared_scale\nbool\nFalse\nIf True, both plots will share the same scale and limits\n\n\nwspace\nfloat\n0.3\nWidth spacing between subplots\n\n\nReturns\nNone\n\n\n\n\n\n\nfrom orbit_generation.data import get_example_orbit_data\nfrom orbit_generation.constants import EM_POINTS\n\n\norbit_data= get_example_orbit_data()\norbit_data.shape\n\n(400, 7, 100)\n\n\n\nvisualize_static_orbits(data= orbit_data, orbit_indices=[315,120,70,180,190], point_dict=EM_POINTS)\n\n\n\n\n\n\n\n\n\nvisualize_static_orbits(data= orbit_data, time_instants=[0,50], orbit_indices=[40], plot_reference_box=False)\n\n\n\n\n\n\n\n\n\n\nDynamic\n\nsource\n\n\nexport_dynamic_orbits_html\n\n export_dynamic_orbits_html (data:numpy.ndarray,\n                             time_instants:Optional[List[int]]=None,\n                             orbit_indices:Optional[List[int]]=None,\n                             point_dict:Optional[Dict[str,tuple]]=None,\n                             filename:str='orbits.html')\n\nGenerates an interactive 3D visualization of orbits and saves it as an HTML file, including the ability to highlight specific time instants and show named points.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nOrbit data as a 3D numpy array (num_orbits, 6, num_time_points).\n\n\ntime_instants\nOptional\nNone\nTime instants to highlight.\n\n\norbit_indices\nOptional\nNone\nIndices of orbits to visualize.\n\n\npoint_dict\nOptional\nNone\nNamed points as a dict with 3D coordinates.\n\n\nfilename\nstr\norbits.html\nPath and name of the file to save the HTML plot.\n\n\nReturns\nNone\n\n\n\n\n\n\nexport_dynamic_orbits_html(data=orbit_data, filename='../data/example_training_data/example_orbits_visualization.html')\n\nVisualization saved to ../data/example_training_data/example_orbits_visualization.html\n\n\nView Orbit Visualization",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#orbit-features",
    "href": "visualization.html#orbit-features",
    "title": "Visualization",
    "section": "Orbit Features",
    "text": "Orbit Features\n\nsource\n\nplot_histogram\n\n plot_histogram (data:Union[list,numpy.ndarray,pandas.core.series.Series],\n                 bins:int=10, title:str='Histogram', xlabel:str='Data',\n                 ylabel:str='Frequency')\n\nPlots a histogram for the given data.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nUnion\n\nThe data to be plotted\n\n\nbins\nint\n10\nNumber of histogram bins to use\n\n\ntitle\nstr\nHistogram\nTitle of the histogram\n\n\nxlabel\nstr\nData\nLabel for the x-axis\n\n\nylabel\nstr\nFrequency\nLabel for the y-axis\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nplot_grouped_features\n\n plot_grouped_features (df:pandas.core.frame.DataFrame, columns:List[str],\n                        group_col:str, plot_type:str, figsize:tuple=(5,\n                        5), fontsize:int=10)\n\nGroup the DataFrame by a specified column and plot the specified type of plot for each column for each group.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataFrame containing the data.\n\n\ncolumns\nList\n\nList of column names to plot.\n\n\ngroup_col\nstr\n\nColumn name to group by.\n\n\nplot_type\nstr\n\nType of plot: ‘violin’, ‘box’, ‘facetgrid’, or ‘histogram’\n\n\nfigsize\ntuple\n(5, 5)\nSize of the plot (width, height) per subplot\n\n\nfontsize\nint\n10\nFont size for labels and titles\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nplot_value_proportions\n\n plot_value_proportions (data, grid:str='horizontal',\n                         show_percentages:bool=True,\n                         show_labels:bool=True,\n                         percentage_font_size:int=10,\n                         label_distance:float=1.1,\n                         pct_distance:float=0.85,\n                         explode_factor:float=0.1)\n\nCount occurrences of each unique value in data and plot the proportions in pie charts.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nList or array of labels to plot\n\n\ngrid\nstr\nhorizontal\nOption to plot in grid (horizontal, vertical, or square) or separate images.\n\n\nshow_percentages\nbool\nTrue\nOption to print or not print percentages.\n\n\nshow_labels\nbool\nTrue\nOption to print or not print labels.\n\n\npercentage_font_size\nint\n10\nFont size for percentages.\n\n\nlabel_distance\nfloat\n1.1\nDistance of labels from center.\n\n\npct_distance\nfloat\n0.85\nDistance of percentages from center.\n\n\nexplode_factor\nfloat\n0.1\nFactor to separate slices.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nplot_mean_distance_by_group_column\n\n plot_mean_distance_by_group_column (df, group_column, value_column)",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#dataframe",
    "href": "visualization.html#dataframe",
    "title": "Visualization",
    "section": "Dataframe",
    "text": "Dataframe\n\nCorrelation Matrix\n\nsource\n\n\nplot_corr_matrix\n\n plot_corr_matrix (dataframe:pandas.core.frame.DataFrame,\n                   figsize:tuple=(14, 10), cmap:str='coolwarm',\n                   save_path:Optional[str]=None)\n\nPlots a correlation matrix heatmap with annotations.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataframe\nDataFrame\n\nThe DataFrame containing the data to be analyzed.\n\n\nfigsize\ntuple\n(14, 10)\nThe size of the figure (width, height).\n\n\ncmap\nstr\ncoolwarm\nThe color map to be used for the heatmap.\n\n\nsave_path\nOptional\nNone\nThe path to save the plot image. If None, the plot is not saved.\n\n\nReturns\nNone\n\n\n\n\n\n\n\nFeatures Summary\n\nsource\n\n\nsummarize_and_test\n\n summarize_and_test (df:pandas.core.frame.DataFrame, group_col:str,\n                     features:List[str]=None, visualize:bool=True,\n                     figsize:tuple=(10, 40),\n                     plot_significant_only:bool=True,\n                     plot_continuous:bool=False, show_values:bool=True)",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#other",
    "href": "visualization.html#other",
    "title": "Visualization",
    "section": "Other",
    "text": "Other\n\nImages\n\nsource\n\n\nplot_single_image\n\n plot_single_image (image_path:str, crop_length:int=0, font_size:int=17,\n                    save_path:Optional[str]=None, figsize:tuple=(15, 15),\n                    title:Optional[str]=None)\n\nPlot a single image with customization options.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_path\nstr\n\nPath to the image file.\n\n\ncrop_length\nint\n0\nNumber of pixels to crop from each side of the image.\n\n\nfont_size\nint\n17\nFont size for the title.\n\n\nsave_path\nOptional\nNone\nPath to save the plotted image. If None, the image is not saved.\n\n\nfigsize\ntuple\n(15, 15)\nSize of the figure (width, height).\n\n\ntitle\nOptional\nNone\n\n\n\nReturns\nNone\n\nTitle for the image.\n\n\n\n\nsource\n\n\ncreate_image_grid_from_routes\n\n create_image_grid_from_routes (image_routes:list, crop_length:int=0,\n                                font_size:int=12,\n                                save_path:Optional[str]=None,\n                                grid_size:tuple=(3, 2),\n                                hspace:float=-0.37,\n                                label_images:Optional[list]=None)\n\nCreate a grid of images from a list of image paths.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_routes\nlist\n\nList of image file paths.\n\n\ncrop_length\nint\n0\nNumber of pixels to crop from each side of the image.\n\n\nfont_size\nint\n12\nFont size for the experiment label.\n\n\nsave_path\nOptional\nNone\nPath to save the generated grid image. If None, the grid is not saved.\n\n\ngrid_size\ntuple\n(3, 2)\nNumber of rows and columns in the grid.\n\n\nhspace\nfloat\n-0.37\nVertical spacing between grid rows.\n\n\nlabel_images\nOptional\nNone\n\n\n\nReturns\nNone\n\nList of labels for images or a boolean to add default labels.",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "orbit_processing.html",
    "href": "orbit_processing.html",
    "title": "Processing",
    "section": "",
    "text": "source\n\n\n\n\n downsample_3d_array (data:numpy.ndarray, axis:int, hop:int=None,\n                      target_size:int=None)\n\nDownsample a 3D numpy array along a specified axis by keeping only every hop-th element or to a target size.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe original 3D array to be downsampled.\n\n\naxis\nint\n\nThe axis along which to perform the downsampling.\n\n\nhop\nint\nNone\nThe interval at which to keep elements.\n\n\ntarget_size\nint\nNone\nThe target size for the specified axis.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n resample_3d_array (data:numpy.ndarray, axis:int, target_size:int)\n\nResample a 3D numpy array along a specified axis using linear interpolation.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be resampled.\n\n\naxis\nint\nThe axis along which to perform the interpolation.\n\n\ntarget_size\nint\nThe new size of the axis after resampling.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n average_downsample_3d_array (data:numpy.ndarray, axis:int,\n                              target_size:int)\n\nDownsample a 3D numpy array along a specified axis using averaging.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be downsampled.\n\n\naxis\nint\nThe axis along which to perform the downsampling (0, 1, or 2).\n\n\ntarget_size\nint\nThe desired size of the specified axis after downsampling.\n\n\nReturns\nndarray",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#resampling",
    "href": "orbit_processing.html#resampling",
    "title": "Processing",
    "section": "",
    "text": "source\n\n\n\n\n downsample_3d_array (data:numpy.ndarray, axis:int, hop:int=None,\n                      target_size:int=None)\n\nDownsample a 3D numpy array along a specified axis by keeping only every hop-th element or to a target size.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe original 3D array to be downsampled.\n\n\naxis\nint\n\nThe axis along which to perform the downsampling.\n\n\nhop\nint\nNone\nThe interval at which to keep elements.\n\n\ntarget_size\nint\nNone\nThe target size for the specified axis.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n resample_3d_array (data:numpy.ndarray, axis:int, target_size:int)\n\nResample a 3D numpy array along a specified axis using linear interpolation.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be resampled.\n\n\naxis\nint\nThe axis along which to perform the interpolation.\n\n\ntarget_size\nint\nThe new size of the axis after resampling.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n average_downsample_3d_array (data:numpy.ndarray, axis:int,\n                              target_size:int)\n\nDownsample a 3D numpy array along a specified axis using averaging.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be downsampled.\n\n\naxis\nint\nThe axis along which to perform the downsampling (0, 1, or 2).\n\n\ntarget_size\nint\nThe desired size of the specified axis after downsampling.\n\n\nReturns\nndarray",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#reorder-orbit-with-time",
    "href": "orbit_processing.html#reorder-orbit-with-time",
    "title": "Processing",
    "section": "Reorder Orbit with Time",
    "text": "Reorder Orbit with Time\n\nsource\n\nreorder_orbits\n\n reorder_orbits (orbit_dataset:numpy.ndarray)\n\nReorders the time steps of each orbit in the dataset such that the time values are always incrementally increasing. Returns the reordered dataset, a 2D array of metric values for each orbit, and a list of metric labels.\n\n\n\n\nType\nDetails\n\n\n\n\norbit_dataset\nndarray\n\n\n\nReturns\nTuple\n3D numpy array of reordered orbits.",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#reshaping-arrays",
    "href": "orbit_processing.html#reshaping-arrays",
    "title": "Processing",
    "section": "Reshaping Arrays",
    "text": "Reshaping Arrays\n\nsource\n\npad_and_convert_to_3d\n\n pad_and_convert_to_3d (orbits:Dict[int,numpy.ndarray], timesteps:int)\n\nTruncate and pad each orbit to a uniform length and convert to a 3D numpy array.\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\ntimesteps\nint\nDesired number of timesteps.\n\n\nReturns\nndarray\n3D numpy array of padded orbits.\n\n\n\n\nsource\n\n\nsegment_and_convert_to_3d\n\n segment_and_convert_to_3d (orbits:Dict[int,numpy.ndarray],\n                            segment_length:int)\n\nDivide each orbit into segments of a given length and convert to a 3D numpy array.\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\nsegment_length\nint\nDesired length of each segment.\n\n\nReturns\nTuple\n3D numpy array of segments.\n\n\n\n\norbits = {\n    1: np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n                    [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n                    [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],\n                    [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],\n                    [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n                    [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]]),\n    2: np.array([[73, 74, 75, 76, 77, 78, 79],\n                    [81, 82, 83, 84, 85, 86, 87],\n                    [89, 90, 91, 92, 93, 94, 95],\n                    [97, 98, 99, 100, 101, 102, 103],\n                    [105, 106, 107, 108, 109, 110, 111],\n                    [113, 114, 115, 116, 117, 118, 119]])\n}\nsegment_length = 4\n\n# Expected segments and IDs\nexpected_segments = np.array([\n    [[1, 2, 3, 4], [13, 14, 15, 16], [25, 26, 27, 28], [37, 38, 39, 40], [49, 50, 51, 52], [61, 62, 63, 64]],\n    [[5, 6, 7, 8], [17, 18, 19, 20], [29, 30, 31, 32], [41, 42, 43, 44], [53, 54, 55, 56], [65, 66, 67, 68]],\n    [[9, 10, 11, 12], [21, 22, 23, 24], [33, 34, 35, 36], [45, 46, 47, 48], [57, 58, 59, 60], [69, 70, 71, 72]],\n    [[73, 74, 75, 76], [81, 82, 83, 84], [89, 90, 91, 92], [97, 98, 99, 100], [105, 106, 107, 108], [113, 114, 115, 116]]\n])\nexpected_ids = [1, 1, 1, 2]\n\n# Call the function\nsegments_3d, segment_ids = segment_and_convert_to_3d(orbits, segment_length)\n\n# Use test_eq to check the results\ntest_eq(segments_3d.tolist(), expected_segments.tolist())\ntest_eq(segment_ids, expected_ids)",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#add-time-vector",
    "href": "orbit_processing.html#add-time-vector",
    "title": "Processing",
    "section": "Add Time Vector",
    "text": "Add Time Vector\n\nsource\n\nadd_time_vector_to_orbits\n\n add_time_vector_to_orbits (orbits:Dict[int,numpy.ndarray],\n                            propagated_periods:List[float],\n                            periods:List[float])\n\nAdd a time vector to each orbit in the dictionary.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\npropagated_periods\nList\nList of propagated periods for each orbit.\n\n\nperiods\nList\nList of periods for each orbit.\n\n\nReturns\nDict\nDictionary of updated orbits with time vectors added.",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#interpolating-equal-times",
    "href": "orbit_processing.html#interpolating-equal-times",
    "title": "Processing",
    "section": "Interpolating Equal Times",
    "text": "Interpolating Equal Times\n\nsource\n\ninterpolate_equal_times\n\n interpolate_equal_times (orbit_dataset:numpy.ndarray)\n\n\n# Testing with dummy data that has equal time values at the beginning\ntest_data = np.array([\n    [0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n])\n\n# Reshape to 3D array (1 orbit with 3 scalars and 10 timesteps)\ntest_data = test_data.reshape(1, 3, 10)\n\noutput = interpolate_equal_times(test_data)\nprint(output[:,0])\n\n[[0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.    2.   ]]",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_statistics.html",
    "href": "orbit_statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "from orbit_generation.data import get_example_orbit_data\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n(400, 7, 100)",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_statistics.html#simple-statistics",
    "href": "orbit_statistics.html#simple-statistics",
    "title": "Statistics",
    "section": "Simple statistics",
    "text": "Simple statistics\n\nsource\n\ncalculate_overall_spatial_statistics\n\n calculate_overall_spatial_statistics (orbits:numpy.ndarray)\n\nCalculate the overall min, mean, max, and percentile statistics for each scalar (position and velocity in X, Y, Z) across all time instants and orbits.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nndarray\nArray of shape (number_of_orbits, 6 or 7, number_of_time_instants) containing orbit data\n\n\nReturns\nndarray\n\n\n\n\n\norbits = np.array([\n    [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8]],  # Orbit 1\n    [[4, 4, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 9]]   # Orbit 2\n])\n\n# Call the function to calculate statistics\nstats = calculate_overall_spatial_statistics(orbits)\n\n# Define the expected values for each statistic\nexpected_stats = np.array([\n    [1, 3, 4, 2.25, 3.5, 4],    # posx\n    [2, 3.5, 5, 3, 3.5, 4],     # posy\n    [3, 4.5, 6, 4, 4.5, 5],     # posz\n    [4, 5.5, 7, 5, 5.5, 6],     # velx\n    [5, 6.5, 8, 6, 6.5, 7],     # vely\n    [6, 7.5, 9, 7, 7.5, 8]      # velz\n])\n\n# Test each statistic for each scalar\nfor i, scalar_name in enumerate(['posx', 'posy', 'posz', 'velx', 'vely', 'velz']):\n    test_eq(stats[i, 0], expected_stats[i, 0])\n    test_eq(stats[i, 1], expected_stats[i, 1])\n    test_eq(stats[i, 2], expected_stats[i, 2])\n    test_eq(stats[i, 3], expected_stats[i, 3])\n    test_eq(stats[i, 4], expected_stats[i, 4])\n    test_eq(stats[i, 5], expected_stats[i, 5])\n\n\nsource\n\n\ncalculate_per_orbit_spatial_statistics\n\n calculate_per_orbit_spatial_statistics (orbits:numpy.ndarray)\n\nCalculate per-orbit min, mean, max, and percentile statistics for each scalar (position and velocity in X, Y, Z) across all time instants.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nndarray\nA numpy array of shape (number_of_orbits, 6 or 7, number_of_time_instants) containing orbit data\n\n\nReturns\nndarray",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_statistics.html#plot-time",
    "href": "orbit_statistics.html#plot-time",
    "title": "Statistics",
    "section": "Plot Time",
    "text": "Plot Time\n\nsource\n\nplot_time_increments\n\n plot_time_increments (orbit_dataset:numpy.ndarray,\n                       orbits_to_plot:Optional[List[int]]=None,\n                       show_legend:bool=True)\n\nPlots the time as a function to visualize how it increments for each orbit.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_dataset\nndarray\n\nThe 3D numpy array representing the orbits\n\n\norbits_to_plot\nOptional\nNone\nOptional list of integers referring to the orbits to plot\n\n\nshow_legend\nbool\nTrue\nBoolean to control the display of the legend\n\n\nReturns\nNone",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_statistics.html#plot-histograms",
    "href": "orbit_statistics.html#plot-histograms",
    "title": "Statistics",
    "section": "Plot Histograms",
    "text": "Plot Histograms\n\nsource\n\nplot_orbit_data_lengths\n\n plot_orbit_data_lengths (orbit_data:Dict[int,numpy.ndarray],\n                          key_range:Tuple[int,int]=(1, 36072),\n                          dimension:int=0, bins:int=30, color:str='blue',\n                          plot:bool=True, title:str='Histogram of Orbits\n                          Time Steps')\n\nAnalyzes and optionally plots the distribution of time steps across multiple orbits. Returns the list of lengths if plot=False, otherwise displays histogram and returns None.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nDict\n\nDictionary mapping orbit IDs to orbit data arrays\n\n\nkey_range\nTuple\n(1, 36072)\nRange of orbit IDs to analyze (start, end)\n\n\ndimension\nint\n0\nDimension of the orbit data to measure length\n\n\nbins\nint\n30\nNumber of bins for histogram\n\n\ncolor\nstr\nblue\nColor of histogram bars\n\n\nplot\nbool\nTrue\nWhether to plot the histogram\n\n\ntitle\nstr\nHistogram of Orbits Time Steps\nTitle of the plot\n\n\nReturns\nOptional\n\n\n\n\n\n\nsource\n\n\nplot_histograms_position\n\n plot_histograms_position (data:numpy.ndarray,\n                           save_path:Optional[str]=None,\n                           last_time_elements:bool=True)\n\nPlots histograms for the scalar values (position and velocity in X, Y, Z, and optionally time) across all orbits and time points. Handles arrays with 6 or 7 scalar dimensions, with the 7th being ‘time’.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe orbit data array of shape (num_orbits, num_scalars, num_time_points)\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image\n\n\nlast_time_elements\nbool\nTrue\nWhether to plot only the last elements of the time vectors\n\n\nReturns\nNone\n\n\n\n\n\n\nplot_histograms_position(orbit_data)\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_histograms_comparison\n\n plot_histograms_comparison (data1:numpy.ndarray, data2:numpy.ndarray,\n                             label1:str='Dataset 1', label2:str='Dataset\n                             2', save_path:str=None, normalize:bool=False)\n\nPlots histograms for scalar values (position, velocity in X, Y, Z, and optionally time) from two datasets on the same chart with different colors. Supports both 6 and 7 scalar dimensions, with the 7th being ‘time’. Optionally saves the plot to a specified file path and can normalize histograms for relative comparison.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata1\nndarray\n\nFirst orbit data array of shape (num_orbits, num_scalars, num_time_points).\n\n\ndata2\nndarray\n\nSecond orbit data array of shape (num_orbits, num_scalars, num_time_points).\n\n\nlabel1\nstr\nDataset 1\nLabel for the first dataset.\n\n\nlabel2\nstr\nDataset 2\nLabel for the second dataset.\n\n\nsave_path\nstr\nNone\nOptional path to save the plot image.\n\n\nnormalize\nbool\nFalse\nNormalize histograms to show relative frequencies.\n\n\nReturns\nNone\n\n\n\n\n\n\norbit_data1 = orbit_data[:100]\norbit_data2 = orbit_data[100:]\n\nplot_histograms_comparison(orbit_data1, orbit_data2)\n\n\n\n\n\n\n\n\n\norbit_data3 = orbit_data2[:5]\n\nplot_histograms_comparison(orbit_data1, orbit_data3, normalize=True)",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "Auxiliar Functions\nsource",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#evaluate-clustering-with-multiple-labels",
    "href": "evaluation.html#evaluate-clustering-with-multiple-labels",
    "title": "Evaluation",
    "section": "Evaluate Clustering with Multiple Labels",
    "text": "Evaluate Clustering with Multiple Labels\n\nsource\n\nevaluate_clustering_multiple_labels\n\n evaluate_clustering_multiple_labels\n                                      (latent_representations:numpy.ndarra\n                                      y, list_of_labels:list,\n                                      clustering_method:str='kmeans',\n                                      label_names:list=None, **kwargs)\n\nEvaluates the clustering quality of the latent representations for one or multiple sets of labels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\nThe latent space data.\n\n\nlist_of_labels\nlist\n\nList of true labels or a single true labels array.\n\n\nclustering_method\nstr\nkmeans\nThe clustering algorithm to use (‘kmeans’, ‘gmm’, ‘dbscan’).\n\n\nlabel_names\nlist\nNone\nOptional names for the label sets.\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\nReturns\ndict\n\nReturns a dictionary with clustering metrics.",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#physical-distances",
    "href": "evaluation.html#physical-distances",
    "title": "Evaluation",
    "section": "Physical Distances",
    "text": "Physical Distances\n\nEuclidean\n\nsource\n\n\neuclidean_distance\n\n euclidean_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nManhattan\n\nsource\n\n\nmanhattan_distance\n\n manhattan_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nCosine\n\nsource\n\n\ncosine_distance\n\n cosine_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nDynamic Time Warping\n\nsource\n\n\ndtw_distance\n\n dtw_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nGeneric\n\nsource\n\n\ncalculate_distance\n\n calculate_distance (point1:numpy.ndarray, point2:numpy.ndarray,\n                     distance_metric:str='euclidean')\n\n*Calculates the distance between two points based on the specified distance metric.\n:param point1: First data point array. :param point2: Second data point array. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: Distance as a float.*\n\nsource\n\n\ncalculate_pairwise_distances\n\n calculate_pairwise_distances (array1:numpy.ndarray, array2:numpy.ndarray,\n                               distance_metric:str='euclidean')\n\n*Calculates the distance between corresponding pairs of points from two arrays using the specified distance metric.\n:param array1: A 2D numpy array where each row represents a data point. :param array2: A 2D numpy array where each row represents a data point. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: A 1D numpy array containing the distances between corresponding pairs.*\n\n\nBatch\n\nsource\n\n\ncalculate_distances_batch\n\n calculate_distances_batch (single_points:numpy.ndarray,\n                            points_array:numpy.ndarray,\n                            distance_metric:str='euclidean')\n\n*Calculates the distances between single data points and an array of data points based on the specified distance metric.\n:param single_points: Single data point array or a batch of data points. :param points_array: Array of data points to compare against. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: Array of distances.*\n\n\nNearest Points\n\nsource\n\n\nfind_nearest_points\n\n find_nearest_points (single_point:numpy.ndarray,\n                      points_array:numpy.ndarray, n:int,\n                      distance_metric:str='euclidean')\n\n\nsource\n\n\nfind_nearest_points_batch\n\n find_nearest_points_batch (single_points:numpy.ndarray,\n                            points_array:numpy.ndarray, n:int,\n                            distance_metric:str='euclidean')\n\n*Finds the nearest indices and distances for a batch of single data points to an array of data points based on the specified distance metric.\n:param single_points: Array of single data points (2D array). :param points_array: Array of data points to compare against (2D array). :param n: Number of nearest points to retrieve for each single point. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: Tuple of nearest indices and nearest distances for each single point.*",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#orbits-distance",
    "href": "evaluation.html#orbits-distance",
    "title": "Evaluation",
    "section": "Orbits Distance",
    "text": "Orbits Distance\n\nsource\n\norbits_distances\n\n orbits_distances (orbit_data1:numpy.ndarray, orbit_data2:numpy.ndarray,\n                   distance_metric:str)\n\n*Calculates distances between orbits in two datasets using a specified distance metric.\nThis function is robust to input shapes. If an input is a 2D array (representing a single orbit), it is automatically converted to a 3D array with one sample. This allows for flexible comparisons between single or multiple orbits.\n:param orbit_data1: First set of orbits (shape: [n_samples1, n_features, n_time_steps] or [n_features, n_time_steps]). :param orbit_data2: Second set of orbits or a single orbit. Shape: [n_samples2, n_features, n_time_steps] or [n_features, n_time_steps]. :param distance_metric: A string representing the distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’).\n\nreturn: NumPy array of distances. - If one input is single and the other is multiple: - Shape: [n_samples1] or [n_samples2] - If both inputs are multiple: - Shape: [n_samples1, n_samples2]*\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbit_data1\nndarray\nShape: [n_samples1, n_features, n_time_steps] or [n_features, n_time_steps]\n\n\norbit_data2\nndarray\nShape: [n_samples2, n_features, n_time_steps] or [n_features, n_time_steps]\n\n\ndistance_metric\nstr\nString representing the distance metric (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’)\n\n\nReturns\nndarray\n\n\n\n\n\n\nGet the Closest Orbits\n\nsource\n\n\nfind_nearest_orbits\n\n find_nearest_orbits (single_orbit:numpy.ndarray,\n                      orbit_data:numpy.ndarray, n:int,\n                      distance_metric:str='euclidean')\n\n*Finds the n closest orbits in orbit_data to the single_orbit based on the specified distance metric.\n:param single_orbit: The reference orbit (shape: [n_features, n_time_steps]). :param orbit_data: The dataset of orbits (shape: [n_samples, n_features, n_time_steps]). :param n: The number of closest orbits to return. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). Defaults to ‘euclidean’. :return: A tuple containing: - Indices of the n closest orbits in orbit_data. - Distances of the n closest orbits.*\n\nsource\n\n\nfind_nearest_orbits_batch\n\n find_nearest_orbits_batch (single_orbits:numpy.ndarray,\n                            orbit_data:numpy.ndarray, n:int,\n                            distance_metric:str='euclidean')\n\n*Iteratively finds the n closest orbits in orbit_data for each orbit in single_orbits.\n\nparam single_orbits: The reference orbits (shape: [num_single_orbits, n_features, n_time_steps]). :param orbit_data: The dataset of orbits to search within (shape: [n_samples, n_features, n_time_steps]). :param n: The number of closest orbits to return for each single_orbit. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). Defaults to ‘euclidean’. :return: A tuple containing: - A 2D array of shape [num_single_orbits, n] with indices of the n closest orbits. - A 2D array of shape [num_single_orbits, n] with distances of the n closest orbits.*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsingle_orbits\nndarray\n\nShape: [num_single_orbits, n_features, n_time_steps]\n\n\norbit_data\nndarray\n\nShape: [n_samples, n_features, n_time_steps]\n\n\nn\nint\n\nNumber of nearest orbits to find\n\n\ndistance_metric\nstr\neuclidean\nDistance metric\n\n\nReturns\ntuple\n\n\n\n\n\n\n\nCalculate Pairwise distances\n\nsource\n\n\ncalculate_pairwise_orbit_distances\n\n calculate_pairwise_orbit_distances (orbit_data1:numpy.ndarray,\n                                     orbit_data2:numpy.ndarray,\n                                     distance_metric:str='euclidean')\n\n*Calculates the distance between corresponding orbits in two orbit datasets.\n\nparam orbit_data1: The first set of orbits (shape: [n_samples, n_features, n_time_steps]). :param orbit_data2: The second set of orbits (shape: [n_samples, n_features, n_time_steps]). :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). Defaults to ‘euclidean’. :return: An array of distances with shape [n_samples].*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data1\nndarray\n\nShape: [n_samples, n_features, n_time_steps]\n\n\norbit_data2\nndarray\n\nShape: [n_samples, n_features, n_time_steps]\n\n\ndistance_metric\nstr\neuclidean\nDistance metric\n\n\nReturns\nndarray",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#evaluate-distance-metrics",
    "href": "evaluation.html#evaluate-distance-metrics",
    "title": "Evaluation",
    "section": "Evaluate Distance Metrics",
    "text": "Evaluate Distance Metrics\n\nsource\n\nevaluate_distance_metrics_and_clustering\n\n evaluate_distance_metrics_and_clustering (orbit_data:numpy.ndarray,\n                                           true_labels:numpy.ndarray, dist\n                                           ance_metrics:Optional[list]=Non\n                                           e, clustering_algorithms:Option\n                                           al[list]=None, evaluation_metri\n                                           cs:Optional[list]=None,\n                                           n_clusters:Optional[int]=None,\n                                           plot_results:bool=True)\n\nEvaluates combinations of distance metrics and clustering algorithms on orbit data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\nOrbit data as multivariate time series [n_samples, n_features, n_time_steps] or point data [n_samples, n_features]\n\n\ntrue_labels\nndarray\n\nArray of true labels for the orbit data\n\n\ndistance_metrics\nOptional\nNone\nList of distance metrics to use. If None, uses all available metrics\n\n\nclustering_algorithms\nOptional\nNone\nList of clustering algorithms to use. If None, uses all available algorithms\n\n\nevaluation_metrics\nOptional\nNone\nList of evaluation metrics to use. If None, uses all available metrics\n\n\nn_clusters\nOptional\nNone\nNumber of clusters. If None, inferred from labels\n\n\nplot_results\nbool\nTrue\nIf True, plot heatmaps of results\n\n\nReturns\ndict",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#machine-learning",
    "href": "evaluation.html#machine-learning",
    "title": "Evaluation",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nsource\n\nmachine_learning_evaluation\n\n machine_learning_evaluation (X:numpy.ndarray, y:numpy.ndarray,\n                              print_results:bool=False,\n                              return_best_model:bool=False,\n                              scale_data:bool=True)\n\nEvaluates multiple machine learning algorithms on the provided dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\nFeatures array, expected to be 2D. Will attempt to reshape if higher dimensions\n\n\ny\nndarray\n\nTarget labels\n\n\nprint_results\nbool\nFalse\nIf True, visualizes the evaluation results\n\n\nreturn_best_model\nbool\nFalse\nIf True, returns the best model based on accuracy\n\n\nscale_data\nbool\nTrue\nIf True, scales the features using StandardScaler\n\n\nReturns\nUnion",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "constants.html",
    "href": "constants.html",
    "title": "Constants",
    "section": "",
    "text": "MU_BY_SYSTEM\n\n{'SaE': 1.901109735892602e-07,\n 'MP': 1.611081404409632e-08,\n 'SaT': 0.0002366393158331484,\n 'EM': 0.01215058560962404,\n 'JE': 2.52801752854e-05,\n 'SE': 3.0542e-06,\n 'SM': 3.227154996101724e-07}\n\n\n\nEM_POINTS\n\n{'Moon': (0.987849414390376, 0, 0),\n 'Earth': (-0.01215058560962404, 0, 0),\n 'Lagrange 1': (0.8369, 0, 0),\n 'Lagrange 2': (1.1557, 0, 0),\n 'Lagrange 3': (-1.0051, 0, 0),\n 'Lagrange 4': (0.4879, 0.866, 0),\n 'Lagrange 5': (0.4879, -0.866, 0)}",
    "crumbs": [
      "Constants"
    ]
  },
  {
    "objectID": "constants.html#system-points",
    "href": "constants.html#system-points",
    "title": "Constants",
    "section": "",
    "text": "MU_BY_SYSTEM\n\n{'SaE': 1.901109735892602e-07,\n 'MP': 1.611081404409632e-08,\n 'SaT': 0.0002366393158331484,\n 'EM': 0.01215058560962404,\n 'JE': 2.52801752854e-05,\n 'SE': 3.0542e-06,\n 'SM': 3.227154996101724e-07}\n\n\n\nEM_POINTS\n\n{'Moon': (0.987849414390376, 0, 0),\n 'Earth': (-0.01215058560962404, 0, 0),\n 'Lagrange 1': (0.8369, 0, 0),\n 'Lagrange 2': (1.1557, 0, 0),\n 'Lagrange 3': (-1.0051, 0, 0),\n 'Lagrange 4': (0.4879, 0.866, 0),\n 'Lagrange 5': (0.4879, -0.866, 0)}",
    "crumbs": [
      "Constants"
    ]
  },
  {
    "objectID": "constants.html#orbit-classification",
    "href": "constants.html#orbit-classification",
    "title": "Constants",
    "section": "Orbit Classification",
    "text": "Orbit Classification\n\nFamily\n\nORBIT_FAMILIES.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n\nId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\nLabel\nS_BN\nS_BS\nS_DN\nS_DPO\nS_DRO\nS_DS\nS_L1_A\nS_L1_HN\nS_L1_HS\nS_L1_L\n...\nS_R12\nS_R13\nS_R14\nS_R21\nS_R23\nS_R31\nS_R32\nS_R34\nS_R41\nS_R43\n\n\n\n\n2 rows × 42 columns\n\n\n\n\n\nFull Organization\nOrganization done by Walther Litteri\n\nEXTENDED_ORBIT_CLASSIFICATION\n\n\n\n\n\n\n\n\nId\nLabel\nType\nSubtype\nDirection\n\n\n\n\n0\n1\nS_BN\nSystem-wide\nButterfly\nNorth\n\n\n1\n2\nS_BS\nSystem-wide\nButterfly\nSouth\n\n\n2\n3\nS_DN\nSystem-wide\nDragonfly\nNorth\n\n\n3\n4\nS_DPO\nSystem-wide\nDistant Prograde\nPlanar\n\n\n4\n5\nS_DRO\nSystem-wide\nDistant Retrograde\nPlanar\n\n\n5\n6\nS_DS\nSystem-wide\nDragonfly\nSouth\n\n\n6\n7\nS_L1_A\nL1\nAxial\nNo specification\n\n\n7\n8\nS_L1_HN\nL1\nHalo\nNorth\n\n\n8\n9\nS_L1_HS\nL1\nHalo\nSouth\n\n\n9\n10\nS_L1_L\nL1\nLyapunov\nPlanar\n\n\n10\n11\nS_L1_V\nL1\nVertical\nNo specification\n\n\n11\n12\nS_L2_A\nL2\nAxial\nNo specification\n\n\n12\n13\nS_L2_HN\nL2\nHalo\nNorth\n\n\n13\n14\nS_L2_HS\nL2\nHalo\nSouth\n\n\n14\n15\nS_L2_L\nL2\nLyapunov\nPlanar\n\n\n15\n16\nS_L2_V\nL2\nVertical\nNo specification\n\n\n16\n17\nS_L3_A\nL3\nAxial\nNo specification\n\n\n17\n18\nS_L3_HN\nL3\nHalo\nNorth\n\n\n18\n19\nS_L3_HS\nL3\nHalo\nSouth\n\n\n19\n20\nS_L3_L\nL3\nLyapunov\nPlanar\n\n\n20\n21\nS_L3_V\nL3\nVertical\nNo specification\n\n\n21\n22\nS_L4_A\nL4\nAxial\nNo specification\n\n\n22\n23\nS_L4_LP\nL4\nLong Period\nNo specification\n\n\n23\n24\nS_L4_SP\nL4\nShort Period\nNo specification\n\n\n24\n25\nS_L4_V\nL4\nVertical\nNo specification\n\n\n25\n26\nS_L5_A\nL5\nAxial\nNo specification\n\n\n26\n27\nS_L5_LP\nL5\nLong Period\nNo specification\n\n\n27\n28\nS_L5_SP\nL5\nShort Period\nNo specification\n\n\n28\n29\nS_L5_V\nL5\nVertical\nNo specification\n\n\n29\n30\nS_LPOE\nSystem-wide\nLow Prograde\nEast\n\n\n30\n31\nS_LPOW\nSystem-wide\nLow Prograde\nWest\n\n\n31\n32\nS_R11\nResonant\nResonant 1,1\nPlanar\n\n\n32\n33\nS_R12\nResonant\nResonant 1,2\nPlanar\n\n\n33\n34\nS_R13\nResonant\nResonant 1,3\nPlanar\n\n\n34\n35\nS_R14\nResonant\nResonant 1,4\nPlanar\n\n\n35\n36\nS_R21\nResonant\nResonant 2,1\nPlanar\n\n\n36\n37\nS_R23\nResonant\nResonant 2,3\nPlanar\n\n\n37\n38\nS_R31\nResonant\nResonant 3,1\nPlanar\n\n\n38\n39\nS_R32\nResonant\nResonant 3,2\nPlanar\n\n\n39\n40\nS_R34\nResonant\nResonant 3,4\nPlanar\n\n\n40\n41\nS_R41\nResonant\nResonant 4,1\nPlanar\n\n\n41\n42\nS_R43\nResonant\nResonant 4,3\nPlanar",
    "crumbs": [
      "Constants"
    ]
  },
  {
    "objectID": "latent_space.html",
    "href": "latent_space.html",
    "title": "Latent Space",
    "section": "",
    "text": "source\n\n\n\n plot_2d_latent_space (latent_representations:numpy.ndarray,\n                       labels:numpy.ndarray,\n                       latent_stdevs:Optional[numpy.ndarray]=None,\n                       features:Optional[Any]=None,\n                       feature_names:Optional[List[str]]=None,\n                       figsize:tuple=(12, 12),\n                       save_path:Optional[str]=None,\n                       many_classes:bool=False, show_legend:bool=True,\n                       legend_fontsize:int=8, plot_std:bool=True,\n                       title:Optional[str]='2D Latent Space\n                       Visualization', title_size:int=14,\n                       axis_labels:Optional[Tuple[str,str]]=('Dimension\n                       1', 'Dimension 2'), normalize_data:bool=True,\n                       **kwargs:Any)\n\n*Plots a 2D latent space visualization with class labels and feature distributions.\nParameters: - latent_representations: np.ndarray, shape (n_samples, 2) The 2D coordinates of the latent representations. - labels: np.ndarray, shape (n_samples,) The class labels for each sample. - features: Optional[Any], shape (n_samples, n_features) The feature data to plot distributions. Can be a list or a NumPy array. - feature_names: Optional[List[str]] The names of the features. - figsize: tuple, default (12, 12) The size of the entire figure. - save_path: Optional[str] Path to save the figure. If None, the plot is not saved. - many_classes: bool, default False If True, uses different markers for classes. - show_legend: bool, default True If True, displays legends. - legend_fontsize: int, default 8 Font size for the legends. - plot_std: bool, default True If True, plots the standard deviation shading for feature distributions. - title: Optional[str], default ‘2D Latent Space Visualization’ Title of the plot. - title_size: int, default 14 Font size for the title. - axis_labels: Optional[Tuple[str, str]], default (‘Dimension 1’, ‘Dimension 2’) Labels for the X and Y axes. - normalize_data: bool, default False If True, normalizes the latent representations. - kwargs: Any Additional keyword arguments passed to scatter plots.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\n\n\n\nlabels\nndarray\n\n\n\n\nlatent_stdevs\nOptional\nNone\n\n\n\nfeatures\nOptional\nNone\n\n\n\nfeature_names\nOptional\nNone\n\n\n\nfigsize\ntuple\n(12, 12)\n\n\n\nsave_path\nOptional\nNone\n\n\n\nmany_classes\nbool\nFalse\n\n\n\nshow_legend\nbool\nTrue\n\n\n\nlegend_fontsize\nint\n8\n\n\n\nplot_std\nbool\nTrue\n\n\n\ntitle\nOptional\n2D Latent Space Visualization\nNew title parameter\n\n\ntitle_size\nint\n14\nNew title_size parameter\n\n\naxis_labels\nOptional\n(‘Dimension 1’, ‘Dimension 2’)\nNew axis_labels parameter\n\n\nnormalize_data\nbool\nTrue\nNew parameter to control normalization\n\n\nkwargs\nAny\n\n\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\n\n\n plot_combined_2d_latent_space (real_latent:numpy.ndarray,\n                                synthetic_latent:numpy.ndarray, synthetic_\n                                labels:Union[int,List[int],numpy.ndarray,N\n                                oneType]=None, figsize:tuple=(12, 9),\n                                save_path:Optional[str]=None,\n                                show_legend:bool=True,\n                                axis_labels:tuple=('X-axis', 'Y-axis'),\n                                title:Optional[str]=None,\n                                colormap:str='viridis',\n                                feature_title:Optional[str]='Feature\n                                Value', label_names:Optional[dict]=None)\n\n*Plots the combined latent space of real and synthetic data. Assumes the latent space is 2D. If synthetic_latent is a 3D array, it plots arrows. Numeric annotations for arrows are only displayed if synthetic_labels are provided.\nArgs: real_latent (np.ndarray): Latent representations of real data. synthetic_latent (np.ndarray): Latent representations of synthetic data or arrows. synthetic_labels (Optional[Union[int, List[int], np.ndarray]]): Labels for synthetic data. Can be None, a single label, or a list of labels. figsize (tuple): Size of the figure. save_path (Optional[str]): Optional path to save the plot image. show_legend (bool): Flag to show or hide the legend. axis_labels (tuple): Labels for the X and Y axes. title (Optional[str]): Title of the plot. colormap (str): Colormap to use when coloring by features. feature_title (Optional[str]): Title for the feature color bar. label_names (Optional[dict]): Dictionary mapping label values to names for discrete labels.\nReturns: None*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreal_latent\nndarray\n\nLatent representations of real data.\n\n\nsynthetic_latent\nndarray\n\nLatent representations of synthetic data or arrows.\n\n\nsynthetic_labels\nUnion\nNone\nLabels for synthetic data. Can be None, a single label, or a list of labels.\n\n\nfigsize\ntuple\n(12, 9)\nSize of the figure.\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image.\n\n\nshow_legend\nbool\nTrue\nFlag to show or hide the legend.\n\n\naxis_labels\ntuple\n(‘X-axis’, ‘Y-axis’)\nLabels for the X and Y axes.\n\n\ntitle\nOptional\nNone\nTitle of the plot.\n\n\ncolormap\nstr\nviridis\nColormap to use when coloring by features.\n\n\nfeature_title\nOptional\nFeature Value\nTitle for the feature color bar.\n\n\nlabel_names\nOptional\nNone\nNew parameter: dictionary mapping label values to names\n\n\nReturns\nNone",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#plot-2-dimensions",
    "href": "latent_space.html#plot-2-dimensions",
    "title": "Latent Space",
    "section": "",
    "text": "source\n\n\n\n plot_2d_latent_space (latent_representations:numpy.ndarray,\n                       labels:numpy.ndarray,\n                       latent_stdevs:Optional[numpy.ndarray]=None,\n                       features:Optional[Any]=None,\n                       feature_names:Optional[List[str]]=None,\n                       figsize:tuple=(12, 12),\n                       save_path:Optional[str]=None,\n                       many_classes:bool=False, show_legend:bool=True,\n                       legend_fontsize:int=8, plot_std:bool=True,\n                       title:Optional[str]='2D Latent Space\n                       Visualization', title_size:int=14,\n                       axis_labels:Optional[Tuple[str,str]]=('Dimension\n                       1', 'Dimension 2'), normalize_data:bool=True,\n                       **kwargs:Any)\n\n*Plots a 2D latent space visualization with class labels and feature distributions.\nParameters: - latent_representations: np.ndarray, shape (n_samples, 2) The 2D coordinates of the latent representations. - labels: np.ndarray, shape (n_samples,) The class labels for each sample. - features: Optional[Any], shape (n_samples, n_features) The feature data to plot distributions. Can be a list or a NumPy array. - feature_names: Optional[List[str]] The names of the features. - figsize: tuple, default (12, 12) The size of the entire figure. - save_path: Optional[str] Path to save the figure. If None, the plot is not saved. - many_classes: bool, default False If True, uses different markers for classes. - show_legend: bool, default True If True, displays legends. - legend_fontsize: int, default 8 Font size for the legends. - plot_std: bool, default True If True, plots the standard deviation shading for feature distributions. - title: Optional[str], default ‘2D Latent Space Visualization’ Title of the plot. - title_size: int, default 14 Font size for the title. - axis_labels: Optional[Tuple[str, str]], default (‘Dimension 1’, ‘Dimension 2’) Labels for the X and Y axes. - normalize_data: bool, default False If True, normalizes the latent representations. - kwargs: Any Additional keyword arguments passed to scatter plots.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\n\n\n\nlabels\nndarray\n\n\n\n\nlatent_stdevs\nOptional\nNone\n\n\n\nfeatures\nOptional\nNone\n\n\n\nfeature_names\nOptional\nNone\n\n\n\nfigsize\ntuple\n(12, 12)\n\n\n\nsave_path\nOptional\nNone\n\n\n\nmany_classes\nbool\nFalse\n\n\n\nshow_legend\nbool\nTrue\n\n\n\nlegend_fontsize\nint\n8\n\n\n\nplot_std\nbool\nTrue\n\n\n\ntitle\nOptional\n2D Latent Space Visualization\nNew title parameter\n\n\ntitle_size\nint\n14\nNew title_size parameter\n\n\naxis_labels\nOptional\n(‘Dimension 1’, ‘Dimension 2’)\nNew axis_labels parameter\n\n\nnormalize_data\nbool\nTrue\nNew parameter to control normalization\n\n\nkwargs\nAny\n\n\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\n\n\n plot_combined_2d_latent_space (real_latent:numpy.ndarray,\n                                synthetic_latent:numpy.ndarray, synthetic_\n                                labels:Union[int,List[int],numpy.ndarray,N\n                                oneType]=None, figsize:tuple=(12, 9),\n                                save_path:Optional[str]=None,\n                                show_legend:bool=True,\n                                axis_labels:tuple=('X-axis', 'Y-axis'),\n                                title:Optional[str]=None,\n                                colormap:str='viridis',\n                                feature_title:Optional[str]='Feature\n                                Value', label_names:Optional[dict]=None)\n\n*Plots the combined latent space of real and synthetic data. Assumes the latent space is 2D. If synthetic_latent is a 3D array, it plots arrows. Numeric annotations for arrows are only displayed if synthetic_labels are provided.\nArgs: real_latent (np.ndarray): Latent representations of real data. synthetic_latent (np.ndarray): Latent representations of synthetic data or arrows. synthetic_labels (Optional[Union[int, List[int], np.ndarray]]): Labels for synthetic data. Can be None, a single label, or a list of labels. figsize (tuple): Size of the figure. save_path (Optional[str]): Optional path to save the plot image. show_legend (bool): Flag to show or hide the legend. axis_labels (tuple): Labels for the X and Y axes. title (Optional[str]): Title of the plot. colormap (str): Colormap to use when coloring by features. feature_title (Optional[str]): Title for the feature color bar. label_names (Optional[dict]): Dictionary mapping label values to names for discrete labels.\nReturns: None*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreal_latent\nndarray\n\nLatent representations of real data.\n\n\nsynthetic_latent\nndarray\n\nLatent representations of synthetic data or arrows.\n\n\nsynthetic_labels\nUnion\nNone\nLabels for synthetic data. Can be None, a single label, or a list of labels.\n\n\nfigsize\ntuple\n(12, 9)\nSize of the figure.\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image.\n\n\nshow_legend\nbool\nTrue\nFlag to show or hide the legend.\n\n\naxis_labels\ntuple\n(‘X-axis’, ‘Y-axis’)\nLabels for the X and Y axes.\n\n\ntitle\nOptional\nNone\nTitle of the plot.\n\n\ncolormap\nstr\nviridis\nColormap to use when coloring by features.\n\n\nfeature_title\nOptional\nFeature Value\nTitle for the feature color bar.\n\n\nlabel_names\nOptional\nNone\nNew parameter: dictionary mapping label values to names\n\n\nReturns\nNone",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#reduce-dimensions",
    "href": "latent_space.html#reduce-dimensions",
    "title": "Latent Space",
    "section": "Reduce dimensions",
    "text": "Reduce dimensions\n\nsource\n\nreduce_dimensions_latent_space\n\n reduce_dimensions_latent_space (latent_representations:numpy.ndarray,\n                                 labels:numpy.ndarray,\n                                 techniques:List[str]=['PCA'],\n                                 n_components:int=2, figsize:tuple=(12,\n                                 9), save_path:Optional[str]=None,\n                                 many_classes:bool=False,\n                                 grid_view:bool=True,\n                                 class_names:Optional[List[str]]=None,\n                                 show_legend:bool=True, plot:bool=True,\n                                 **kwargs:Any)\n\n*Reduces dimensions of latent representations using specified techniques and optionally plots the results.\nReturns: A dictionary containing the reduced latent space for each technique.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\nPrecomputed latent representations (numpy array).\n\n\nlabels\nndarray\n\nLabels for the data points, used for coloring in the plot.\n\n\ntechniques\nList\n[‘PCA’]\nTechniques to use for reduction (‘PCA’, ‘t-SNE’, ‘UMAP’, ‘LDA’).\n\n\nn_components\nint\n2\nNumber of dimensions to reduce to (1, 2, or 3).\n\n\nfigsize\ntuple\n(12, 9)\nSize of the figure for each subplot.\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image.\n\n\nmany_classes\nbool\nFalse\nFlag to use enhanced plotting for many classes.\n\n\ngrid_view\nbool\nTrue\nFlag to plot all techniques in a single grid view.\n\n\nclass_names\nOptional\nNone\nOptional class names for the legend\n\n\nshow_legend\nbool\nTrue\nFlag to show or hide the legend\n\n\nplot\nbool\nTrue\nFlag to plot the latent space\n\n\nkwargs\nAny\n\n\n\n\nReturns\nDict\n\nAdditional keyword arguments for dimensionality reduction methods.\n\n\n\n\nsource\n\n\nreduce_dimensions_combined_latent_space\n\n reduce_dimensions_combined_latent_space (train_latent:numpy.ndarray,\n                                          val_latent:numpy.ndarray, train_\n                                          labels:Optional[numpy.ndarray]=N\n                                          one,\n                                          techniques:List[str]=['PCA'],\n                                          n_components:int=2,\n                                          **kwargs:Any)\n\n*Reduces dimensions of latent representations using specified techniques.\nReturns: A dictionary containing the reduced latent space for each technique and dataset (train and val).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_latent\nndarray\n\nLatent representations of training data.\n\n\nval_latent\nndarray\n\nLatent representations of validation data.\n\n\ntrain_labels\nOptional\nNone\nLabels for the training data points (optional for LDA).\n\n\ntechniques\nList\n[‘PCA’]\nTechniques to use for reduction (‘PCA’, ‘t-SNE’, ‘UMAP’, ‘LDA’).\n\n\nn_components\nint\n2\nNumber of dimensions to reduce to (1, 2, or 3).\n\n\nkwargs\nAny\n\n\n\n\nReturns\nDict\n\nAdditional keyword arguments for dimensionality reduction methods.\n\n\n\n\nfrom orbit_generation.data import get_example_orbit_data\n\n\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n# Reshape data to 2D (num_orbits, 6 * num_time_points)\norbit_data_reshaped = orbit_data.reshape(200, -1)\n\n# Use PCA to reduce to a lower-dimensional space (e.g., 10 dimensions)\npca = PCA(n_components=10)\nlatent_representations = pca.fit_transform(orbit_data_reshaped)\n\nlabels = np.random.randint(0, 5, size=200)  # 5 different classes\n\nreduced_latent_spaces = reduce_dimensions_latent_space(latent_representations, labels, techniques=['UMAP','LDA'])\nreduced_latent_spaces['UMAP'].shape\n\n\n\n\n\n\n\n\n\nreduced_latent_spaces=reduce_dimensions_latent_space(latent_representations, labels, techniques=['PCA'], n_components=1, many_classes=True)\nreduced_latent_spaces['PCA'].shape\n\n\n\n\n\n\n\n\n\nreduced_latent_spaces=reduce_dimensions_latent_space(latent_representations, labels, techniques=['t-SNE'], n_components=3)\nreduced_latent_spaces['t-SNE'].shape",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#sampling",
    "href": "latent_space.html#sampling",
    "title": "Latent Space",
    "section": "Sampling",
    "text": "Sampling\n\nsource\n\nsample_random_distributions\n\n sample_random_distributions (means, log_vars, n_samples:int,\n                              log_var_multiplier:float=1.0)\n\n\nsource\n\n\ninterpolate_sample\n\n interpolate_sample (centroids, granularity=10, variance=0.0)\n\n*Perform interpolating sampling between all pairs of centroids.\nParameters: - centroids (np.ndarray): Array of shape (n_centroids, latent_dim). - granularity (int): Number of interpolation steps between each pair. - variance (float): Standard deviation for Gaussian sampling.\nReturns: - samples (np.ndarray): Array of sampled points.*\n\nsource\n\n\nslerp\n\n slerp (z1, z2, steps)\n\nPerform spherical linear interpolation between two points.\n\nsource\n\n\nlinear_interpolation\n\n linear_interpolation (z1, z2, steps)\n\nPerform linear interpolation between two points.\n\n# Define example centroids for a 2-dimensional latent space\ncentroids = np.array([\n    [1.0, 2.0],\n    [3.0, 4.0],\n    [5.0, 6.0]\n])\n\ngranularity = 3\nvariance = 0.0  # Set to 0 for deterministic interpolation\n\nsampled_points = interpolate_sample(centroids, granularity, variance)\n\n# Define the expected sampled points manually for granularity=3\nexpected_data = np.array([\n    [1.0, 2.0],\n    [2.0, 3.0],\n    [3.0, 4.0],\n    [1.0, 2.0],\n    [3.0, 4.0],\n    [5.0, 6.0],\n    [3.0, 4.0],\n    [4.0, 5.0],\n    [5.0, 6.0]\n])\n\n# Check the sampled points against the expected data\ntest_eq(sampled_points, expected_data)\n\n\nsource\n\n\ngrid_sample\n\n grid_sample (encodings:numpy.ndarray,\n              grid_size:Union[int,Tuple[int,...]]=100)\n\n\ndef visual_test():\n    # Generate random encodings\n    np.random.seed(42)\n    encodings = np.random.rand(100, 2) * 100  # 100 points in a 100x100 space\n\n    # Perform grid sampling\n    grid_size = (10, 10)\n    sampled_grid = grid_sample(encodings, grid_size)\n\n    # Calculate bounds for visualization\n    x_min, y_min = np.min(encodings, axis=0)\n    x_max, y_max = np.max(encodings, axis=0)\n\n    # Plot the encodings\n    plt.figure(figsize=(8, 8))\n    plt.scatter(encodings[:, 0], encodings[:, 1], c='blue', label='Encodings')\n\n    # Plot the sampled grid points\n    plt.scatter(sampled_grid[:, 0], sampled_grid[:, 1], c='red', marker='o', label='Sampled Grid Points')\n\n    # Draw grid lines for reference\n    x_edges = np.linspace(x_min, x_max, grid_size[0] + 1)\n    y_edges = np.linspace(y_min, y_max, grid_size[1] + 1)\n\n    plt.title('Grid Sampling Visualization with Sampled Grid Points')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\n# Run the visual test\nvisual_test()",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#explore",
    "href": "latent_space.html#explore",
    "title": "Latent Space",
    "section": "Explore",
    "text": "Explore\n\nsource\n\ntrimmed_mean_centroid\n\n trimmed_mean_centroid (points, trim_ratio=0.1)\n\n\nsource\n\n\ncompute_medoid\n\n compute_medoid (points)\n\n\nsource\n\n\ngeometric_median\n\n geometric_median (points, tol=1e-05)\n\n\nsource\n\n\ncompute_centroids\n\n compute_centroids (latents, labels, method='mean', return_labels=False,\n                    **kwargs)\n\n*Compute the centroid of each class in the latent space using various methods.\nParameters: - latents (np.ndarray): Array of shape (n_samples, latent_dim). - labels (np.ndarray): Array of shape (n_samples,) with class labels. - method (str): Method to compute centroids. Options: ‘mean’, ‘median’, ‘geom_median’, ‘medoid’, ‘trimmed_mean’, ‘gmm’. - return_labels (bool): If True, also return the unique labels corresponding to the centroids. - kwargs: Additional arguments for specific methods.\nReturns: - centroids (np.ndarray): Array of shape (n_classes, latent_dim) containing centroids. - unique_labels (np.ndarray, optional): Array of shape (n_classes,) with unique class labels.*",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#measure",
    "href": "latent_space.html#measure",
    "title": "Latent Space",
    "section": "Measure",
    "text": "Measure\n\nsource\n\nplot_linear_regression\n\n plot_linear_regression (latent_means, features, feature_names,\n                         normalize=False)\n\n*Perform linear regression for each feature, visualize the results, and return regression metrics.\nParameters: - latent_means: np.ndarray of shape (n_samples, latent_dim), the latent space coordinates. - features: np.ndarray of shape (n_samples, n_features), the feature values. - feature_names: List of strings representing the names of the features. - normalize: Boolean, whether to normalize the features and latent space (default: False).\nReturns: - results: Dictionary containing coefficients, intercepts, and R² values for each feature. - simple_results: Dictionary containing R² values for each feature with modified keys.*",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "VAE",
    "section": "",
    "text": "source\n\nAbstractVAE\n\n AbstractVAE (seq_len:int, feat_dim:int, latent_dim:int)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nBetaVAE\n\n BetaVAE (encoder:torch.nn.modules.module.Module,\n          decoder:torch.nn.modules.module.Module, beta:float=1.0,\n          loss_fn=None, optimizer_cls=&lt;class 'torch.optim.adam.Adam'&gt;,\n          lr:float=0.001, **kwargs)\n\nHooks to be used in LightningModule.\n\nsource\n\n\ncBetaVAE\n\n cBetaVAE (encoder:torch.nn.modules.module.Module,\n           decoder:torch.nn.modules.module.Module, beta:float=1.0,\n           loss_fn=None, optimizer_cls=&lt;class 'torch.optim.adam.Adam'&gt;,\n           lr:float=0.001, **kwargs)\n\nHooks to be used in LightningModule.",
    "crumbs": [
      "VAE"
    ]
  },
  {
    "objectID": "path_utils.html",
    "href": "path_utils.html",
    "title": "Path Utils",
    "section": "",
    "text": "source\n\n\n\n get_julia_file_path (filename:str)\n\nGet the absolute path to a Julia file in the project’s julia directory.\n\n\n\n\nType\nDetails\n\n\n\n\nfilename\nstr\nName of the Julia file\n\n\nReturns\nstr\n\n\n\n\n\nsource\n\n\n\n\n make_project_path (*parts:str)\n\nCreate a path relative to the project root.\n\n\n\n\nType\nDetails\n\n\n\n\nparts\nstr\nPath components to join\n\n\nReturns\nPath\n\n\n\n\n\nsource\n\n\n\n\n get_data_path ()\n\nGet the path to the data directory.\n\nsource\n\n\n\n\n get_project_root ()\n\nGet the project root directory by searching for settings.ini or setup.py files.",
    "crumbs": [
      "Path Utils"
    ]
  },
  {
    "objectID": "path_utils.html#path-handling",
    "href": "path_utils.html#path-handling",
    "title": "Path Utils",
    "section": "",
    "text": "source\n\n\n\n get_julia_file_path (filename:str)\n\nGet the absolute path to a Julia file in the project’s julia directory.\n\n\n\n\nType\nDetails\n\n\n\n\nfilename\nstr\nName of the Julia file\n\n\nReturns\nstr\n\n\n\n\n\nsource\n\n\n\n\n make_project_path (*parts:str)\n\nCreate a path relative to the project root.\n\n\n\n\nType\nDetails\n\n\n\n\nparts\nstr\nPath components to join\n\n\nReturns\nPath\n\n\n\n\n\nsource\n\n\n\n\n get_data_path ()\n\nGet the path to the data directory.\n\nsource\n\n\n\n\n get_project_root ()\n\nGet the project root directory by searching for settings.ini or setup.py files.",
    "crumbs": [
      "Path Utils"
    ]
  },
  {
    "objectID": "propagation.html",
    "href": "propagation.html",
    "title": "Propagation",
    "section": "",
    "text": "Propagation implemented by Walther Litteri",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#tolerance-constants",
    "href": "propagation.html#tolerance-constants",
    "title": "Propagation",
    "section": "Tolerance Constants",
    "text": "Tolerance Constants",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#jacobi-constant",
    "href": "propagation.html#jacobi-constant",
    "title": "Propagation",
    "section": "Jacobi Constant",
    "text": "Jacobi Constant\n\nsource\n\njacobi_constant\n\n jacobi_constant (X:numpy.ndarray, mu:float)\n\n*State-dependent Jacobi constant for a given state vector X and gravitational parameter mu.\nParameters: X (np.ndarray): Cartesian state vector with 6 components (x, y, z, xp, yp, zp). mu (float): Gravitational parameter.\nReturns: Tuple[float, float]: Jacobi constant (J) and total energy (E).*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nCartesian state vector with 6 components (x, y, z, xp, yp, zp)\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nTuple\n\n\n\n\n\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n(400, 7, 100)\n\n\n\n# Calculate Jacobi constants and energies for all orbits at all time points\njacobi_constants = np.zeros((400, 100))\ntotal_energies = np.zeros((400, 100))\n\nfor orbit_index in range(400):\n    for time_index in range(100):\n        X = orbit_data[orbit_index, 1:, time_index]\n        J, E = jacobi_constant(X, EM_MU)\n        jacobi_constants[orbit_index, time_index] = J\n        total_energies[orbit_index, time_index] = E\n\n# Flatten the Jacobi constants array to plot the histogram of all values\njacobi_constants_all = jacobi_constants.flatten()\n\n# Plot histogram of Jacobi constants for all orbits\nplt.figure(figsize=(10, 5))\nplt.hist(jacobi_constants_all, bins=50, color='blue', alpha=0.7)\nplt.title('Histogram of Jacobi Constants for All Orbits')\nplt.xlabel('Jacobi Constant')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n# Plot histogram of Jacobi constants for the first orbit\njacobi_constants_first_orbit = jacobi_constants[0, :]\n\nplt.figure(figsize=(10, 5))\nplt.hist(jacobi_constants_first_orbit, bins=50, color='green', alpha=0.7)\nplt.title('Histogram of Jacobi Constants for the First Orbit')\nplt.xlabel('Jacobi Constant')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#equations-of-motion-cr3bp",
    "href": "propagation.html#equations-of-motion-cr3bp",
    "title": "Propagation",
    "section": "Equations of motion CR3BP",
    "text": "Equations of motion CR3BP\n\nsource\n\neom_cr3bp\n\n eom_cr3bp (t:float, X:numpy.ndarray, mu:float)\n\n*Equations of motion for the Circular Restricted 3 Body Problem (CR3BP). The form is X_dot = f(t, X, (parameters,)). This formulation is time-independent as it does not depend explicitly on t.\nParameters: t (float): Time variable (not used in this formulation). X (np.ndarray): State vector with 6 components (x, y, z, v_x, v_y, v_z). mu (float): Gravitational parameter.\nReturns: List[float]: Derivatives of the state vector.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nt\nfloat\nTime variable (not used in this formulation)\n\n\nX\nndarray\nState vector with 6 components (x, y, z, v_x, v_y, v_z)\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nList\n\n\n\n\n\n# Select a random orbit from the dataset\nnum_orbits, num_components, num_time_points = orbit_data.shape  # Now (400,7,100)\nrandom_orbit_index = np.random.randint(0, num_orbits)\nX0 = orbit_data[random_orbit_index, :6, 0]  # Take only first 6 components for state vector\nmu = 0.01215058560962404\nT0 = 2.7430007981241529E+0  # Total time for the propagation, can be adjusted as needed\n\n# Propagate the orbit using solve_ivp\nsol = solve_ivp(eom_cr3bp, [0, T0], X0, args=(mu,), dense_output=True, rtol=1e-9, atol=1e-9, method='Radau')\ntvec = np.linspace(0, T0, num_time_points)\nz = sol.sol(tvec)\n\n# Compute derivatives using eom_cr3bp for a specific state in the propagated orbit\ntime_index = np.random.randint(0, num_time_points - 1)  # Choose a random time index\nt = tvec[time_index]\nX = z[:, time_index]\ncomputed_derivatives = eom_cr3bp(t, X, mu)\n\n# Compare with actual changes in state vector\ndelta_t = tvec[1] - tvec[0]\nactual_derivatives = (z[:, time_index + 1] - z[:, time_index]) / delta_t\n\n# Visualize the actual trajectory and computed derivatives\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\n\n# Plot the actual trajectory\naxs[0].plot(z[0], z[1], label='Trajectory')\naxs[0].scatter(z[0, time_index], z[1, time_index], color='red', label='Point of Interest')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].set_title('Trajectory in the XY plane')\naxs[0].legend()\naxs[0].grid(True)\n\n# Plot computed vs. actual derivatives\nlabels = ['x_dot', 'y_dot', 'z_dot', 'x_ddot', 'y_ddot', 'z_ddot']\nwidth = 0.3  # width of the bars\nx = np.arange(len(labels))  # the label locations\n\naxs[1].bar(x - width/2, computed_derivatives, width, label='Computed')\naxs[1].bar(x + width/2, actual_derivatives, width, label='Actual')\naxs[1].set_xticks(x)\naxs[1].set_xticklabels(labels)\naxs[1].set_xlabel('Derivative')\naxs[1].set_ylabel('Value')\naxs[1].set_title('Computed vs Actual Derivatives')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#propagation",
    "href": "propagation.html#propagation",
    "title": "Propagation",
    "section": "Propagation",
    "text": "Propagation\n\nsource\n\nprop_node\n\n prop_node (X:numpy.ndarray, dt:float, mu:float)\n\n*Return the state X after a given time step dt = T_end - T_start.\nParameters: X (np.ndarray): Initial state vector with 6 components (x, y, z, v_x, v_y, v_z). dt (float): Time step for propagation. mu (float): Gravitational parameter.\nReturns: np.ndarray: Final state vector after time step dt.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nInitial state vector with 6 components (x, y, z, v_x, v_y, v_z)\n\n\ndt\nfloat\nTime step for propagation\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nndarray\n\n\n\n\n\n# Select a random orbit from the dataset\nnum_orbits, num_components, num_time_points = orbit_data.shape  # (400, 7, 100)\nrandom_orbit_index = np.random.randint(0, num_orbits)\nX0 = orbit_data[random_orbit_index, 1:, 0]  # Take components 1-6 (skip time)\nmu = 0.01215058560962404\ndt = 0.1  # Small time step for propagation\n\n# Propagate the state vector using prop_node\nX_final = prop_node(X0, dt, mu)\n\n# Print the initial and final state vectors\nprint(\"Initial state vector:\", X0)\nprint(\"Final state vector after time step dt:\", X_final)\n\n# To visualize the propagation, we can propagate over multiple time steps and plot the trajectory\nT_total = 2.0  # Total time for propagation\ntime_steps = int(T_total / dt)\ntrajectory = np.zeros((time_steps + 1, 6))\ntrajectory[0] = X0\n\n# Propagate step by step\nX_current = X0\nfor i in range(1, time_steps + 1):\n    X_current = prop_node(X_current, dt, mu)\n    trajectory[i] = X_current\n\n# Plot the trajectory\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2], label='Propagated trajectory')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Trajectory Propagation using prop_node')\nax.legend()\nplt.show()\n\nInitial state vector: [ 9.2006886e-01 -2.0666480e-29 -7.9184706e-13  1.5364921e-12\n -1.9181405e+00 -5.0562638e-01]\nFinal state vector after time step dt: [ 0.90657459 -0.18761657 -0.04958607 -0.3095898  -1.83133625 -0.48735496]",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#compute-error",
    "href": "propagation.html#compute-error",
    "title": "Propagation",
    "section": "Compute Error",
    "text": "Compute Error\n\nsource\n\njacobi_test\n\n jacobi_test (X:numpy.ndarray, mu:float)\n\nCompute the energy error. X can have either 6 columns (state vector) or 7 columns (time + state vector). The returned quantity is the cumulative error with respect to the initial value. If propagation is perfect, err = 0 (or very small).\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nState vector with shape (n, 6) or (n, 7), where n is the number of samples\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\ndynamics_defect\n\n dynamics_defect (X:numpy.ndarray, mu:float)\n\nCompute the dynamical defect for the generated time-state sequence. The returned quantity is the cumulative error on the position and velocity components. The overall metrics can be a combination of these two last errors.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nTime-state vector with shape (n, 7), where the first column is the time vector\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nTuple\n\n\n\n\n\n# Select a random orbit from the dataset\nnum_orbits, num_components, num_time_points = orbit_data.shape  # Should be (400, 7, 100)\nrandom_orbit_index = np.random.randint(0, num_orbits)\nselected_orbit = orbit_data[random_orbit_index, :, :]\n\n# The time vector is already included as the first component\n# No need to add time column since data is already (400,7,100)\ntime_state_vector = selected_orbit.T  # Transpose to get (100,7) shape needed for dynamics_defect\n\n# Test jacobi_test function\nenergy_error = jacobi_test(time_state_vector, mu)  # Pass all 7 columns since jacobi_test handles both cases\nprint(\"Cumulative energy error for the selected orbit:\", energy_error)\n\n# Test dynamics_defect function\npos_error, vel_error = dynamics_defect(time_state_vector, mu)\nprint(\"Cumulative position error for the selected orbit:\", pos_error)\nprint(\"Cumulative velocity error for the selected orbit:\", vel_error)\n\n# Visualize the numerically propagated orbit\nvisualize_static_orbits(orbit_data, time_instants=[0, 25, 50, 75], orbit_indices=[random_orbit_index])\n\n# Visualize the cumulative errors calculated\nfig, ax = plt.subplots(figsize=(10, 6))\n\nlabels = ['Cumulative Position Error', 'Cumulative Velocity Error']\nerrors = [pos_error, vel_error]\n\nax.bar(labels, errors, color=['blue', 'green'])\nax.set_ylabel('Error')\nax.set_title('Cumulative Position and Velocity Errors')\nax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nCumulative energy error for the selected orbit: 0.00018692017\nCumulative position error for the selected orbit: 1.4306169667270132e-05\nCumulative velocity error for the selected orbit: 0.00011048713047470115\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ncalculate_errors\n\n calculate_errors (orbit_data:numpy.ndarray, mu:float,\n                   orbit_indices:List[int]=None,\n                   error_types:List[str]=['position', 'velocity',\n                   'energy'], time_step:Optional[float]=None,\n                   display_results:bool=True, cumulative:bool=False)\n\nCalculate and return the cumulative error and the average error per time step for the selected orbits together. Optionally, display the evolution of each error as a chart.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\n3D array of orbit data\n\n\nmu\nfloat\n\nGravitational parameter\n\n\norbit_indices\nList\nNone\nList of integers referring to the orbits to analyze\n\n\nerror_types\nList\n[‘position’, ‘velocity’, ‘energy’]\nTypes of errors to calculate\n\n\ntime_step\nOptional\nNone\nOptional time step if time dimension is not included\n\n\ndisplay_results\nbool\nTrue\nBoolean to control whether to display the results\n\n\ncumulative\nbool\nFalse\nBoolean to control cumulative or average error\n\n\nReturns\nDict\n\n\n\n\n\n\nerrors = calculate_errors(orbit_data, EM_MU, orbit_indices = [0, 1, 2])\n\nCumulative position error for selected orbits: 0.0014242519694434052\nAverage position error per time step: 4.7954611765771215e-06\n\n\n\n\n\n\n\n\n\nCumulative velocity error for selected orbits: 0.0798643959065006\nAverage velocity error per time step: 0.00026890368992087745\n\n\n\n\n\n\n\n\n\nCumulative energy error for selected orbits: 0.06817007064819336\nAverage energy error per time step: 0.00022952885774429888\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ncalculate_errors_per_orbit\n\n calculate_errors_per_orbit (orbit_data:numpy.ndarray, mu:float,\n                             error_types:List[str]=['position',\n                             'velocity', 'energy'],\n                             time_step:Optional[float]=None,\n                             display_results:bool=False)\n\nCalculate and return the average error per orbit for the selected error types. Optionally, display the evolution of each error as a chart.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\n3D array of orbit data\n\n\nmu\nfloat\n\nGravitational parameter\n\n\nerror_types\nList\n[‘position’, ‘velocity’, ‘energy’]\nTypes of errors to calculate\n\n\ntime_step\nOptional\nNone\nOptional time step if time dimension is not included\n\n\ndisplay_results\nbool\nFalse\nBoolean to control whether to display the results\n\n\nReturns\nDict\n\n\n\n\n\n\nerrors = calculate_errors_per_orbit(orbit_data[0:3], EM_MU, display_results=True)\n\nAverage Position Error per Orbit:\n[5.14651324e-07 3.51359548e-07 1.35203727e-05]\nAverage Velocity Error per Orbit:\n[1.81187043e-06 1.44955704e-06 8.03449642e-04]\nAverage Energy Error per Orbit:\n[3.79543121e-06 2.81285747e-06 6.81978301e-04]",
    "crumbs": [
      "Propagation"
    ]
  }
]