[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Utils",
    "section": "",
    "text": "source\n\n\n\n load_orbit_data (file_path:str, variable_name:Optional[str]=None,\n                  dataset_path:Optional[str]=None)\n\nLoad orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .mat, .h5, or .npy file.\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nAny\n\nThe loaded orbit data.\n\n\n\n\nsource\n\n\n\n\n load_memmap_array (file_path:str, mode:str='c')\n\nLoad a .npy file as a memory-mapped array using numpy.memmap.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .npy file as a string.\n\n\nmode\nstr\nc\nMode for memory-mapping (‘r’, ‘r+’, ‘w+’, ‘c’).\n\n\nReturns\nmemmap\n\nReturns a memory-mapped array.\n\n\n\n\nsource\n\n\n\n\n get_orbit_features (file_path:str, variable_name:Optional[str]=None,\n                     dataset_path:Optional[str]=None)\n\nLoad orbit feature data from a specified file and convert it to a DataFrame.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the file (can be .mat, .h5, or .npy).\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nDataFrame\n\nDataFrame with detailed orbit features.",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#loading-data",
    "href": "data.html#loading-data",
    "title": "Data Utils",
    "section": "",
    "text": "source\n\n\n\n load_orbit_data (file_path:str, variable_name:Optional[str]=None,\n                  dataset_path:Optional[str]=None)\n\nLoad orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .mat, .h5, or .npy file.\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nAny\n\nThe loaded orbit data.\n\n\n\n\nsource\n\n\n\n\n load_memmap_array (file_path:str, mode:str='c')\n\nLoad a .npy file as a memory-mapped array using numpy.memmap.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the .npy file as a string.\n\n\nmode\nstr\nc\nMode for memory-mapping (‘r’, ‘r+’, ‘w+’, ‘c’).\n\n\nReturns\nmemmap\n\nReturns a memory-mapped array.\n\n\n\n\nsource\n\n\n\n\n get_orbit_features (file_path:str, variable_name:Optional[str]=None,\n                     dataset_path:Optional[str]=None)\n\nLoad orbit feature data from a specified file and convert it to a DataFrame.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nThe path to the file (can be .mat, .h5, or .npy).\n\n\nvariable_name\nOptional\nNone\nName of the variable in the .mat file, optional.\n\n\ndataset_path\nOptional\nNone\nPath to the dataset in the .h5 file, optional.\n\n\nReturns\nDataFrame\n\nDataFrame with detailed orbit features.",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#save-data",
    "href": "data.html#save-data",
    "title": "Data Utils",
    "section": "Save Data",
    "text": "Save Data\n\nsource\n\nsave_data\n\n save_data (data:numpy.ndarray, file_name:str)\n\nSave a numpy array to a file based on the file extension specified in file_name. Supports saving to HDF5 (.hdf5) or NumPy (.npy) file formats.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe numpy array data to save.\n\n\nfile_name\nstr\nThe name of the file to save the data in, including the extension.\n\n\nReturns\nNone",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#get-example-data",
    "href": "data.html#get-example-data",
    "title": "Data Utils",
    "section": "Get Example Data",
    "text": "Get Example Data\n\nsource\n\nget_example_orbit_data\n\n get_example_orbit_data ()\n\nLoad example orbit data from a numpy file located in the example_data directory.\n\ndata = get_example_orbit_data()\ndata.shape\n\n(400, 7, 100)",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#order-labels-and-array-given-target",
    "href": "data.html#order-labels-and-array-given-target",
    "title": "Data Utils",
    "section": "Order labels and array given target",
    "text": "Order labels and array given target\n\nsource\n\norder_labels_and_array_with_target\n\n order_labels_and_array_with_target (labels:numpy.ndarray,\n                                     array:numpy.ndarray,\n                                     target_label:str,\n                                     place_at_end:bool=False)\n\nOrders labels and array by placing entries with target_label either at start or end.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlabels\nndarray\n\nArray of labels to be ordered\n\n\narray\nndarray\n\nArray to be ordered according to labels\n\n\ntarget_label\nstr\n\nLabel to order by\n\n\nplace_at_end\nbool\nFalse\nWhether to place target label at end\n\n\nReturns\ntuple\n\nReturns ordered labels and array\n\n\n\n\n# Sample labels and a sample 3D array\nlabels = np.array(['apple', 'banana', 'apple', 'orange', 'banana', 'grape'])\narray = np.array([[[1, 2], [3, 4]], \n                  [[5, 6], [7, 8]], \n                  [[9, 10], [11, 12]], \n                  [[13, 14], [15, 16]], \n                  [[17, 18], [19, 20]], \n                  [[21, 22], [23, 24]]])\ntarget_label = 'apple'\n\nordered_labels, ordered_array = order_labels_and_array_with_target(labels, array, target_label)\n\nprint(ordered_labels)\nprint(ordered_array)\n\n['apple' 'apple' 'banana' 'orange' 'banana' 'grape']\n[[[ 1  2]\n  [ 3  4]]\n\n [[ 9 10]\n  [11 12]]\n\n [[ 5  6]\n  [ 7  8]]\n\n [[13 14]\n  [15 16]]\n\n [[17 18]\n  [19 20]]\n\n [[21 22]\n  [23 24]]]",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#random-sampler",
    "href": "data.html#random-sampler",
    "title": "Data Utils",
    "section": "Random Sampler",
    "text": "Random Sampler\n\nsource\n\nsample_orbits\n\n sample_orbits (orbit_data:numpy.ndarray, sample_spec:Union[dict,int],\n                labels:Optional[numpy.ndarray]=None)\n\nRandomly sample orbits from the provided dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\nArray of orbit data with shape (num_orbits, 6, num_time_points)\n\n\nsample_spec\nUnion\n\nNumber of samples per class (dict) or total samples (int)\n\n\nlabels\nOptional\nNone\nArray of labels for each orbit\n\n\nReturns\ntuple",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#random-discarder",
    "href": "data.html#random-discarder",
    "title": "Data Utils",
    "section": "Random Discarder",
    "text": "Random Discarder\n\nsource\n\ndiscard_random_labels\n\n discard_random_labels (data:numpy.ndarray, labels:numpy.ndarray,\n                        discard_labels:Union[List,Dict,int])\n\n*Discards random or specified labels from the dataset.\nReturns tuple of (discarded labels, filtered data, filtered labels).*\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nDataset to filter\n\n\nlabels\nndarray\nLabels corresponding to the data\n\n\ndiscard_labels\nUnion\nLabels to discard - list, dict or number\n\n\nReturns\nTuple",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#remove-duplicates-preserve-order",
    "href": "data.html#remove-duplicates-preserve-order",
    "title": "Data Utils",
    "section": "Remove Duplicates preserve Order",
    "text": "Remove Duplicates preserve Order\n\nsource\n\nremove_duplicates_preserve_order\n\n remove_duplicates_preserve_order (input_list:List)\n\nRemoves duplicate items from a list while preserving the original order.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_list\nList\nInput list that may contain duplicates\n\n\nReturns\nList\nReturns list with duplicates removed while preserving order",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#dataloaders",
    "href": "data.html#dataloaders",
    "title": "Data Utils",
    "section": "Dataloaders",
    "text": "Dataloaders\n\nsource\n\ncreate_dataloaders\n\n create_dataloaders (scaled_data:torch.Tensor, val_split:float=0.2,\n                     batch_size:int=32)\n\nCreates train and validation dataloaders from input tensor data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nscaled_data\nTensor\n\nInput tensor of scaled data\n\n\nval_split\nfloat\n0.2\nFraction of data to use for validation\n\n\nbatch_size\nint\n32\nBatch size for dataloaders\n\n\nReturns\nTuple\n\nReturns train and optional val dataloaders",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "data.html#scaler",
    "href": "data.html#scaler",
    "title": "Data Utils",
    "section": "Scaler",
    "text": "Scaler\n/usr/local/lib/python3.10/dist-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Parameters:\n  else: warn(msg)\n/usr/local/lib/python3.10/dist-packages/fastcore/docscrape.py:230: UserWarning: Unknown section Attributes:\n  else: warn(msg)\n\nsource\n\nTSFeatureWiseScaler\n\n TSFeatureWiseScaler (feature_range:tuple=(0, 1))\n\nScales time series data feature-wise using PyTorch tensors.\n\nsource\n\n\nTSGlobalScaler\n\n TSGlobalScaler ()\n\nScales time series data globally using PyTorch tensors.",
    "crumbs": [
      "Data Utils"
    ]
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "source\n\n\n\n get_orbit_data_from_hdf5 (file_path:str)\n\nLoad orbit data from an HDF5 file.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nTuple\nDictionary of orbits with numerical keys.",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#read-data",
    "href": "dataset.html#read-data",
    "title": "Dataset",
    "section": "",
    "text": "source\n\n\n\n get_orbit_data_from_hdf5 (file_path:str)\n\nLoad orbit data from an HDF5 file.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nTuple\nDictionary of orbits with numerical keys.",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-features",
    "href": "dataset.html#get-features",
    "title": "Dataset",
    "section": "Get Features",
    "text": "Get Features\n\nOrbit Features\n\nsource\n\n\nget_orbit_features_from_hdf5\n\n get_orbit_features_from_hdf5 (file_path:str)\n\nLoad orbit DataFrame from an HDF5 file.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nDataFrame\nDataFrame containing orbit features.\n\n\n\n\nsource\n\n\nget_orbit_features_from_folder\n\n get_orbit_features_from_folder (folder_path:str)\n\nConcatenate orbit DataFrames from all HDF5 files in a folder, preserving original index and adding system column.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfolder_path\nstr\nPath to the folder\n\n\nReturns\nDataFrame\nDataFrame containing concatenated orbit features.\n\n\n\n\n\nSystem Features\n\ndef get_system_data_from_hdf5(file_path: str              # Path to the HDF5 file.\n                             ) -&gt; Dict[str, float]:       # Dictionary containing system features.\n    \"\"\"\n    Load system data from an HDF5 file.\n    \"\"\"\n    with h5py.File(file_path, 'r') as file:\n        # Extract system features and labels\n        system_features = file['system_features'][:]\n        system_labels = file['system_labels'][:].astype(str)\n        \n        # Create a dictionary for system\n        system_dict = {label: feature[0] for label, feature in zip(system_labels.flatten().tolist(), system_features)}\n        \n    return system_dict\n\n\ndef get_system_features_from_folder(folder_path: str    # Path to the folder\n                                   ) -&gt; pd.DataFrame:   # DataFrame containing concatenated system features.\n    \"\"\"\n    Concatenate system DataFrames from all HDF5 files in a folder, preserving original index and adding system column.\n    \"\"\"\n    all_systems = []  # List to store individual system dictionaries\n\n    # Iterate over all files in the folder\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n\n        # Check if the file is an HDF5 file\n        if file_name.endswith('.h5') or file_name.endswith('.hdf5'):\n            # Get the system dictionary from the HDF5 file\n            system_dict = get_system_data_from_hdf5(file_path)\n            \n            # Add a new entry to the dictionary for the system name\n            system_dict['system'] = os.path.splitext(file_name)[0].split('_')[0]\n            \n            # Append the dictionary to the list\n            all_systems.append(system_dict)\n\n    # Convert the list of dictionaries to a DataFrame\n    concatenated_df = pd.DataFrame(all_systems)\n    \n    return concatenated_df",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-classes",
    "href": "dataset.html#get-classes",
    "title": "Dataset",
    "section": "Get Classes",
    "text": "Get Classes\n\nsource\n\nsubstitute_values_from_df\n\n substitute_values_from_df (values:List[Any],\n                            df:pandas.core.frame.DataFrame,\n                            goal_column:str, id_column:str='Id')\n\n*Substitute values in the given list based on the mapping from a DataFrame’s id column to goal column.\nParameters: values (List[Any]): List of values to be substituted. df (pd.DataFrame): DataFrame containing the mapping from id_column to goal_column. goal_column (str): Column in the DataFrame to get the substitution values from. id_column (str, optional): Column in the DataFrame to match the values with. Default is ‘Id’.\nReturns: List[Any]: A list with substituted values from the DataFrame’s goal_column.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalues\nList\n\nList of values to be substituted.\n\n\ndf\nDataFrame\n\nDataFrame containing the mapping.\n\n\ngoal_column\nstr\n\nColumn in the DataFrame to get the substitution values from.\n\n\nid_column\nstr\nId\nColumn in the DataFrame to match the values with. Default is ‘Id’.\n\n\nReturns\nList\n\n\n\n\n\n\nsource\n\n\nget_orbit_classes\n\n get_orbit_classes (values:List[Any])\n\n*Get orbit classes based on the given values and DataFrame. Returns four lists corresponding to ‘Label’, ‘Type’, ‘Subtype’, and ‘Direction’ columns.\nParameters: values (List[Any]): List of values to be substituted.\nReturns: Tuple[List[Any], List[Any], List[Any], List[Any]]: Four lists with substituted values from ‘Label’, ‘Type’, ‘Subtype’, and ‘Direction’ columns.*\n\nvalues = [1,7,23]\nget_orbit_classes(values)\n\n(['S_BN', 'S_L1_A', 'S_L4_LP'],\n ['System-wide', 'L1', 'L4'],\n ['Butterfly', 'Axial', 'Long Period'],\n ['North', 'No specification', 'No specification'])",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-periods",
    "href": "dataset.html#get-periods",
    "title": "Dataset",
    "section": "Get Periods",
    "text": "Get Periods\n\nsource\n\nget_periods_of_orbit_dict\n\n get_periods_of_orbit_dict (orbits:Dict[int,numpy.ndarray],\n                            propagated_periods:Dict[int,int],\n                            desired_periods:int)\n\nProcess the orbits to extract the desired periods and print the percentage of the dataset returned.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\npropagated_periods\nDict\nDictionary of propagated periods for each orbit.\n\n\ndesired_periods\nint\nDesired number of periods.\n\n\nReturns\nDict\nProcessed dictionary of orbits.",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-dataset",
    "href": "dataset.html#get-dataset",
    "title": "Dataset",
    "section": "Get Dataset",
    "text": "Get Dataset\n\nFixed Period\n\nsource\n\n\nget_first_period_of_fixed_period_dataset\n\n get_first_period_of_fixed_period_dataset (file_path:str)\n\nLoad and process orbit data from an HDF5 file for the first period.\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nReturns\nTuple\n3D numpy array of padded orbits.\n\n\n\n\n\nFixed Step\n\nsource\n\n\nget_full_fixed_step_dataset\n\n get_full_fixed_step_dataset (file_path:str, segment_length:int)\n\nLoad and process orbit data from an HDF5 file, segmenting each orbit into specified length.\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nsegment_length\nint\nDesired length of each segment.\n\n\nReturns\nTuple\n3D numpy array of segmented orbits.\n\n\n\n\nsource\n\n\nget_first_period_fixed_step_dataset\n\n get_first_period_fixed_step_dataset (file_path:str, segment_length:int)\n\nLoad and process orbit data from an HDF5 file, segmenting each orbit into specified length.\n\n\n\n\nType\nDetails\n\n\n\n\nfile_path\nstr\nPath to the HDF5 file.\n\n\nsegment_length\nint\nDesired length of each segment.\n\n\nReturns\nTuple\n3D numpy array of segmented orbits.\n\n\n\n\n\nFirst Period\n\nsource\n\n\nget_first_period_dataset\n\n get_first_period_dataset (file_path:str,\n                           segment_length:Optional[int]=100)\n\nLoad orbit data based on the file path. Calls the appropriate function depending on the name of the file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nPath to the HDF5 file.\n\n\nsegment_length\nOptional\n100\nDesired length of each segment, optional.\n\n\nReturns\nTuple\n\nMemmap of segmented orbits.\n\n\n\n\nsource\n\n\nget_first_period_dataset_all_systems\n\n get_first_period_dataset_all_systems (folder_path:str,\n                                       segment_length:Optional[int]=100)\n\n*Processes all system files in a folder, concatenates their data while maintaining order.\nParameters: folder_path (str): Path to the folder containing system files. segment_length (Optional[int]): Desired length of each segment.\nReturns: Tuple[np.memmap, pd.DataFrame, np.ndarray, Dict[str, float]]: - Concatenated orbits as a 3D NumPy memmap. - Concatenated orbit DataFrame with an added ‘system’ column. - Concatenated orbit IDs as a NumPy array. - Merged system dictionary with keys prefixed by system names.*",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "dataset.html#get-constants",
    "href": "dataset.html#get-constants",
    "title": "Dataset",
    "section": "Get Constants",
    "text": "Get Constants\n\nsource\n\nget_system_constants\n\n get_system_constants (system_dict, system_labels, constant)\n\n*Extracts values for a specified constant from a system dictionary based on system labels.\nParameters: system_dict (dict): Dictionary containing system constants for different systems. system_labels (np.ndarray): Array of system labels. constant (str): The constant to extract (e.g., ‘mu’, ‘LU’, etc.).\nReturns: np.ndarray: Array of constant values corresponding to the system labels.*",
    "crumbs": [
      "Dataset"
    ]
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "VAE",
    "section": "",
    "text": "source\n\nAbstractVAE\n\n AbstractVAE (seq_len:int, feat_dim:int, latent_dim:int)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nBetaVAE\n\n BetaVAE (encoder:torch.nn.modules.module.Module,\n          decoder:torch.nn.modules.module.Module, beta:float=1.0,\n          loss_fn=None, optimizer_cls=&lt;class 'torch.optim.adam.Adam'&gt;,\n          lr:float=0.001, **kwargs)\n\nHooks to be used in LightningModule.\n\nsource\n\n\ncBetaVAE\n\n cBetaVAE (encoder:torch.nn.modules.module.Module,\n           decoder:torch.nn.modules.module.Module, beta:float=1.0,\n           loss_fn=None, optimizer_cls=&lt;class 'torch.optim.adam.Adam'&gt;,\n           lr:float=0.001, **kwargs)\n\nHooks to be used in LightningModule.",
    "crumbs": [
      "VAE"
    ]
  },
  {
    "objectID": "latent_space.html",
    "href": "latent_space.html",
    "title": "Latent Space",
    "section": "",
    "text": "source\n\n\n\n plot_2d_latent_space (latent_representations:numpy.ndarray,\n                       labels:numpy.ndarray,\n                       latent_stdevs:Optional[numpy.ndarray]=None,\n                       features:Optional[Any]=None,\n                       feature_names:Optional[List[str]]=None,\n                       figsize:tuple=(12, 12),\n                       save_path:Optional[str]=None,\n                       many_classes:bool=False, show_legend:bool=True,\n                       legend_fontsize:int=8, plot_std:bool=True,\n                       title:Optional[str]='2D Latent Space\n                       Visualization', title_size:int=14,\n                       axis_labels:Optional[Tuple[str,str]]=('Dimension\n                       1', 'Dimension 2'), normalize_data:bool=True,\n                       **kwargs:Any)\n\n*Plots a 2D latent space visualization with class labels and feature distributions.\nParameters: - latent_representations: np.ndarray, shape (n_samples, 2) The 2D coordinates of the latent representations. - labels: np.ndarray, shape (n_samples,) The class labels for each sample. - features: Optional[Any], shape (n_samples, n_features) The feature data to plot distributions. Can be a list or a NumPy array. - feature_names: Optional[List[str]] The names of the features. - figsize: tuple, default (12, 12) The size of the entire figure. - save_path: Optional[str] Path to save the figure. If None, the plot is not saved. - many_classes: bool, default False If True, uses different markers for classes. - show_legend: bool, default True If True, displays legends. - legend_fontsize: int, default 8 Font size for the legends. - plot_std: bool, default True If True, plots the standard deviation shading for feature distributions. - title: Optional[str], default ‘2D Latent Space Visualization’ Title of the plot. - title_size: int, default 14 Font size for the title. - axis_labels: Optional[Tuple[str, str]], default (‘Dimension 1’, ‘Dimension 2’) Labels for the X and Y axes. - normalize_data: bool, default False If True, normalizes the latent representations. - kwargs: Any Additional keyword arguments passed to scatter plots.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\n\n\n\nlabels\nndarray\n\n\n\n\nlatent_stdevs\nOptional\nNone\n\n\n\nfeatures\nOptional\nNone\n\n\n\nfeature_names\nOptional\nNone\n\n\n\nfigsize\ntuple\n(12, 12)\n\n\n\nsave_path\nOptional\nNone\n\n\n\nmany_classes\nbool\nFalse\n\n\n\nshow_legend\nbool\nTrue\n\n\n\nlegend_fontsize\nint\n8\n\n\n\nplot_std\nbool\nTrue\n\n\n\ntitle\nOptional\n2D Latent Space Visualization\nNew title parameter\n\n\ntitle_size\nint\n14\nNew title_size parameter\n\n\naxis_labels\nOptional\n(‘Dimension 1’, ‘Dimension 2’)\nNew axis_labels parameter\n\n\nnormalize_data\nbool\nTrue\nNew parameter to control normalization\n\n\nkwargs\nAny\n\n\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\n\n\n plot_combined_2d_latent_space (real_latent:numpy.ndarray,\n                                synthetic_latent:numpy.ndarray, synthetic_\n                                labels:Union[int,List[int],numpy.ndarray,N\n                                oneType]=None, figsize:tuple=(12, 9),\n                                save_path:Optional[str]=None,\n                                show_legend:bool=True,\n                                axis_labels:tuple=('X-axis', 'Y-axis'),\n                                title:Optional[str]=None,\n                                colormap:str='viridis',\n                                feature_title:Optional[str]='Feature\n                                Value', label_names:Optional[dict]=None)\n\n*Plots the combined latent space of real and synthetic data. Assumes the latent space is 2D. If synthetic_latent is a 3D array, it plots arrows. Numeric annotations for arrows are only displayed if synthetic_labels are provided.\nArgs: real_latent (np.ndarray): Latent representations of real data. synthetic_latent (np.ndarray): Latent representations of synthetic data or arrows. synthetic_labels (Optional[Union[int, List[int], np.ndarray]]): Labels for synthetic data. Can be None, a single label, or a list of labels. figsize (tuple): Size of the figure. save_path (Optional[str]): Optional path to save the plot image. show_legend (bool): Flag to show or hide the legend. axis_labels (tuple): Labels for the X and Y axes. title (Optional[str]): Title of the plot. colormap (str): Colormap to use when coloring by features. feature_title (Optional[str]): Title for the feature color bar. label_names (Optional[dict]): Dictionary mapping label values to names for discrete labels.\nReturns: None*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreal_latent\nndarray\n\nLatent representations of real data.\n\n\nsynthetic_latent\nndarray\n\nLatent representations of synthetic data or arrows.\n\n\nsynthetic_labels\nUnion\nNone\nLabels for synthetic data. Can be None, a single label, or a list of labels.\n\n\nfigsize\ntuple\n(12, 9)\nSize of the figure.\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image.\n\n\nshow_legend\nbool\nTrue\nFlag to show or hide the legend.\n\n\naxis_labels\ntuple\n(‘X-axis’, ‘Y-axis’)\nLabels for the X and Y axes.\n\n\ntitle\nOptional\nNone\nTitle of the plot.\n\n\ncolormap\nstr\nviridis\nColormap to use when coloring by features.\n\n\nfeature_title\nOptional\nFeature Value\nTitle for the feature color bar.\n\n\nlabel_names\nOptional\nNone\nNew parameter: dictionary mapping label values to names\n\n\nReturns\nNone",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#plot-2-dimensions",
    "href": "latent_space.html#plot-2-dimensions",
    "title": "Latent Space",
    "section": "",
    "text": "source\n\n\n\n plot_2d_latent_space (latent_representations:numpy.ndarray,\n                       labels:numpy.ndarray,\n                       latent_stdevs:Optional[numpy.ndarray]=None,\n                       features:Optional[Any]=None,\n                       feature_names:Optional[List[str]]=None,\n                       figsize:tuple=(12, 12),\n                       save_path:Optional[str]=None,\n                       many_classes:bool=False, show_legend:bool=True,\n                       legend_fontsize:int=8, plot_std:bool=True,\n                       title:Optional[str]='2D Latent Space\n                       Visualization', title_size:int=14,\n                       axis_labels:Optional[Tuple[str,str]]=('Dimension\n                       1', 'Dimension 2'), normalize_data:bool=True,\n                       **kwargs:Any)\n\n*Plots a 2D latent space visualization with class labels and feature distributions.\nParameters: - latent_representations: np.ndarray, shape (n_samples, 2) The 2D coordinates of the latent representations. - labels: np.ndarray, shape (n_samples,) The class labels for each sample. - features: Optional[Any], shape (n_samples, n_features) The feature data to plot distributions. Can be a list or a NumPy array. - feature_names: Optional[List[str]] The names of the features. - figsize: tuple, default (12, 12) The size of the entire figure. - save_path: Optional[str] Path to save the figure. If None, the plot is not saved. - many_classes: bool, default False If True, uses different markers for classes. - show_legend: bool, default True If True, displays legends. - legend_fontsize: int, default 8 Font size for the legends. - plot_std: bool, default True If True, plots the standard deviation shading for feature distributions. - title: Optional[str], default ‘2D Latent Space Visualization’ Title of the plot. - title_size: int, default 14 Font size for the title. - axis_labels: Optional[Tuple[str, str]], default (‘Dimension 1’, ‘Dimension 2’) Labels for the X and Y axes. - normalize_data: bool, default False If True, normalizes the latent representations. - kwargs: Any Additional keyword arguments passed to scatter plots.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\n\n\n\nlabels\nndarray\n\n\n\n\nlatent_stdevs\nOptional\nNone\n\n\n\nfeatures\nOptional\nNone\n\n\n\nfeature_names\nOptional\nNone\n\n\n\nfigsize\ntuple\n(12, 12)\n\n\n\nsave_path\nOptional\nNone\n\n\n\nmany_classes\nbool\nFalse\n\n\n\nshow_legend\nbool\nTrue\n\n\n\nlegend_fontsize\nint\n8\n\n\n\nplot_std\nbool\nTrue\n\n\n\ntitle\nOptional\n2D Latent Space Visualization\nNew title parameter\n\n\ntitle_size\nint\n14\nNew title_size parameter\n\n\naxis_labels\nOptional\n(‘Dimension 1’, ‘Dimension 2’)\nNew axis_labels parameter\n\n\nnormalize_data\nbool\nTrue\nNew parameter to control normalization\n\n\nkwargs\nAny\n\n\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\n\n\n plot_combined_2d_latent_space (real_latent:numpy.ndarray,\n                                synthetic_latent:numpy.ndarray, synthetic_\n                                labels:Union[int,List[int],numpy.ndarray,N\n                                oneType]=None, figsize:tuple=(12, 9),\n                                save_path:Optional[str]=None,\n                                show_legend:bool=True,\n                                axis_labels:tuple=('X-axis', 'Y-axis'),\n                                title:Optional[str]=None,\n                                colormap:str='viridis',\n                                feature_title:Optional[str]='Feature\n                                Value', label_names:Optional[dict]=None)\n\n*Plots the combined latent space of real and synthetic data. Assumes the latent space is 2D. If synthetic_latent is a 3D array, it plots arrows. Numeric annotations for arrows are only displayed if synthetic_labels are provided.\nArgs: real_latent (np.ndarray): Latent representations of real data. synthetic_latent (np.ndarray): Latent representations of synthetic data or arrows. synthetic_labels (Optional[Union[int, List[int], np.ndarray]]): Labels for synthetic data. Can be None, a single label, or a list of labels. figsize (tuple): Size of the figure. save_path (Optional[str]): Optional path to save the plot image. show_legend (bool): Flag to show or hide the legend. axis_labels (tuple): Labels for the X and Y axes. title (Optional[str]): Title of the plot. colormap (str): Colormap to use when coloring by features. feature_title (Optional[str]): Title for the feature color bar. label_names (Optional[dict]): Dictionary mapping label values to names for discrete labels.\nReturns: None*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreal_latent\nndarray\n\nLatent representations of real data.\n\n\nsynthetic_latent\nndarray\n\nLatent representations of synthetic data or arrows.\n\n\nsynthetic_labels\nUnion\nNone\nLabels for synthetic data. Can be None, a single label, or a list of labels.\n\n\nfigsize\ntuple\n(12, 9)\nSize of the figure.\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image.\n\n\nshow_legend\nbool\nTrue\nFlag to show or hide the legend.\n\n\naxis_labels\ntuple\n(‘X-axis’, ‘Y-axis’)\nLabels for the X and Y axes.\n\n\ntitle\nOptional\nNone\nTitle of the plot.\n\n\ncolormap\nstr\nviridis\nColormap to use when coloring by features.\n\n\nfeature_title\nOptional\nFeature Value\nTitle for the feature color bar.\n\n\nlabel_names\nOptional\nNone\nNew parameter: dictionary mapping label values to names\n\n\nReturns\nNone",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#reduce-dimensions",
    "href": "latent_space.html#reduce-dimensions",
    "title": "Latent Space",
    "section": "Reduce dimensions",
    "text": "Reduce dimensions\n\nsource\n\nreduce_dimensions_latent_space\n\n reduce_dimensions_latent_space (latent_representations:numpy.ndarray,\n                                 labels:numpy.ndarray,\n                                 techniques:List[str]=['PCA'],\n                                 n_components:int=2, figsize:tuple=(12,\n                                 9), save_path:Optional[str]=None,\n                                 many_classes:bool=False,\n                                 grid_view:bool=True,\n                                 class_names:Optional[List[str]]=None,\n                                 show_legend:bool=True, plot:bool=True,\n                                 **kwargs:Any)\n\n*Reduces dimensions of latent representations using specified techniques and optionally plots the results.\nReturns: A dictionary containing the reduced latent space for each technique.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\nPrecomputed latent representations (numpy array).\n\n\nlabels\nndarray\n\nLabels for the data points, used for coloring in the plot.\n\n\ntechniques\nList\n[‘PCA’]\nTechniques to use for reduction (‘PCA’, ‘t-SNE’, ‘UMAP’, ‘LDA’).\n\n\nn_components\nint\n2\nNumber of dimensions to reduce to (1, 2, or 3).\n\n\nfigsize\ntuple\n(12, 9)\nSize of the figure for each subplot.\n\n\nsave_path\nOptional\nNone\nOptional path to save the plot image.\n\n\nmany_classes\nbool\nFalse\nFlag to use enhanced plotting for many classes.\n\n\ngrid_view\nbool\nTrue\nFlag to plot all techniques in a single grid view.\n\n\nclass_names\nOptional\nNone\nOptional class names for the legend\n\n\nshow_legend\nbool\nTrue\nFlag to show or hide the legend\n\n\nplot\nbool\nTrue\nFlag to plot the latent space\n\n\nkwargs\nAny\n\n\n\n\nReturns\nDict\n\nAdditional keyword arguments for dimensionality reduction methods.\n\n\n\n\nsource\n\n\nreduce_dimensions_combined_latent_space\n\n reduce_dimensions_combined_latent_space (train_latent:numpy.ndarray,\n                                          val_latent:numpy.ndarray, train_\n                                          labels:Optional[numpy.ndarray]=N\n                                          one,\n                                          techniques:List[str]=['PCA'],\n                                          n_components:int=2,\n                                          **kwargs:Any)\n\n*Reduces dimensions of latent representations using specified techniques.\nReturns: A dictionary containing the reduced latent space for each technique and dataset (train and val).*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_latent\nndarray\n\nLatent representations of training data.\n\n\nval_latent\nndarray\n\nLatent representations of validation data.\n\n\ntrain_labels\nOptional\nNone\nLabels for the training data points (optional for LDA).\n\n\ntechniques\nList\n[‘PCA’]\nTechniques to use for reduction (‘PCA’, ‘t-SNE’, ‘UMAP’, ‘LDA’).\n\n\nn_components\nint\n2\nNumber of dimensions to reduce to (1, 2, or 3).\n\n\nkwargs\nAny\n\n\n\n\nReturns\nDict\n\nAdditional keyword arguments for dimensionality reduction methods.\n\n\n\n\nfrom orbit_generation.data import get_example_orbit_data\n\n\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n# Reshape data to 2D (num_orbits, 6 * num_time_points)\norbit_data_reshaped = orbit_data.reshape(200, -1)\n\n# Use PCA to reduce to a lower-dimensional space (e.g., 10 dimensions)\npca = PCA(n_components=10)\nlatent_representations = pca.fit_transform(orbit_data_reshaped)\n\nlabels = np.random.randint(0, 5, size=200)  # 5 different classes\n\nreduced_latent_spaces = reduce_dimensions_latent_space(latent_representations, labels, techniques=['UMAP','LDA'])\nreduced_latent_spaces['UMAP'].shape\n\n\n\n\n\n\n\n\n\nreduced_latent_spaces=reduce_dimensions_latent_space(latent_representations, labels, techniques=['PCA'], n_components=1, many_classes=True)\nreduced_latent_spaces['PCA'].shape\n\n\n\n\n\n\n\n\n\nreduced_latent_spaces=reduce_dimensions_latent_space(latent_representations, labels, techniques=['t-SNE'], n_components=3)\nreduced_latent_spaces['t-SNE'].shape",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#sampling",
    "href": "latent_space.html#sampling",
    "title": "Latent Space",
    "section": "Sampling",
    "text": "Sampling\n\nsource\n\nsample_random_distributions\n\n sample_random_distributions (means, log_vars, n_samples:int,\n                              log_var_multiplier:float=1.0)\n\n\nsource\n\n\ninterpolate_sample\n\n interpolate_sample (centroids, granularity=10, variance=0.0)\n\n*Perform interpolating sampling between all pairs of centroids.\nParameters: - centroids (np.ndarray): Array of shape (n_centroids, latent_dim). - granularity (int): Number of interpolation steps between each pair. - variance (float): Standard deviation for Gaussian sampling.\nReturns: - samples (np.ndarray): Array of sampled points.*\n\nsource\n\n\nslerp\n\n slerp (z1, z2, steps)\n\nPerform spherical linear interpolation between two points.\n\nsource\n\n\nlinear_interpolation\n\n linear_interpolation (z1, z2, steps)\n\nPerform linear interpolation between two points.\n\n# Define example centroids for a 2-dimensional latent space\ncentroids = np.array([\n    [1.0, 2.0],\n    [3.0, 4.0],\n    [5.0, 6.0]\n])\n\ngranularity = 3\nvariance = 0.0  # Set to 0 for deterministic interpolation\n\nsampled_points = interpolate_sample(centroids, granularity, variance)\n\n# Define the expected sampled points manually for granularity=3\nexpected_data = np.array([\n    [1.0, 2.0],\n    [2.0, 3.0],\n    [3.0, 4.0],\n    [1.0, 2.0],\n    [3.0, 4.0],\n    [5.0, 6.0],\n    [3.0, 4.0],\n    [4.0, 5.0],\n    [5.0, 6.0]\n])\n\n# Check the sampled points against the expected data\ntest_eq(sampled_points, expected_data)\n\n\nsource\n\n\ngrid_sample\n\n grid_sample (encodings:numpy.ndarray,\n              grid_size:Union[int,Tuple[int,...]]=100)\n\n\ndef visual_test():\n    # Generate random encodings\n    np.random.seed(42)\n    encodings = np.random.rand(100, 2) * 100  # 100 points in a 100x100 space\n\n    # Perform grid sampling\n    grid_size = (10, 10)\n    sampled_grid = grid_sample(encodings, grid_size)\n\n    # Calculate bounds for visualization\n    x_min, y_min = np.min(encodings, axis=0)\n    x_max, y_max = np.max(encodings, axis=0)\n\n    # Plot the encodings\n    plt.figure(figsize=(8, 8))\n    plt.scatter(encodings[:, 0], encodings[:, 1], c='blue', label='Encodings')\n\n    # Plot the sampled grid points\n    plt.scatter(sampled_grid[:, 0], sampled_grid[:, 1], c='red', marker='o', label='Sampled Grid Points')\n\n    # Draw grid lines for reference\n    x_edges = np.linspace(x_min, x_max, grid_size[0] + 1)\n    y_edges = np.linspace(y_min, y_max, grid_size[1] + 1)\n\n    plt.title('Grid Sampling Visualization with Sampled Grid Points')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.legend()\n    plt.grid(False)\n    plt.show()\n\n# Run the visual test\nvisual_test()",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#explore",
    "href": "latent_space.html#explore",
    "title": "Latent Space",
    "section": "Explore",
    "text": "Explore\n\nsource\n\ntrimmed_mean_centroid\n\n trimmed_mean_centroid (points, trim_ratio=0.1)\n\n\nsource\n\n\ncompute_medoid\n\n compute_medoid (points)\n\n\nsource\n\n\ngeometric_median\n\n geometric_median (points, tol=1e-05)\n\n\nsource\n\n\ncompute_centroids\n\n compute_centroids (latents, labels, method='mean', return_labels=False,\n                    **kwargs)\n\n*Compute the centroid of each class in the latent space using various methods.\nParameters: - latents (np.ndarray): Array of shape (n_samples, latent_dim). - labels (np.ndarray): Array of shape (n_samples,) with class labels. - method (str): Method to compute centroids. Options: ‘mean’, ‘median’, ‘geom_median’, ‘medoid’, ‘trimmed_mean’, ‘gmm’. - return_labels (bool): If True, also return the unique labels corresponding to the centroids. - kwargs: Additional arguments for specific methods.\nReturns: - centroids (np.ndarray): Array of shape (n_classes, latent_dim) containing centroids. - unique_labels (np.ndarray, optional): Array of shape (n_classes,) with unique class labels.*",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "latent_space.html#measure",
    "href": "latent_space.html#measure",
    "title": "Latent Space",
    "section": "Measure",
    "text": "Measure\n\nsource\n\nplot_linear_regression\n\n plot_linear_regression (latent_means, features, feature_names,\n                         normalize=False)\n\n*Perform linear regression for each feature, visualize the results, and return regression metrics.\nParameters: - latent_means: np.ndarray of shape (n_samples, latent_dim), the latent space coordinates. - features: np.ndarray of shape (n_samples, n_features), the feature values. - feature_names: List of strings representing the names of the features. - normalize: Boolean, whether to normalize the features and latent space (default: False).\nReturns: - results: Dictionary containing coefficients, intercepts, and R² values for each feature. - simple_results: Dictionary containing R² values for each feature with modified keys.*",
    "crumbs": [
      "Latent Space"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "orbit-generation",
    "section": "",
    "text": "This library has been built using nbdev, which means that the source code of the library is stored in Jupyter notebooks inside the nbs folder. These notebooks are then automatically converted into Python files inside the orbit_generation folder.\nApart from the library, we have included research experiments using the library in the nbs_experiments folder.\nFirst, we will review the library structure, functions, and finally, we will explain the experiments conducted.",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "orbit-generation",
    "section": "Structure",
    "text": "Structure\n.\n|-- .devcontainer: Configuration for the development container.  \n|  \n|-- .github: workflows for continuous integration in Git.  \n|  \n|-- data: The folder where datasets are stored, in GitHub only example data is included. \n|   |-- example_data\n|   |-- orbits_fix_1500 (needs to be downloaded)\n|\n|-- docs: Documentation files for the project. \n|\n|-- experiments: Results about the experiments performed. \n|\n|-- index_files: Auto-generated images to be used in the README.md. \n| \n|-- julia: Scripts written in Julia for specific computations. \n|   `-- convergence_algorithm.jl\n|\n|-- models: Some Machine Learning models used in the experiments.\n|   |-- family_classificators\n|   |-- orbit_generators\n|\n|-- nbs: Jupyter notebooks containing the source code for the library.  \n|   |-- 00_constants.ipynb\n|   |-- 01_data.ipynb\n|   |-- 02_orbit_processing.ipynb\n|   |-- 03_visualization.ipynb\n|   |-- 04_orbit_statistics.ipynb\n|   |-- 05_dataset.ipynb\n|   |-- 06_architectures.ipynb\n|   |-- 07_propagation.ipynb\n|   |-- 08_experiment.ipynb\n|   |-- 09_evaluation.ipynb\n|   |-- 10_vae.ipynb\n|   |-- 11_model_factory.ipynb\n|   |-- 12_convergence.ipynb\n|   |-- 13_latent_space.ipynb\n|   |-- 14_paper_specific.ipynb\n|   |-- index.ipynb\n|\n|-- nbs_experiments: Notebooks with research experiments using the library.  \n|   |-- 01_generative_discovery_em\n|   |-- 02_conditional_generation_systems\n|\n|-- orbit_generation: Auto-generated Python package containing the processed library code. \n|  \n|-- .gitignore: Defines files Git should ignore.\n|  \n|-- LICENSE: Project license file.  \n|  \n|-- MANIFEST.in: Specifies which files to include in the package distribution.   \n|  \n|-- README.md: Documentation for the project, auto-generated from index.ipynb.  \n|  \n|-- settings.ini: Configuration file for `nbdev`.  \n|  \n|-- setup.py: Script for installing the package.  \n\n## Library Modules\n\n```sh\npip install orbit_generation\n\n0. Constants\nThis module contains physical constants and orbit labels.\n\nfrom orbit_generation.constants import MU_BY_SYSTEM, EM_POINTS, EXTENDED_ORBIT_CLASSIFICATION\n\n\nMU_BY_SYSTEM\n\n{'SaE': 1.901109735892602e-07,\n 'MP': 1.611081404409632e-08,\n 'SaT': 0.0002366393158331484,\n 'EM': 0.01215058560962404,\n 'JE': 2.52801752854e-05,\n 'SE': 3.0542e-06,\n 'SM': 3.227154996101724e-07}\n\n\n\nEM_POINTS\n\n{'Moon': (0.987849414390376, 0, 0),\n 'Earth': (-0.01215058560962404, 0, 0),\n 'Lagrange 1': (0.8369, 0, 0),\n 'Lagrange 2': (1.1557, 0, 0),\n 'Lagrange 3': (-1.0051, 0, 0),\n 'Lagrange 4': (0.4879, 0.866, 0),\n 'Lagrange 5': (0.4879, -0.866, 0)}\n\n\n\nEXTENDED_ORBIT_CLASSIFICATION.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n\nId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\nLabel\nS_BN\nS_BS\nS_DN\nS_DPO\nS_DRO\nS_DS\nS_L1_A\nS_L1_HN\nS_L1_HS\nS_L1_L\n...\nS_R12\nS_R13\nS_R14\nS_R21\nS_R23\nS_R31\nS_R32\nS_R34\nS_R41\nS_R43\n\n\nType\nSystem-wide\nSystem-wide\nSystem-wide\nSystem-wide\nSystem-wide\nSystem-wide\nL1\nL1\nL1\nL1\n...\nResonant\nResonant\nResonant\nResonant\nResonant\nResonant\nResonant\nResonant\nResonant\nResonant\n\n\nSubtype\nButterfly\nButterfly\nDragonfly\nDistant Prograde\nDistant Retrograde\nDragonfly\nAxial\nHalo\nHalo\nLyapunov\n...\nResonant 1,2\nResonant 1,3\nResonant 1,4\nResonant 2,1\nResonant 2,3\nResonant 3,1\nResonant 3,2\nResonant 3,4\nResonant 4,1\nResonant 4,3\n\n\nDirection\nNorth\nSouth\nNorth\nPlanar\nPlanar\nSouth\nNo specification\nNorth\nSouth\nPlanar\n...\nPlanar\nPlanar\nPlanar\nPlanar\nPlanar\nPlanar\nPlanar\nPlanar\nPlanar\nPlanar\n\n\n\n\n5 rows × 42 columns\n\n\n\n\n\n1. Data\nThis module provides utilities for handling orbit data.\n\nfrom orbit_generation.data import get_example_orbit_data\n\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n(200, 6, 300)\n\n\n\nNumber of orbits: 200\nTime instants: 300\n\nEvery orbit dataset is organized within a three-dimensional NumPy array with the following structure:\n\ndata.shape = (num_orbits, 7, num_time_points)\n\n\nnum_orbits: Total number of distinct orbits in the dataset.\n\n7: Represents the seven scalar values for each orbit at each time point, typically including:\n\ntime: The time corresponding to each recorded state.\n\nposX, posY, posZ: Position components in the X, Y, and Z dimensions, respectively.\n\nvelX, velY, velZ: Velocity components in the X, Y, and Z dimensions, respectively.\n\n\nnum_time_points: Number of time instants at which the data for each orbit is recorded.\n\n\n\n2. Processing\nThis module performs various processing tasks on the orbit data described above, including downsampling, interpolation, and reshaping.\n\nfrom orbit_generation.processing import resample_3d_array\n\nresampled_orbit_data = resample_3d_array(data=orbit_data, axis=2, target_size= 100)\nresampled_orbit_data.shape\n\n(200, 6, 100)\n\n\n\nInitial time instants: 300\nTime instants after Resampling: 100\n\n\n\n3. Visualization\nThis module handles the visualization of orbit trajectories and their features.\n\nfrom orbit_generation.visualize import visualize_static_orbits, export_dynamic_orbits_html\n\nvisualize_static_orbits(orbit_data, show_legend=False)\n\n\n\n\n\n\n\n\n\nvisualize_static_orbits(resampled_orbit_data, orbit_indices=[15,20,70,140,190], point_dict=EM_POINTS)\n\n\n\n\n\n\n\n\n\nexport_dynamic_orbits_html(data=orbit_data, filename='../data/example_data/example_orbits.html')\n\nVisualization saved to ../data/example_data/example_orbits.html\n\n\nSee the dynamic orbit visualziation here\n\n\n4. Statistics\nThis module analyzes the orbital data using descriptive statistics.\n\nfrom orbit_generation.stats import plot_histograms_position\n\nplot_histograms_position(orbit_data)",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "index.html#experiments",
    "href": "index.html#experiments",
    "title": "orbit-generation",
    "section": "Experiments",
    "text": "Experiments\n\nGenerative Discovery\nExperiments conducted in Generative Discovery folder have been presented in the following papers: - SPAICE 2024: Generative Design of Periodic Orbits in the Restricted Three-Body Problem\n\n1. Exploratory Data Analysis of Earth-Moon Periodic Orbits\nThis notebook explores the dataset by visualizing the orbits, the proportions within families, the initial conditions, and the distribution of features.\n\n\n2.",
    "crumbs": [
      "orbit-generation"
    ]
  },
  {
    "objectID": "convergence.html",
    "href": "convergence.html",
    "title": "Convergence",
    "section": "",
    "text": "source\n\n\n\n differential_correction (orbit:numpy.ndarray, μ:float,\n                          variable_time:bool=True, time_flight:float=None,\n                          jacobi_constant:float=None,\n                          X_end:numpy.ndarray=None, tol:float=1e-09,\n                          max_iter:int=20, printout:bool=False,\n                          DX_0:numpy.ndarray=None,\n                          X_big_0:numpy.ndarray=None, δ:float=None)\n\n*Wrapper for the Julia differential_correction function.\nParameters: orbit (np.ndarray): Orbit data with shape [num_timesteps, 7]. μ (float): Gravitational parameter. variable_time (bool): Whether to use variable time nodes. time_flight (float, optional): Total time of flight. jacobi_constant (float, optional): Jacobi constant. X_end (np.ndarray, optional): Terminal state vector (shape: [6]). tol (float): Tolerance for convergence. max_iter (int): Maximum number of iterations. printout (bool): Whether to print iteration logs. DX_0 (np.ndarray, optional): Initial guess for state vector correction. X_big_0 (np.ndarray, optional): Auxiliary initial guess. δ (float, optional): Step size or perturbation parameter.\nReturns: tuple: (X_corrected, t_corrected, norm_F_or_G, iterations, success)*\n\nsource\n\n\n\n\n create_converged_orbits_df (converged_indices, orbit_array,\n                             converged_orbits, errors, iterations)\n\n*Creates a DataFrame containing detailed information about converged orbits.\nParameters: converged_indices (list): List of orbit indices that have converged. orbit_array (np.ndarray): Original array containing all orbit data. converged_orbits (np.ndarray): Array containing corrected converged orbits. errors (np.ndarray): Array of norm values for each converged orbit. iterations (np.ndarray): Array of iteration counts for each converged orbit.\nReturns: pd.DataFrame: DataFrame with detailed information about each converged orbit.*\n\nsource\n\n\n\n\n process_diferential_correction_orbits (orbit_array:numpy.ndarray,\n                                        μ:float, variable_time:bool=True,\n                                        tol:float=1e-09, max_iter:int=20,\n                                        printout:bool=False)\n\n*Processes a set of orbits by providing the orbit array directly to differential correction.\nParameters: orbit_array (np.ndarray): Array containing orbit data with shape [num_orbits, num_timesteps, 7]. The first element in the last dimension is assumed to be time. μ (float): Gravitational parameter. variable_time (bool, optional): Whether to use variable time nodes for correction. Default is True. tol (float, optional): Tolerance for convergence in differential correction. Default is 1e-9. max_iter (int, optional): Maximum number of iterations for differential correction. Default is 20. printout (bool, optional): Whether to print iteration logs. Default is False.\nReturns: tuple: (converged_orbits_array, converged_orbits_df) - converged_orbits_array: NumPy array with shape [num_converged_orbits, num_timesteps, 7] containing corrected orbits. - converged_orbits_df: pandas DataFrame with detailed information about each converged orbit.*",
    "crumbs": [
      "Convergence"
    ]
  },
  {
    "objectID": "convergence.html#julia-wrapper",
    "href": "convergence.html#julia-wrapper",
    "title": "Convergence",
    "section": "",
    "text": "source\n\n\n\n differential_correction (orbit:numpy.ndarray, μ:float,\n                          variable_time:bool=True, time_flight:float=None,\n                          jacobi_constant:float=None,\n                          X_end:numpy.ndarray=None, tol:float=1e-09,\n                          max_iter:int=20, printout:bool=False,\n                          DX_0:numpy.ndarray=None,\n                          X_big_0:numpy.ndarray=None, δ:float=None)\n\n*Wrapper for the Julia differential_correction function.\nParameters: orbit (np.ndarray): Orbit data with shape [num_timesteps, 7]. μ (float): Gravitational parameter. variable_time (bool): Whether to use variable time nodes. time_flight (float, optional): Total time of flight. jacobi_constant (float, optional): Jacobi constant. X_end (np.ndarray, optional): Terminal state vector (shape: [6]). tol (float): Tolerance for convergence. max_iter (int): Maximum number of iterations. printout (bool): Whether to print iteration logs. DX_0 (np.ndarray, optional): Initial guess for state vector correction. X_big_0 (np.ndarray, optional): Auxiliary initial guess. δ (float, optional): Step size or perturbation parameter.\nReturns: tuple: (X_corrected, t_corrected, norm_F_or_G, iterations, success)*\n\nsource\n\n\n\n\n create_converged_orbits_df (converged_indices, orbit_array,\n                             converged_orbits, errors, iterations)\n\n*Creates a DataFrame containing detailed information about converged orbits.\nParameters: converged_indices (list): List of orbit indices that have converged. orbit_array (np.ndarray): Original array containing all orbit data. converged_orbits (np.ndarray): Array containing corrected converged orbits. errors (np.ndarray): Array of norm values for each converged orbit. iterations (np.ndarray): Array of iteration counts for each converged orbit.\nReturns: pd.DataFrame: DataFrame with detailed information about each converged orbit.*\n\nsource\n\n\n\n\n process_diferential_correction_orbits (orbit_array:numpy.ndarray,\n                                        μ:float, variable_time:bool=True,\n                                        tol:float=1e-09, max_iter:int=20,\n                                        printout:bool=False)\n\n*Processes a set of orbits by providing the orbit array directly to differential correction.\nParameters: orbit_array (np.ndarray): Array containing orbit data with shape [num_orbits, num_timesteps, 7]. The first element in the last dimension is assumed to be time. μ (float): Gravitational parameter. variable_time (bool, optional): Whether to use variable time nodes for correction. Default is True. tol (float, optional): Tolerance for convergence in differential correction. Default is 1e-9. max_iter (int, optional): Maximum number of iterations for differential correction. Default is 20. printout (bool, optional): Whether to print iteration logs. Default is False.\nReturns: tuple: (converged_orbits_array, converged_orbits_df) - converged_orbits_array: NumPy array with shape [num_converged_orbits, num_timesteps, 7] containing corrected orbits. - converged_orbits_df: pandas DataFrame with detailed information about each converged orbit.*",
    "crumbs": [
      "Convergence"
    ]
  },
  {
    "objectID": "convergence.html#python-not-working",
    "href": "convergence.html#python-not-working",
    "title": "Convergence",
    "section": "Python (not working)",
    "text": "Python (not working)\n\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom typing import List, Tuple, Dict, Optional\nfrom orbit_generation.propagation import jacobi_constant, prop_node\nfrom orbit_generation.constants import EM_MU\n\n\nclass MultipleShooting:\n    def __init__(self, mu: float, max_iterations: int, period: Optional[float] = None, energy: Optional[float] = None):\n        self.mu = mu\n        self.max_iterations = max_iterations\n        self.period = period\n        self.energy = energy\n\n    def adjust_orbit(self, X: np.ndarray) -&gt; Tuple[np.ndarray, List[int]]:\n        if X.ndim == 2:\n            X = X.reshape(1, *X.shape)\n        elif X.ndim != 3:\n            raise ValueError(\"Input X must be 2D or 3D array\")\n\n        N, F, T = X.shape\n        modified_orbits = np.copy(X)\n        \n        if self.period is None:\n            self.period = T - 1  # Assume unit time steps if period is not provided\n\n        converged_indices = []\n\n        for orbit_index in range(N):\n            for _ in range(self.max_iterations):\n                modified_orbits[orbit_index] = self.propagate_orbit(modified_orbits[orbit_index])\n                \n                if self.energy is not None:\n                    modified_orbits[orbit_index] = self.adjust_energy(modified_orbits[orbit_index])\n                \n                errors = self.calculate_errors(modified_orbits[orbit_index])\n                \n                if self.check_convergence(errors):\n                    converged_indices.append(orbit_index)\n                    break\n\n        print(f\"{len(converged_indices)} out of {N} orbits converged.\")\n        return modified_orbits, converged_indices\n\n    def propagate_orbit(self, orbit: np.ndarray) -&gt; np.ndarray:\n        dt = self.period / (orbit.shape[1] - 1)\n        propagated_orbit = np.zeros_like(orbit)\n        propagated_orbit[:, 0] = orbit[:, 0]  # Keep the first time step\n        for i in range(1, orbit.shape[1]):\n            state = orbit[1:, i-1]\n            propagated_state = prop_node(state, dt, self.mu)\n            propagated_orbit[:, i] = np.concatenate(([orbit[0, i-1] + dt], propagated_state))\n        propagated_orbit[:, -1] = propagated_orbit[:, 0]  # Enforce periodicity\n        return propagated_orbit\n\n    def adjust_energy(self, orbit: np.ndarray) -&gt; np.ndarray:\n        current_energy = jacobi_constant(orbit[1:, 0], self.mu)[1]\n        scaling_factor = np.sqrt(self.energy / current_energy)\n        orbit[4:, :] *= scaling_factor\n        return orbit\n\n    def calculate_errors(self, orbit: np.ndarray) -&gt; Dict[str, float]:\n        errors = {}\n        errors['position_error'] = np.linalg.norm(orbit[1:4, -1] - orbit[1:4, 0])\n        errors['velocity_error'] = np.linalg.norm(orbit[4:, -1] - orbit[4:, 0])\n        if self.energy is not None:\n            errors['energy_error'] = np.abs(jacobi_constant(orbit[1:, 0], self.mu)[1] - self.energy)\n        return errors\n\n    def check_convergence(self, errors: Dict[str, float]) -&gt; bool:\n        position_threshold = 1e-6\n        velocity_threshold = 1e-6\n        energy_threshold = 1e-6\n        \n        converged = (errors['position_error'] &lt; position_threshold and\n                     errors['velocity_error'] &lt; velocity_threshold)\n        \n        if self.energy is not None:\n            converged = converged and (errors['energy_error'] &lt; energy_threshold)\n        \n        return converged\n\n\nclass MultipleShooting:\n    def __init__(self, mu: float, max_iterations: int, period: Optional[float] = None, energy: Optional[float] = None):\n        self.mu = mu\n        self.max_iterations = max_iterations\n        self.period = period\n        self.energy = energy\n        self.tolerance = 1e-9\n\n    def adjust_orbit(self, X: np.ndarray) -&gt; Tuple[np.ndarray, List[int]]:\n        N, F, T = X.shape\n        t_vec = np.linspace(0, self.period or T-1, T)\n        \n        X_corrected = np.zeros_like(X)\n        converged_indices = []\n        \n        for i in range(N):\n            X_corrected[i], t_vec_corrected, error, iterations, success = self.differential_correction(\n                X[i], t_vec, variable_time=self.period is not None,\n                time_flight=self.period, jacobi_constant=self.energy\n            )\n            if success == 1:\n                converged_indices.append(i)\n        \n        return X_corrected, converged_indices\n\n    def differential_correction(self, X_old, t_vec_old, variable_time=True, time_flight=None,\n                                jacobi_constant=None, X_end=None, printout=False):\n        k = 0\n        n = len(t_vec_old)\n\n        if not variable_time and time_flight is not None:\n            raise ValueError(\"Set variable_time to True to specify the time of flight.\")\n\n        while k &lt; self.max_iterations:\n            k += 1\n            X_big, F, DF = self.constraints(X_old, t_vec_old, X_end, time_flight, jacobi_constant, variable_time)\n\n            if np.linalg.norm(F) &lt;= self.tolerance and t_vec_old[-1] &gt; 1e-6:\n                if printout:\n                    print(f\"Converged in {k} iterations\")\n                return X_old, t_vec_old, np.linalg.norm(F), k, 1\n\n            if np.linalg.norm(F) &gt;= 10 and k &gt; 1:\n                if printout:\n                    print(f\"Solution diverged after {k} iterations\")\n                return X_old, t_vec_old, np.linalg.norm(F), k, -1\n\n            X_big_new = X_big - np.linalg.pinv(DF) @ F\n\n            X_new = X_big_new[:7*n].reshape(n, 7)\n\n            if variable_time:\n                t_vec_new = np.zeros(n)\n                t_vec_new[1:] = np.cumsum(X_big_new[7*n:])\n                t_vec_old = t_vec_new.copy()\n\n            X_old = X_new.copy()\n\n            if printout:\n                print(f\"{k} | {np.linalg.norm(F)}\")\n\n        return X_old, t_vec_old, np.linalg.norm(F), self.max_iterations, -1\n\n    def constraints(self, X, t_vec, X_end=None, time_flight=None, jacobi_constant=None, variable_time=True):\n        n = len(t_vec)\n        T_vec = np.diff(t_vec)\n\n        if time_flight is None and jacobi_constant is None:\n            dim_F = n * 7\n        elif time_flight is not None:\n            dim_F = n * 7 + 1\n        elif jacobi_constant is not None:\n            dim_F = n * 7 + 1\n\n        X_big = np.concatenate([X.flatten(), T_vec] if variable_time else [X.flatten()])\n        F = np.zeros(dim_F)\n        DF = np.zeros((dim_F, len(X_big)))\n\n        for i in range(n - 1):\n            ind_x = slice(i * 7, (i + 1) * 7)\n            ind_y = slice(i * 7, (i + 2) * 7)\n            Xf_i, Phi = self.get_state(X[i], T_vec[i])\n            F[ind_x] = Xf_i - X[i + 1]\n            DF[ind_x, ind_y] = np.hstack((Phi, -np.eye(7)))\n            if variable_time:\n                DF[ind_x, 7 * n + i] = np.concatenate(([1], self.dynamics_cr3bp(X[i, 1:])))\n\n        if X_end is None:\n            F[-7:] = X[-1] - X[0]\n            DF[-7:, -7:] = np.eye(7)\n            DF[-7:, :7] = -np.eye(7)\n        else:\n            F[-7:] = X[-1] - X_end\n            DF[-7:, -7:] = np.eye(7)\n\n        if time_flight is not None:\n            F[-1] = np.sum(T_vec) - time_flight\n            DF[-1, -n+1:] = np.ones(n - 1)\n        elif jacobi_constant is not None:\n            J = 0\n            for i in range(n):\n                _, J_i, DJ_i = self.jacobi(X[i, 1:])\n                J += J_i\n                DF[-1, i*7+1:(i+1)*7] = DJ_i\n            F[-1] = J - n * jacobi_constant\n\n        return X_big, F, DF\n\n    def get_state(self, X0, dt):\n        t = X0[0]\n        state = X0[1:]\n        sol = solve_ivp(\n            lambda t, y: np.concatenate(([1], self.dynamics_cr3bp(y))),\n            [t, t + dt],\n            state,\n            dense_output=True,\n            rtol=1e-12,\n            atol=1e-12,\n            method='Radau'\n        )\n        Xf = np.concatenate(([t + dt], sol.y[:, -1]))\n        Phi = self.compute_stm(state, dt)\n        return Xf, Phi\n\n    def dynamics_cr3bp(self, X):\n        x, y, z, v_x, v_y, v_z = X\n        r1 = np.sqrt((x + self.mu)**2 + y**2 + z**2)\n        r2 = np.sqrt((x - (1 - self.mu))**2 + y**2 + z**2)\n        x_dot  = v_x\n        y_dot  = v_y\n        z_dot  = v_z\n        x_ddot = x + 2 * v_y - (1 - self.mu) * (x + self.mu) / r1**3 - self.mu * (x - (1 - self.mu)) / r2**3\n        y_ddot = y - 2 * v_x - y * ((1 - self.mu) / r1**3 + self.mu / r2**3)\n        z_ddot = -z * ((1 - self.mu) / r1**3 + self.mu / r2**3)\n        return np.array([x_dot, y_dot, z_dot, x_ddot, y_ddot, z_ddot])\n\n    def jacobi(self, X):\n        x, y, z, xp, yp, zp = X\n        mu1 = 1 - self.mu\n        mu2 = self.mu\n        r1 = np.sqrt((x + mu2)**2 + y**2 + z**2)\n        r2 = np.sqrt((x - mu1)**2 + y**2 + z**2)\n        K = 0.5 * (xp**2 + yp**2 + zp**2)\n        Ubar = -0.5 * (x**2 + y**2) - mu1 / r1 - mu2 / r2 - 0.5 * mu1 * mu2\n        E = K + Ubar\n        J = -2 * E\n        DJ = np.zeros(6)  # Compute the gradient of J here\n        return 0, J, DJ\n\n    def compute_stm(self, X0, dt):\n        def variational_equations(t, y):\n            state = y[:6]\n            phi = y[6:].reshape(6, 6)\n            dxdt = self.dynamics_cr3bp(state)\n            A = self.compute_jacobian(state)\n            dphi_dt = A @ phi\n            return np.concatenate([dxdt, dphi_dt.flatten()])\n\n        y0 = np.concatenate([X0, np.eye(6).flatten()])\n        sol = solve_ivp(variational_equations, [0, dt], y0, \n                        rtol=1e-12, atol=1e-12, method='Radau')\n        return np.vstack([np.hstack([np.eye(1), np.zeros((1, 6))]),\n                          np.hstack([np.zeros((6, 1)), sol.y[6:, -1].reshape(6, 6)])])\n\n    def compute_jacobian(self, X):\n        x, y, z, _, _, _ = X\n        mu1 = 1 - self.mu\n        mu2 = self.mu\n        r1 = np.sqrt((x + mu2)**2 + y**2 + z**2)\n        r2 = np.sqrt((x - mu1)**2 + y**2 + z**2)\n\n        Uxx = 1 - mu1/r1**3 - mu2/r2**3 + 3*mu1*(x+mu2)**2/r1**5 + 3*mu2*(x-mu1)**2/r2**5\n        Uyy = 1 - mu1/r1**3 - mu2/r2**3 + 3*mu1*y**2/r1**5 + 3*mu2*y**2/r2**5\n        Uzz = -mu1/r1**3 - mu2/r2**3 + 3*mu1*z**2/r1**5 + 3*mu2*z**2/r2**5\n        Uxy = 3*mu1*(x+mu2)*y/r1**5 + 3*mu2*(x-mu1)*y/r2**5\n        Uxz = 3*mu1*(x+mu2)*z/r1**5 + 3*mu2*(x-mu1)*z/r2**5\n        Uyz = 3*mu1*y*z/r1**5 + 3*mu2*y*z/r2**5\n\n        return np.array([\n            [0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 1],\n            [Uxx, Uxy, Uxz, 0, 2, 0],\n            [Uxy, Uyy, Uyz, -2, 0, 0],\n            [Uxz, Uyz, Uzz, 0, 0, 0]\n        ])\n\n\n# Example usage with all constraints\nmu = EM_MU\nperiod = 2 * np.pi\nenergy = -1.5\nmax_iterations = 20\n\nms_full = MultipleShooting(mu, max_iterations, period, energy)\n# adjusted_orbits_full = ms_full.adjust_orbit(generation)",
    "crumbs": [
      "Convergence"
    ]
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "Auxiliar Functions\nsource",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#evaluate-clustering-with-multiple-labels",
    "href": "evaluation.html#evaluate-clustering-with-multiple-labels",
    "title": "Evaluation",
    "section": "Evaluate Clustering with Multiple Labels",
    "text": "Evaluate Clustering with Multiple Labels\n\nsource\n\nevaluate_clustering_multiple_labels\n\n evaluate_clustering_multiple_labels\n                                      (latent_representations:numpy.ndarra\n                                      y, list_of_labels:list,\n                                      clustering_method:str='kmeans',\n                                      label_names:list=None, **kwargs)\n\nEvaluates the clustering quality of the latent representations for one or multiple sets of labels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\nThe latent space data.\n\n\nlist_of_labels\nlist\n\nList of true labels or a single true labels array.\n\n\nclustering_method\nstr\nkmeans\nThe clustering algorithm to use (‘kmeans’, ‘gmm’, ‘dbscan’).\n\n\nlabel_names\nlist\nNone\nOptional names for the label sets.\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\nReturns\ndict\n\nReturns a dictionary with clustering metrics.",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#physical-distances",
    "href": "evaluation.html#physical-distances",
    "title": "Evaluation",
    "section": "Physical Distances",
    "text": "Physical Distances\n\nEuclidean\n\nsource\n\n\neuclidean_distance\n\n euclidean_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nManhattan\n\nsource\n\n\nmanhattan_distance\n\n manhattan_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nCosine\n\nsource\n\n\ncosine_distance\n\n cosine_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nDynamic Time Warping\n\nsource\n\n\ndtw_distance\n\n dtw_distance (point1:numpy.ndarray, point2:numpy.ndarray)\n\n\n\nGeneric\n\nsource\n\n\ncalculate_distance\n\n calculate_distance (point1:numpy.ndarray, point2:numpy.ndarray,\n                     distance_metric:str='euclidean')\n\n*Calculates the distance between two points based on the specified distance metric.\n:param point1: First data point array. :param point2: Second data point array. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: Distance as a float.*\n\nsource\n\n\ncalculate_pairwise_distances\n\n calculate_pairwise_distances (array1:numpy.ndarray, array2:numpy.ndarray,\n                               distance_metric:str='euclidean')\n\n*Calculates the distance between corresponding pairs of points from two arrays using the specified distance metric.\n:param array1: A 2D numpy array where each row represents a data point. :param array2: A 2D numpy array where each row represents a data point. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: A 1D numpy array containing the distances between corresponding pairs.*\n\n\nBatch\n\nsource\n\n\ncalculate_distances_batch\n\n calculate_distances_batch (single_points:numpy.ndarray,\n                            points_array:numpy.ndarray,\n                            distance_metric:str='euclidean')\n\n*Calculates the distances between single data points and an array of data points based on the specified distance metric.\n:param single_points: Single data point array or a batch of data points. :param points_array: Array of data points to compare against. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: Array of distances.*\n\n\nNearest Points\n\nsource\n\n\nfind_nearest_points\n\n find_nearest_points (single_point:numpy.ndarray,\n                      points_array:numpy.ndarray, n:int,\n                      distance_metric:str='euclidean')\n\n\nsource\n\n\nfind_nearest_points_batch\n\n find_nearest_points_batch (single_points:numpy.ndarray,\n                            points_array:numpy.ndarray, n:int,\n                            distance_metric:str='euclidean')\n\n*Finds the nearest indices and distances for a batch of single data points to an array of data points based on the specified distance metric.\n:param single_points: Array of single data points (2D array). :param points_array: Array of data points to compare against (2D array). :param n: Number of nearest points to retrieve for each single point. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). :return: Tuple of nearest indices and nearest distances for each single point.*",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#orbits-distance",
    "href": "evaluation.html#orbits-distance",
    "title": "Evaluation",
    "section": "Orbits Distance",
    "text": "Orbits Distance\n\nsource\n\norbits_distances\n\n orbits_distances (orbit_data1:numpy.ndarray, orbit_data2:numpy.ndarray,\n                   distance_metric:str)\n\n*Calculates distances between orbits in two datasets using a specified distance metric.\nThis function is robust to input shapes. If an input is a 2D array (representing a single orbit), it is automatically converted to a 3D array with one sample. This allows for flexible comparisons between single or multiple orbits.\n:param orbit_data1: First set of orbits (shape: [n_samples1, n_features, n_time_steps] or [n_features, n_time_steps]). :param orbit_data2: Second set of orbits or a single orbit. Shape: [n_samples2, n_features, n_time_steps] or [n_features, n_time_steps]. :param distance_metric: A string representing the distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’).\n\nreturn: NumPy array of distances. - If one input is single and the other is multiple: - Shape: [n_samples1] or [n_samples2] - If both inputs are multiple: - Shape: [n_samples1, n_samples2]*\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbit_data1\nndarray\nShape: [n_samples1, n_features, n_time_steps] or [n_features, n_time_steps]\n\n\norbit_data2\nndarray\nShape: [n_samples2, n_features, n_time_steps] or [n_features, n_time_steps]\n\n\ndistance_metric\nstr\nString representing the distance metric (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’)\n\n\nReturns\nndarray\n\n\n\n\n\n\nGet the Closest Orbits\n\nsource\n\n\nfind_nearest_orbits\n\n find_nearest_orbits (single_orbit:numpy.ndarray,\n                      orbit_data:numpy.ndarray, n:int,\n                      distance_metric:str='euclidean')\n\n*Finds the n closest orbits in orbit_data to the single_orbit based on the specified distance metric.\n:param single_orbit: The reference orbit (shape: [n_features, n_time_steps]). :param orbit_data: The dataset of orbits (shape: [n_samples, n_features, n_time_steps]). :param n: The number of closest orbits to return. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). Defaults to ‘euclidean’. :return: A tuple containing: - Indices of the n closest orbits in orbit_data. - Distances of the n closest orbits.*\n\nsource\n\n\nfind_nearest_orbits_batch\n\n find_nearest_orbits_batch (single_orbits:numpy.ndarray,\n                            orbit_data:numpy.ndarray, n:int,\n                            distance_metric:str='euclidean')\n\n*Iteratively finds the n closest orbits in orbit_data for each orbit in single_orbits.\n\nparam single_orbits: The reference orbits (shape: [num_single_orbits, n_features, n_time_steps]). :param orbit_data: The dataset of orbits to search within (shape: [n_samples, n_features, n_time_steps]). :param n: The number of closest orbits to return for each single_orbit. :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). Defaults to ‘euclidean’. :return: A tuple containing: - A 2D array of shape [num_single_orbits, n] with indices of the n closest orbits. - A 2D array of shape [num_single_orbits, n] with distances of the n closest orbits.*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsingle_orbits\nndarray\n\nShape: [num_single_orbits, n_features, n_time_steps]\n\n\norbit_data\nndarray\n\nShape: [n_samples, n_features, n_time_steps]\n\n\nn\nint\n\nNumber of nearest orbits to find\n\n\ndistance_metric\nstr\neuclidean\nDistance metric\n\n\nReturns\ntuple\n\n\n\n\n\n\n\nCalculate Pairwise distances\n\nsource\n\n\ncalculate_pairwise_orbit_distances\n\n calculate_pairwise_orbit_distances (orbit_data1:numpy.ndarray,\n                                     orbit_data2:numpy.ndarray,\n                                     distance_metric:str='euclidean')\n\n*Calculates the distance between corresponding orbits in two orbit datasets.\n\nparam orbit_data1: The first set of orbits (shape: [n_samples, n_features, n_time_steps]). :param orbit_data2: The second set of orbits (shape: [n_samples, n_features, n_time_steps]). :param distance_metric: The distance metric to use (‘euclidean’, ‘manhattan’, ‘cosine’, ‘dtw’). Defaults to ‘euclidean’. :return: An array of distances with shape [n_samples].*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data1\nndarray\n\nShape: [n_samples, n_features, n_time_steps]\n\n\norbit_data2\nndarray\n\nShape: [n_samples, n_features, n_time_steps]\n\n\ndistance_metric\nstr\neuclidean\nDistance metric\n\n\nReturns\nndarray",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#evaluate-distance-metrics",
    "href": "evaluation.html#evaluate-distance-metrics",
    "title": "Evaluation",
    "section": "Evaluate Distance Metrics",
    "text": "Evaluate Distance Metrics\n\nsource\n\nevaluate_distance_metrics_and_clustering\n\n evaluate_distance_metrics_and_clustering (orbit_data:numpy.ndarray,\n                                           true_labels:numpy.ndarray,\n                                           distance_metrics:list=None, clu\n                                           stering_algorithms:list=None,\n                                           evaluation_metrics:list=None,\n                                           n_clusters:int=None,\n                                           plot_results:bool=True)\n\n*Evaluates specified distance metrics and clustering algorithms on the given orbit data.\n:param orbit_data: The orbit data as either: - multivariate time series (shape: [n_samples, n_features, n_time_steps]) - point data (shape: [n_samples, n_features]) :param true_labels: Array of true labels for the orbit data. :param distance_metrics: List of strings specifying distance metrics to use. If None, all available metrics are used. :param clustering_algorithms: List of strings specifying clustering algorithms to use. If None, all available algorithms are used. :param evaluation_metrics: List of strings specifying evaluation metrics to use. If None, all available metrics are used. :param n_clusters: Number of clusters for algorithms that require it. If None, it will be inferred from labels. :param plot_results: If True, plot heatmaps of the results. :return: A dictionary containing results for all combinations of metrics and clustering algorithms.*",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "evaluation.html#machine-learning",
    "href": "evaluation.html#machine-learning",
    "title": "Evaluation",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nsource\n\nmachine_learning_evaluation\n\n machine_learning_evaluation (X, y, print_results=False,\n                              return_best_model=False, scale_data=True)\n\n*Evaluates multiple machine learning algorithms on the provided dataset.\nParameters: - X: Features, expected to be a 2D array. If higher dimensions, the function attempts to reshape. - y: Target labels. - print_results: If True, visualizes the evaluation results. - return_best_model: If True, returns the best model based on accuracy. - scale_data: If True, scales the features using StandardScaler.\nReturns: - results: Dictionary containing accuracy and classification report for each algorithm. - best_model: The fitted model with the highest accuracy if return_best_model is True.*",
    "crumbs": [
      "Evaluation"
    ]
  },
  {
    "objectID": "orbit_statistics.html",
    "href": "orbit_statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "from orbit_generation.data import get_example_orbit_data\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n(200, 6, 300)",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_statistics.html#simple-statistics",
    "href": "orbit_statistics.html#simple-statistics",
    "title": "Statistics",
    "section": "Simple statistics",
    "text": "Simple statistics\n\nsource\n\ncalculate_overall_spatial_statistics\n\n calculate_overall_spatial_statistics (orbits:numpy.ndarray)\n\n*Calculate the overall min, mean, max, and percentile statistics for each scalar (position and velocity in X, Y, Z) across all time instants and orbits.\nParameters: - orbits (np.ndarray): A numpy array of shape (number_of_orbits, 6 or 7, number_of_time_instants) containing orbit data.\nReturns: - np.ndarray: A NumPy array containing statistics for each scalar.*\n\norbits = np.array([\n    [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8]],  # Orbit 1\n    [[4, 4, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 9]]   # Orbit 2\n])\n\n# Call the function to calculate statistics\nstats = calculate_overall_spatial_statistics(orbits)\n\n# Define the expected values for each statistic\nexpected_stats = np.array([\n    [1, 3, 4, 2.25, 3.5, 4],    # posx\n    [2, 3.5, 5, 3, 3.5, 4],     # posy\n    [3, 4.5, 6, 4, 4.5, 5],     # posz\n    [4, 5.5, 7, 5, 5.5, 6],     # velx\n    [5, 6.5, 8, 6, 6.5, 7],     # vely\n    [6, 7.5, 9, 7, 7.5, 8]      # velz\n])\n\n# Test each statistic for each scalar\nfor i, scalar_name in enumerate(['posx', 'posy', 'posz', 'velx', 'vely', 'velz']):\n    test_eq(stats[i, 0], expected_stats[i, 0])\n    test_eq(stats[i, 1], expected_stats[i, 1])\n    test_eq(stats[i, 2], expected_stats[i, 2])\n    test_eq(stats[i, 3], expected_stats[i, 3])\n    test_eq(stats[i, 4], expected_stats[i, 4])\n    test_eq(stats[i, 5], expected_stats[i, 5])\n\n\nsource\n\n\ncalculate_per_orbit_spatial_statistics\n\n calculate_per_orbit_spatial_statistics (orbits:numpy.ndarray)\n\n*Calculate per-orbit min, mean, max, and percentile statistics for each scalar (position and velocity in X, Y, Z) across all time instants.\nParameters: - orbits (np.ndarray): A numpy array of shape (number_of_orbits, 6 or 7, number_of_time_instants) containing orbit data.\nReturns: - np.ndarray: A NumPy array of shape (number_of_orbits, number_of_scalars, number_of_stats) containing statistics for each scalar per orbit.*",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_statistics.html#plot-time",
    "href": "orbit_statistics.html#plot-time",
    "title": "Statistics",
    "section": "Plot Time",
    "text": "Plot Time\n\nsource\n\nplot_time_increments\n\n plot_time_increments (orbit_dataset:numpy.ndarray,\n                       orbits_to_plot:List[int]=None,\n                       show_legend:bool=True)\n\n*Plots the time as a function to visualize how it increments for each orbit.\nParameters: orbit_dataset (np.ndarray): A 3D numpy array where the first dimension is the number of orbits, the second dimension contains 7 scalars (time, posx, posy, posz, velx, vely, velz), and the third dimension is the time steps. orbits_to_plot (list[int], optional): List of integers referring to the orbits to plot. If None, plots all orbits. show_legend (bool, optional): Whether to display the legend. Default is True.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_dataset\nndarray\n\nThe 3D numpy array representing the orbits\n\n\norbits_to_plot\nList\nNone\nOptional list of integers referring to the orbits to plot\n\n\nshow_legend\nbool\nTrue\nBoolean to control the display of the legend\n\n\nReturns\nNone",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_statistics.html#plot-histograms",
    "href": "orbit_statistics.html#plot-histograms",
    "title": "Statistics",
    "section": "Plot Histograms",
    "text": "Plot Histograms\n\nsource\n\nplot_orbit_data_lengths\n\n plot_orbit_data_lengths (orbit_data, key_range=(1, 36072), dimension=0,\n                          bins=30, color='blue', plot=True,\n                          title='Histogram of Orbits Time Steps')\n\n\nsource\n\n\nplot_histograms_position\n\n plot_histograms_position (data:numpy.ndarray, save_path:str=None,\n                           last_time_elements:bool=True)\n\n*Plots histograms for the scalar values (position and velocity in X, Y, Z, and optionally time) across all orbits and time points. Handles arrays with 6 or 7 scalar dimensions, with the 7th being ‘time’.\nParameters: - data (np.ndarray): The orbit data array. - save_path (str, optional): If provided, the plot will be saved to this file path. - last_time_elements (bool): If True, plot only the last elements of the time vectors for the time histogram.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe orbit data array of shape (num_orbits, num_scalars, num_time_points).\n\n\nsave_path\nstr\nNone\nOptional path to save the plot image.\n\n\nlast_time_elements\nbool\nTrue\nWhether to plot only the last elements of the time vectors.\n\n\nReturns\nNone\n\n\n\n\n\n\nplot_histograms_position(orbit_data)\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_histograms_comparison\n\n plot_histograms_comparison (data1:numpy.ndarray, data2:numpy.ndarray,\n                             label1:str='Dataset 1', label2:str='Dataset\n                             2', save_path:str=None, normalize:bool=False)\n\nPlots histograms for scalar values (position, velocity in X, Y, Z, and optionally time) from two datasets on the same chart with different colors. Supports both 6 and 7 scalar dimensions, with the 7th being ‘time’. Optionally saves the plot to a specified file path and can normalize histograms for relative comparison.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata1\nndarray\n\nFirst orbit data array of shape (num_orbits, num_scalars, num_time_points).\n\n\ndata2\nndarray\n\nSecond orbit data array of shape (num_orbits, num_scalars, num_time_points).\n\n\nlabel1\nstr\nDataset 1\nLabel for the first dataset.\n\n\nlabel2\nstr\nDataset 2\nLabel for the second dataset.\n\n\nsave_path\nstr\nNone\nOptional path to save the plot image.\n\n\nnormalize\nbool\nFalse\nNormalize histograms to show relative frequencies.\n\n\nReturns\nNone\n\n\n\n\n\n\norbit_data1 = orbit_data[:100]\norbit_data2 = orbit_data[100:]\n\nplot_histograms_comparison(orbit_data1, orbit_data2)\n\n\n\n\n\n\n\n\n\norbit_data3 = orbit_data2[:5]\n\nplot_histograms_comparison(orbit_data1, orbit_data3, normalize=True)",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "orbit_processing.html",
    "href": "orbit_processing.html",
    "title": "Processing",
    "section": "",
    "text": "source\n\n\n\n\n downsample_3d_array (data:numpy.ndarray, axis:int, hop:int=None,\n                      target_size:int=None)\n\nDownsample a 3D numpy array along a specified axis by keeping only every hop-th element or to a target size.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe original 3D array to be downsampled.\n\n\naxis\nint\n\nThe axis along which to perform the downsampling.\n\n\nhop\nint\nNone\nThe interval at which to keep elements.\n\n\ntarget_size\nint\nNone\nThe target size for the specified axis.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n resample_3d_array (data:numpy.ndarray, axis:int, target_size:int)\n\nResample a 3D numpy array along a specified axis using linear interpolation.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be resampled.\n\n\naxis\nint\nThe axis along which to perform the interpolation.\n\n\ntarget_size\nint\nThe new size of the axis after resampling.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n average_downsample_3d_array (data:numpy.ndarray, axis:int,\n                              target_size:int)\n\nDownsample a 3D numpy array along a specified axis using averaging.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be downsampled.\n\n\naxis\nint\nThe axis along which to perform the downsampling (0, 1, or 2).\n\n\ntarget_size\nint\nThe desired size of the specified axis after downsampling.\n\n\nReturns\nndarray",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#resampling",
    "href": "orbit_processing.html#resampling",
    "title": "Processing",
    "section": "",
    "text": "source\n\n\n\n\n downsample_3d_array (data:numpy.ndarray, axis:int, hop:int=None,\n                      target_size:int=None)\n\nDownsample a 3D numpy array along a specified axis by keeping only every hop-th element or to a target size.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe original 3D array to be downsampled.\n\n\naxis\nint\n\nThe axis along which to perform the downsampling.\n\n\nhop\nint\nNone\nThe interval at which to keep elements.\n\n\ntarget_size\nint\nNone\nThe target size for the specified axis.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n resample_3d_array (data:numpy.ndarray, axis:int, target_size:int)\n\nResample a 3D numpy array along a specified axis using linear interpolation.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be resampled.\n\n\naxis\nint\nThe axis along which to perform the interpolation.\n\n\ntarget_size\nint\nThe new size of the axis after resampling.\n\n\nReturns\nndarray\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n average_downsample_3d_array (data:numpy.ndarray, axis:int,\n                              target_size:int)\n\nDownsample a 3D numpy array along a specified axis using averaging.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nndarray\nThe original 3D array to be downsampled.\n\n\naxis\nint\nThe axis along which to perform the downsampling (0, 1, or 2).\n\n\ntarget_size\nint\nThe desired size of the specified axis after downsampling.\n\n\nReturns\nndarray",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#reorder-orbit-with-time",
    "href": "orbit_processing.html#reorder-orbit-with-time",
    "title": "Processing",
    "section": "Reorder Orbit with Time",
    "text": "Reorder Orbit with Time\n\nsource\n\nreorder_orbits\n\n reorder_orbits (orbit_dataset:numpy.ndarray)\n\nReorders the time steps of each orbit in the dataset such that the time values are always incrementally increasing. Returns the reordered dataset, a 2D array of metric values for each orbit, and a list of metric labels.\n\n\n\n\nType\nDetails\n\n\n\n\norbit_dataset\nndarray\n\n\n\nReturns\nTuple\n3D numpy array of reordered orbits.",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#reshaping-arrays",
    "href": "orbit_processing.html#reshaping-arrays",
    "title": "Processing",
    "section": "Reshaping Arrays",
    "text": "Reshaping Arrays\n\nsource\n\npad_and_convert_to_3d\n\n pad_and_convert_to_3d (orbits:Dict[int,numpy.ndarray], timesteps:int)\n\nTruncate and pad each orbit to a uniform length and convert to a 3D numpy array.\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\ntimesteps\nint\nDesired number of timesteps.\n\n\nReturns\nndarray\n3D numpy array of padded orbits.\n\n\n\n\nsource\n\n\nsegment_and_convert_to_3d\n\n segment_and_convert_to_3d (orbits:Dict[int,numpy.ndarray],\n                            segment_length:int)\n\nDivide each orbit into segments of a given length and convert to a 3D numpy array.\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\nsegment_length\nint\nDesired length of each segment.\n\n\nReturns\nTuple\n3D numpy array of segments.\n\n\n\n\norbits = {\n    1: np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n                    [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n                    [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],\n                    [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],\n                    [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n                    [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]]),\n    2: np.array([[73, 74, 75, 76, 77, 78, 79],\n                    [81, 82, 83, 84, 85, 86, 87],\n                    [89, 90, 91, 92, 93, 94, 95],\n                    [97, 98, 99, 100, 101, 102, 103],\n                    [105, 106, 107, 108, 109, 110, 111],\n                    [113, 114, 115, 116, 117, 118, 119]])\n}\nsegment_length = 4\n\n# Expected segments and IDs\nexpected_segments = np.array([\n    [[1, 2, 3, 4], [13, 14, 15, 16], [25, 26, 27, 28], [37, 38, 39, 40], [49, 50, 51, 52], [61, 62, 63, 64]],\n    [[5, 6, 7, 8], [17, 18, 19, 20], [29, 30, 31, 32], [41, 42, 43, 44], [53, 54, 55, 56], [65, 66, 67, 68]],\n    [[9, 10, 11, 12], [21, 22, 23, 24], [33, 34, 35, 36], [45, 46, 47, 48], [57, 58, 59, 60], [69, 70, 71, 72]],\n    [[73, 74, 75, 76], [81, 82, 83, 84], [89, 90, 91, 92], [97, 98, 99, 100], [105, 106, 107, 108], [113, 114, 115, 116]]\n])\nexpected_ids = [1, 1, 1, 2]\n\n# Call the function\nsegments_3d, segment_ids = segment_and_convert_to_3d(orbits, segment_length)\n\n# Use test_eq to check the results\ntest_eq(segments_3d.tolist(), expected_segments.tolist())\ntest_eq(segment_ids, expected_ids)",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#add-time-vector",
    "href": "orbit_processing.html#add-time-vector",
    "title": "Processing",
    "section": "Add Time Vector",
    "text": "Add Time Vector\n\nsource\n\nadd_time_vector_to_orbits\n\n add_time_vector_to_orbits (orbits:Dict[int,numpy.ndarray],\n                            propagated_periods:List[float],\n                            periods:List[float])\n\nAdd a time vector to each orbit in the dictionary.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\norbits\nDict\nDictionary of orbits with numerical keys.\n\n\npropagated_periods\nList\nList of propagated periods for each orbit.\n\n\nperiods\nList\nList of periods for each orbit.\n\n\nReturns\nDict\nDictionary of updated orbits with time vectors added.",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "orbit_processing.html#interpolating-equal-times",
    "href": "orbit_processing.html#interpolating-equal-times",
    "title": "Processing",
    "section": "Interpolating Equal Times",
    "text": "Interpolating Equal Times\n\nsource\n\ninterpolate_equal_times\n\n interpolate_equal_times (orbit_dataset:numpy.ndarray)\n\n\n# Testing with dummy data that has equal time values at the beginning\ntest_data = np.array([\n    [0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n])\n\n# Reshape to 3D array (1 orbit with 3 scalars and 10 timesteps)\ntest_data = test_data.reshape(1, 3, 10)\n\noutput = interpolate_equal_times(test_data)\nprint(output[:,0])\n\n[[0.    0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.    2.   ]]",
    "crumbs": [
      "Processing"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "source\n\n\n\n plot_3d_points (data, labels=None, plot_velocity=True, arrow_width=0.005,\n                 show_legend=True, figsize=(10, 8))\n\n*Plots each point in space with a 3D arrow based on the first 3 coordinates (position) and optionally the next 3 coordinates (velocity).\nParameters: data (numpy.ndarray): Array of shape (samples, 3) for positions or (samples, 6) for positions and velocities. - data[:, 0:3] represents the 3D positions (x, y, z) - data[:, 3:6] represents the velocity components (vx, vy, vz) if provided labels (list of str): Optional list of labels for color coding the points. plot_velocity (bool): If True and velocities are provided, plot arrows representing velocity vectors. arrow_width (float): Width of the arrows. show_legend (bool): If True, show the legend for color coding. figsize (tuple): Size of the figure in inches (width, height). Default is (10, 8).*",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#points",
    "href": "visualization.html#points",
    "title": "Visualization",
    "section": "",
    "text": "source\n\n\n\n plot_3d_points (data, labels=None, plot_velocity=True, arrow_width=0.005,\n                 show_legend=True, figsize=(10, 8))\n\n*Plots each point in space with a 3D arrow based on the first 3 coordinates (position) and optionally the next 3 coordinates (velocity).\nParameters: data (numpy.ndarray): Array of shape (samples, 3) for positions or (samples, 6) for positions and velocities. - data[:, 0:3] represents the 3D positions (x, y, z) - data[:, 3:6] represents the velocity components (vx, vy, vz) if provided labels (list of str): Optional list of labels for color coding the points. plot_velocity (bool): If True and velocities are provided, plot arrows representing velocity vectors. arrow_width (float): Width of the arrows. show_legend (bool): If True, show the legend for color coding. figsize (tuple): Size of the figure in inches (width, height). Default is (10, 8).*",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#orbits",
    "href": "visualization.html#orbits",
    "title": "Visualization",
    "section": "Orbits",
    "text": "Orbits\n\nStatic\n\nsource\n\n\nvisualize_static_orbits\n\n visualize_static_orbits (data:numpy.ndarray,\n                          time_instants:Optional[List[int]]=None,\n                          orbit_indices:Optional[List[int]]=None,\n                          point_dict:Optional[Dict[str,tuple]]=None,\n                          show_legend:bool=True,\n                          save_path:Optional[str]=None,\n                          plot_reference_box:bool=True,\n                          title:Optional[str]=None,\n                          orbit_names:Optional[List[str]]=None,\n                          equal_aspect:bool=False)\n\n*Visualizes orbits in 3D space and highlights specified time instants for each selected orbit.\nArgs: data (np.ndarray): The orbit data with shape (num_orbits, 6, num_time_points). time_instants (Optional[List[int]]): Time points to highlight; defaults to None. orbit_indices (Optional[List[int]]): Indices of orbits to visualize; defaults to all. point_dict (Optional[Dict[str, tuple]]): Dictionary of extra points to plot. show_legend (bool): Flag to indicate whether to show a legend. save_path (Optional[str]): Path to save the figure; defaults to None. plot_reference_box (bool): Flag to indicate whether to plot the reference box. title (Optional[str]): Custom title for the plot. orbit_names (Optional[List[str]]): Custom names for orbits; defaults to “Orbit {index}”. equal_aspect (bool): Flag to enforce equal scaling for all axes.\nReturns: None*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe orbit data with shape (num_orbits, 6, num_time_points).\n\n\ntime_instants\nOptional\nNone\nTime points to highlight; defaults to None.\n\n\norbit_indices\nOptional\nNone\nIndices of orbits to visualize; defaults to all.\n\n\npoint_dict\nOptional\nNone\nDictionary of extra points to plot.\n\n\nshow_legend\nbool\nTrue\nFlag to indicate whether to show a legend.\n\n\nsave_path\nOptional\nNone\nPath to save the figure; defaults to None.\n\n\nplot_reference_box\nbool\nTrue\nFlag to indicate whether to plot the reference box.\n\n\ntitle\nOptional\nNone\nCustom title for the plot.\n\n\norbit_names\nOptional\nNone\nCustom names for orbits; defaults to “Orbit {index}”.\n\n\nequal_aspect\nbool\nFalse\nFlag to enforce equal scaling for all axes.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nvisualize_orbits_minimal\n\n visualize_orbits_minimal (data:numpy.ndarray,\n                           orbit_indices:Optional[List[int]]=None,\n                           time_instants:Optional[List[int]]=None,\n                           save_path:Optional[str]=None)\n\nVisualizes orbits in 3D space with a completely blank background (no axes, no labels, no grid).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nThe orbit data with shape (num_orbits, 6, num_time_points).\n\n\norbit_indices\nOptional\nNone\nIndices of orbits to visualize; defaults to all.\n\n\ntime_instants\nOptional\nNone\nTime points to highlight; defaults to None.\n\n\nsave_path\nOptional\nNone\nPath to save the figure; defaults to None.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nvisualize_orbits_comparison\n\n visualize_orbits_comparison (data1:numpy.ndarray, data2:numpy.ndarray,\n                              title1:Optional[str]='Set 1',\n                              title2:Optional[str]='Set 2',\n                              equal_aspect:bool=False, title_size:int=18,\n                              title_pad:float=20.0,\n                              shared_scale:bool=False, wspace:float=0.3)\n\n*Visualizes two sets of orbits side by side in 3D space.\nArgs: data1 (np.ndarray): First set of orbit data with shape (num_orbits, 6, num_time_points) data2 (np.ndarray): Second set of orbit data with shape (num_orbits, 6, num_time_points) title1 (str): Title for the first plot title2 (str): Title for the second plot equal_aspect (bool): Flag to enforce equal scaling for all axes title_size (int): Font size for the plot titles title_pad (float): Padding between plot and title in points. Default is 20.0 shared_scale (bool): If True, both plots will share the same scale and limits wspace (float): Width spacing between subplots. Default is 0.3.\nReturns: None*\n\nfrom orbit_generation.data import get_example_orbit_data\nfrom orbit_generation.constants import EM_POINTS\n\n\norbit_data= get_example_orbit_data()\norbit_data.shape\n\n(200, 6, 300)\n\n\n\nvisualize_static_orbits(data= orbit_data, orbit_indices=[15,20,70,140,190], point_dict=EM_POINTS)\n\n\n\n\n\n\n\n\n\nvisualize_static_orbits(data= orbit_data,time_instants=[0,50], orbit_indices=[0,20,40], plot_reference_box=False)\n\n\n\n\n\n\n\n\n\n\nDynamic\n\nsource\n\n\nexport_dynamic_orbits_html\n\n export_dynamic_orbits_html (data:numpy.ndarray,\n                             time_instants:Optional[List[int]]=None,\n                             orbit_indices:Optional[List[int]]=None,\n                             point_dict:Optional[Dict[str,tuple]]=None,\n                             filename:str='orbits.html')\n\nGenerates an interactive 3D visualization of orbits and saves it as an HTML file, including the ability to highlight specific time instants and show named points.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nndarray\n\nOrbit data as a 3D numpy array (num_orbits, 6, num_time_points).\n\n\ntime_instants\nOptional\nNone\nTime instants to highlight.\n\n\norbit_indices\nOptional\nNone\nIndices of orbits to visualize.\n\n\npoint_dict\nOptional\nNone\nNamed points as a dict with 3D coordinates.\n\n\nfilename\nstr\norbits.html\nPath and name of the file to save the HTML plot.\n\n\nReturns\nNone\n\n\n\n\n\n\nexport_dynamic_orbits_html(data=orbit_data, filename='../data/example_data/example_orbits.html')\n\nVisualization saved to ../data/example_data/example_orbits.html\n\n\nView Orbit Visualization",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#orbit-features",
    "href": "visualization.html#orbit-features",
    "title": "Visualization",
    "section": "Orbit Features",
    "text": "Orbit Features\n\nsource\n\nplot_histogram\n\n plot_histogram (data, bins=10, title='Histogram', xlabel='Data',\n                 ylabel='Frequency')\n\n*Plots a histogram for the given data.\nParameters: data : list, array, or pandas Series The data to be plotted. bins : int, optional Number of histogram bins to use (default is 10). title : str, optional Title of the histogram (default is ‘Histogram’). xlabel : str, optional Label for the x-axis (default is ‘Data’). ylabel : str, optional Label for the y-axis (default is ‘Frequency’).*\n\nsource\n\n\nplot_grouped_features\n\n plot_grouped_features (df:pandas.core.frame.DataFrame, columns:List[str],\n                        group_col:str, plot_type:str, figsize:tuple=(5,\n                        5), fontsize:int=10)\n\n*Group the DataFrame by a specified column and plot the specified type of plot for each column for each group.\nParameters: - df : pd.DataFrame : The DataFrame containing the data. - columns : List[str] : List of column names to plot. - group_col : str : Column name to group by. - plot_type : str : Type of plot (‘violin’, ‘box’, ‘facetgrid’, or ‘histogram’). - figsize : tuple : Size of each subplot (width, height). Default is (5, 5). - fontsize : int : Font size for labels and titles. Default is 10.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataFrame containing the data.\n\n\ncolumns\nList\n\nList of column names to plot.\n\n\ngroup_col\nstr\n\nColumn name to group by.\n\n\nplot_type\nstr\n\nType of plot: ‘violin’, ‘box’, ‘facetgrid’, or ‘histogram’\n\n\nfigsize\ntuple\n(5, 5)\nSize of the plot (width, height) per subplot\n\n\nfontsize\nint\n10\nFont size for labels and titles\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nplot_value_proportions\n\n plot_value_proportions (data, grid:str='horizontal',\n                         show_percentages:bool=True,\n                         show_labels:bool=True,\n                         percentage_font_size:int=10,\n                         label_distance:float=1.1,\n                         pct_distance:float=0.85,\n                         explode_factor:float=0.1)\n\nCount occurrences of each unique value in data and plot the proportions in pie charts.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nList or array of labels to plot\n\n\ngrid\nstr\nhorizontal\nOption to plot in grid (horizontal, vertical, or square) or separate images.\n\n\nshow_percentages\nbool\nTrue\nOption to print or not print percentages.\n\n\nshow_labels\nbool\nTrue\nOption to print or not print labels.\n\n\npercentage_font_size\nint\n10\nFont size for percentages.\n\n\nlabel_distance\nfloat\n1.1\nDistance of labels from center.\n\n\npct_distance\nfloat\n0.85\nDistance of percentages from center.\n\n\nexplode_factor\nfloat\n0.1\nFactor to separate slices.\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nplot_mean_distance_by_group_column\n\n plot_mean_distance_by_group_column (df, group_column, value_column)",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#dataframe",
    "href": "visualization.html#dataframe",
    "title": "Visualization",
    "section": "Dataframe",
    "text": "Dataframe\n\nCorrelation Matrix\n\nsource\n\n\nplot_corr_matrix\n\n plot_corr_matrix (dataframe:pandas.core.frame.DataFrame, figsize=(14,\n                   10), cmap='coolwarm', save_path:Optional[str]=None)\n\n*Plots a correlation matrix heatmap with annotations.\nParameters: dataframe (pd.DataFrame): The DataFrame containing the data to be analyzed. figsize (tuple): The size of the figure (width, height). cmap (str): The color map to be used for the heatmap. save_path (Optional[str]): The path to save the plot image. If None, the plot is not saved.\nReturns: None: Displays the correlation matrix heatmap.*\n\n\nFeatures Summary\n\nsource\n\n\nsummarize_and_test\n\n summarize_and_test (df:pandas.core.frame.DataFrame, group_col:str,\n                     features:List[str]=None, visualize:bool=True,\n                     figsize:tuple=(10, 40),\n                     plot_significant_only:bool=True,\n                     plot_continuous:bool=False, show_values:bool=True)",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "visualization.html#other",
    "href": "visualization.html#other",
    "title": "Visualization",
    "section": "Other",
    "text": "Other\n\nImages\n\nsource\n\n\nplot_single_image\n\n plot_single_image (image_path, crop_length=0, font_size=17,\n                    save_path=None, figsize=(15, 15), title=None)\n\n*Plot a single image with customization options.\nArgs: image_path (str): Path to the image file. crop_length (int): Number of pixels to crop from each side of the image. font_size (int): Font size for the title. save_path (str): Path to save the plotted image. If None, the image is not saved. figsize (tuple): Size of the figure (width, height). title (str): Title for the image.\nReturns: None*\n\nsource\n\n\ncreate_image_grid_from_routes\n\n create_image_grid_from_routes (image_routes, crop_length=0, font_size=12,\n                                save_path=None, grid_size=(3, 2),\n                                hspace=-0.37, label_images=None)\n\n*Create a grid of images from a list of image paths.\nArgs: image_routes (list): List of image file paths. crop_length (int): Number of pixels to crop from each side of the image. font_size (int): Font size for the experiment label. save_path (str): Path to save the generated grid image. If None, the grid is not saved. grid_size (tuple): Number of rows and columns in the grid. hspace (float): Vertical spacing between grid rows. label_images (list or bool): List of labels for images or a boolean to add default labels.\nReturns: None*",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "architectures.html",
    "href": "architectures.html",
    "title": "Architectures",
    "section": "",
    "text": "source\n\n\n\n Sampling (*args, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#sampling",
    "href": "architectures.html#sampling",
    "title": "Architectures",
    "section": "",
    "text": "source\n\n\n\n Sampling (*args, **kwargs)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#callback",
    "href": "architectures.html#callback",
    "title": "Architectures",
    "section": "Callback",
    "text": "Callback\n\nsource\n\nVAELossHistory\n\n VAELossHistory ()\n\n*Abstract base class used to build new callbacks.\nSubclass this class and override any of the relevant hooks*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#vaes-encoders-and-decoders",
    "href": "architectures.html#vaes-encoders-and-decoders",
    "title": "Architectures",
    "section": "VAEs Encoders and Decoders",
    "text": "VAEs Encoders and Decoders\n\nsource\n\nVAEEncoder\n\n VAEEncoder (latent_dim:int)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nVAEDecoder\n\n VAEDecoder (latent_dim:int)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\nSimple Convolutional Architecture\n\nKernel Sizes: 5, 7, 9, 13\n\nEncoder\n\nsource\n\n\n\n\nConv5Encoder\n\n Conv5Encoder (seq_len:int, feat_dim:int, latent_dim:int,\n               dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nDecoder\n\nsource\n\n\n\nConv5Decoder\n\n Conv5Decoder (seq_len:int, feat_dim:int, latent_dim:int,\n               dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nGetter\n\nsource\n\n\n\nget_conv5_vae_components\n\n get_conv5_vae_components (seq_len, feat_dim, latent_dim, **kwargs)\n\n*Returns an instance of Conv5Encoder and Conv5Decoder based on the given parameters.\nArgs: seq_len (int): Length of input sequence. feat_dim (int): Dimensionality of input features. latent_dim (int): Dimensionality of the latent space. **kwargs: Additional keyword arguments to be passed to the encoder and decoder.\nReturns: encoder (Conv5Encoder): The encoder part of the VAE. decoder (Conv5Decoder): The decoder part of the VAE.*\n\nLegit Tsgm\n\nEncoder\n\nsource\n\n\n\n\nConv5EncoderLegitTsgm\n\n Conv5EncoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                        dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nDecoder\n\nsource\n\n\n\nConv5DecoderLegitTsgm\n\n Conv5DecoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                        dropout_rate:float)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nGetter\n\nsource\n\n\n\nget_conv5_legit_tsgm_vae_components\n\n get_conv5_legit_tsgm_vae_components (seq_len, feat_dim, latent_dim,\n                                      **kwargs)\n\n*Returns an instance of Conv5Encoder and Conv5Decoder based on the given parameters.\nArgs: seq_len (int): Length of input sequence. feat_dim (int): Dimensionality of input features. latent_dim (int): Dimensionality of the latent space. **kwargs: Additional keyword arguments to be passed to the encoder and decoder.\nReturns: encoder (Conv5Encoder): The encoder part of the VAE. decoder (Conv5Decoder): The decoder part of the VAE.*\n\n\nInception Time\nThe implemetation in the following cell is taken from https://github.com/TheMrGhostman/InceptionTime-Pytorch/blob/master/inception.py , the next cell is an adjustment for our problem\n\nsource\n\n\nInceptionBlock\n\n InceptionBlock (in_channels, n_filters=32, kernel_sizes=[9, 19, 39],\n                 bottleneck_channels=32, use_residual=True,\n                 activation=ReLU(), return_indices=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInception\n\n Inception (in_channels, n_filters, kernel_sizes=[9, 19, 39],\n            bottleneck_channels=32, activation=ReLU(),\n            return_indices=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\npass_through\n\n pass_through (X)\n\n\nsource\n\n\ncorrect_sizes\n\n correct_sizes (sizes)\n\n\nsource\n\n\nInceptionTransposeBlockWithoutPool\n\n InceptionTransposeBlockWithoutPool (in_channels, out_channels=32,\n                                     kernel_sizes=[9, 19, 39],\n                                     bottleneck_channels=32,\n                                     use_residual=True, activation=ReLU())\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInceptionTransposeWithoutPool\n\n InceptionTransposeWithoutPool (in_channels, out_channels,\n                                kernel_sizes=[9, 19, 39],\n                                bottleneck_channels=32, activation=ReLU())\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInceptionBlockWithoutPool\n\n InceptionBlockWithoutPool (in_channels, n_filters=32, kernel_sizes=[9,\n                            19, 39], bottleneck_channels=32,\n                            use_residual=True, activation=ReLU(),\n                            return_indices=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\nInceptionWithoutPool\n\n InceptionWithoutPool (in_channels, n_filters, kernel_sizes=[9, 19, 39],\n                       bottleneck_channels=32, activation=ReLU())\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nEncoder\n\nsource\n\n\n\nInceptionTimeVAEEncoder\n\n InceptionTimeVAEEncoder (feat_dim=7, seq_len=100, n_filters=32,\n                          kernel_sizes=[5, 11, 23],\n                          bottleneck_channels=32, latent_dim=2)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nWPInceptionTimeVAEEncoder\n\n WPInceptionTimeVAEEncoder (feat_dim=7, seq_len=100, n_filters=32,\n                            kernel_sizes=[5, 11, 23],\n                            bottleneck_channels=32, latent_dim=2)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nDecoder\n\nsource\n\n\n\nWPInceptionTimeVAEDecoder\n\n WPInceptionTimeVAEDecoder (feat_dim=7, seq_len=100, n_filters=32,\n                            kernel_sizes=[5, 11, 23],\n                            bottleneck_channels=32, latent_dim=2)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nGetter\n\nsource\n\n\n\nget_inception_time_vae_components\n\n get_inception_time_vae_components (seq_len, feat_dim, latent_dim,\n                                    without_pooling=True, **model_kwargs)\n\n*Returns an instance of InceptionTimeVAEEncoder or WPInceptionTimeVAEEncoder and InceptionTimeVAEDecoder based on the given parameters.\nArgs: seq_len (int): Length of input sequence. feat_dim (int): Dimensionality of input features. latent_dim (int): Dimensionality of the latent space. model_kwargs (dict): Dictionary containing model-specific keyword arguments. without_pooling (bool): If True, returns WPInceptionTimeVAEEncoder instead of InceptionTimeVAEEncoder.\nReturns: encoder (InceptionTimeVAEEncoder or WPInceptionTimeVAEEncoder): The encoder part of the VAE. decoder (InceptionTimeVAEDecoder): The decoder part of the VAE.*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "architectures.html#cvaes-encoders-and-decoders",
    "href": "architectures.html#cvaes-encoders-and-decoders",
    "title": "Architectures",
    "section": "cVAEs Encoders and Decoders",
    "text": "cVAEs Encoders and Decoders\n\nsource\n\ncVAEEncoder\n\n cVAEEncoder (latent_dim:int)\n\nAbstract base for a conditional VAE encoder: Encodes data + condition into z_mean, z_log_var\n\nsource\n\n\ncVAEDecoder\n\n cVAEDecoder (latent_dim:int)\n\nAbstract base for a conditional VAE decoder: Decodes z + condition into reconstructed data\n\n\nSimple Convolutions\n\nsource\n\n\ncConv5EncoderLegitTsgm\n\n cConv5EncoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                         cond_dim:int, dropout_rate:float)\n\nAbstract base for a conditional VAE encoder: Encodes data + condition into z_mean, z_log_var\n\nsource\n\n\ncConv5DecoderLegitTsgm\n\n cConv5DecoderLegitTsgm (seq_len:int, feat_dim:int, latent_dim:int,\n                         cond_dim:int, dropout_rate:float)\n\nAbstract base for a conditional VAE decoder: Decodes z + condition into reconstructed data\n\nsource\n\n\nget_conditional_conv5_legit_tsgm_vae_components\n\n get_conditional_conv5_legit_tsgm_vae_components (seq_len, feat_dim,\n                                                  latent_dim, **kwargs)\n\n*Returns an instance of Conv5Encoder and Conv5Decoder based on the given parameters.\nArgs: seq_len (int): Length of input sequence. feat_dim (int): Dimensionality of input features. latent_dim (int): Dimensionality of the latent space. **kwargs: Additional keyword arguments to be passed to the encoder and decoder.\nReturns: encoder (Conv5Encoder): The encoder part of the VAE. decoder (Conv5Decoder): The decoder part of the VAE.*",
    "crumbs": [
      "Architectures"
    ]
  },
  {
    "objectID": "paper_specific.html",
    "href": "paper_specific.html",
    "title": "Paper",
    "section": "",
    "text": "source\n\nplot_latent_space_with_feature_distributions_paper\n\n plot_latent_space_with_feature_distributions_paper\n                                                     (latent_representatio\n                                                     ns:numpy.ndarray,\n                                                     labels:numpy.ndarray,\n                                                     features:Optional[num\n                                                     py.ndarray]=None, fea\n                                                     ture_names:Optional[l\n                                                     ist]=None,\n                                                     figsize:tuple=(12,\n                                                     12), save_path:Option\n                                                     al[str]=None, many_cl\n                                                     asses:bool=False, sho\n                                                     w_legend:bool=True, l\n                                                     egend_fontsize:int=12\n                                                     , title_fontsize:int=\n                                                     14, **kwargs:Any)\n\nPlots the latent space with class colors and normalized vertical and horizontal feature distributions in separate subplots.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlatent_representations\nndarray\n\n\n\n\nlabels\nndarray\n\n\n\n\nfeatures\nOptional\nNone\n\n\n\nfeature_names\nOptional\nNone\n\n\n\nfigsize\ntuple\n(12, 12)\nAdjusted to be a square\n\n\nsave_path\nOptional\nNone\n\n\n\nmany_classes\nbool\nFalse\n\n\n\nshow_legend\nbool\nTrue\n\n\n\nlegend_fontsize\nint\n12\n\n\n\ntitle_fontsize\nint\n14\nAdjusted fontsize for consistent titles\n\n\nkwargs\nAny\n\n\n\n\nReturns\nNone",
    "crumbs": [
      "Paper"
    ]
  },
  {
    "objectID": "constants.html",
    "href": "constants.html",
    "title": "Constants",
    "section": "",
    "text": "MU_BY_SYSTEM\n\n{'SaE': 1.901109735892602e-07,\n 'MP': 1.611081404409632e-08,\n 'SaT': 0.0002366393158331484,\n 'EM': 0.01215058560962404,\n 'JE': 2.52801752854e-05,\n 'SE': 3.0542e-06,\n 'SM': 3.227154996101724e-07}\n\n\n\nEM_POINTS\n\n{'Moon': (0.987849414390376, 0, 0),\n 'Earth': (-0.01215058560962404, 0, 0),\n 'Lagrange 1': (0.8369, 0, 0),\n 'Lagrange 2': (1.1557, 0, 0),\n 'Lagrange 3': (-1.0051, 0, 0),\n 'Lagrange 4': (0.4879, 0.866, 0),\n 'Lagrange 5': (0.4879, -0.866, 0)}",
    "crumbs": [
      "Constants"
    ]
  },
  {
    "objectID": "constants.html#system-points",
    "href": "constants.html#system-points",
    "title": "Constants",
    "section": "",
    "text": "MU_BY_SYSTEM\n\n{'SaE': 1.901109735892602e-07,\n 'MP': 1.611081404409632e-08,\n 'SaT': 0.0002366393158331484,\n 'EM': 0.01215058560962404,\n 'JE': 2.52801752854e-05,\n 'SE': 3.0542e-06,\n 'SM': 3.227154996101724e-07}\n\n\n\nEM_POINTS\n\n{'Moon': (0.987849414390376, 0, 0),\n 'Earth': (-0.01215058560962404, 0, 0),\n 'Lagrange 1': (0.8369, 0, 0),\n 'Lagrange 2': (1.1557, 0, 0),\n 'Lagrange 3': (-1.0051, 0, 0),\n 'Lagrange 4': (0.4879, 0.866, 0),\n 'Lagrange 5': (0.4879, -0.866, 0)}",
    "crumbs": [
      "Constants"
    ]
  },
  {
    "objectID": "constants.html#orbit-classification",
    "href": "constants.html#orbit-classification",
    "title": "Constants",
    "section": "Orbit Classification",
    "text": "Orbit Classification\n\nFamily\n\nORBIT_FAMILIES.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n\n\n\n\nId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n\n\nLabel\nS_BN\nS_BS\nS_DN\nS_DPO\nS_DRO\nS_DS\nS_L1_A\nS_L1_HN\nS_L1_HS\nS_L1_L\n...\nS_R12\nS_R13\nS_R14\nS_R21\nS_R23\nS_R31\nS_R32\nS_R34\nS_R41\nS_R43\n\n\n\n\n2 rows × 42 columns\n\n\n\n\n\nFull Organization\nOrganization done by Walther Litteri\n\nEXTENDED_ORBIT_CLASSIFICATION\n\n\n\n\n\n\n\n\nId\nLabel\nType\nSubtype\nDirection\n\n\n\n\n0\n1\nS_BN\nSystem-wide\nButterfly\nNorth\n\n\n1\n2\nS_BS\nSystem-wide\nButterfly\nSouth\n\n\n2\n3\nS_DN\nSystem-wide\nDragonfly\nNorth\n\n\n3\n4\nS_DPO\nSystem-wide\nDistant Prograde\nPlanar\n\n\n4\n5\nS_DRO\nSystem-wide\nDistant Retrograde\nPlanar\n\n\n5\n6\nS_DS\nSystem-wide\nDragonfly\nSouth\n\n\n6\n7\nS_L1_A\nL1\nAxial\nNo specification\n\n\n7\n8\nS_L1_HN\nL1\nHalo\nNorth\n\n\n8\n9\nS_L1_HS\nL1\nHalo\nSouth\n\n\n9\n10\nS_L1_L\nL1\nLyapunov\nPlanar\n\n\n10\n11\nS_L1_V\nL1\nVertical\nNo specification\n\n\n11\n12\nS_L2_A\nL2\nAxial\nNo specification\n\n\n12\n13\nS_L2_HN\nL2\nHalo\nNorth\n\n\n13\n14\nS_L2_HS\nL2\nHalo\nSouth\n\n\n14\n15\nS_L2_L\nL2\nLyapunov\nPlanar\n\n\n15\n16\nS_L2_V\nL2\nVertical\nNo specification\n\n\n16\n17\nS_L3_A\nL3\nAxial\nNo specification\n\n\n17\n18\nS_L3_HN\nL3\nHalo\nNorth\n\n\n18\n19\nS_L3_HS\nL3\nHalo\nSouth\n\n\n19\n20\nS_L3_L\nL3\nLyapunov\nPlanar\n\n\n20\n21\nS_L3_V\nL3\nVertical\nNo specification\n\n\n21\n22\nS_L4_A\nL4\nAxial\nNo specification\n\n\n22\n23\nS_L4_LP\nL4\nLong Period\nNo specification\n\n\n23\n24\nS_L4_SP\nL4\nShort Period\nNo specification\n\n\n24\n25\nS_L4_V\nL4\nVertical\nNo specification\n\n\n25\n26\nS_L5_A\nL5\nAxial\nNo specification\n\n\n26\n27\nS_L5_LP\nL5\nLong Period\nNo specification\n\n\n27\n28\nS_L5_SP\nL5\nShort Period\nNo specification\n\n\n28\n29\nS_L5_V\nL5\nVertical\nNo specification\n\n\n29\n30\nS_LPOE\nSystem-wide\nLow Prograde\nEast\n\n\n30\n31\nS_LPOW\nSystem-wide\nLow Prograde\nWest\n\n\n31\n32\nS_R11\nResonant\nResonant 1,1\nPlanar\n\n\n32\n33\nS_R12\nResonant\nResonant 1,2\nPlanar\n\n\n33\n34\nS_R13\nResonant\nResonant 1,3\nPlanar\n\n\n34\n35\nS_R14\nResonant\nResonant 1,4\nPlanar\n\n\n35\n36\nS_R21\nResonant\nResonant 2,1\nPlanar\n\n\n36\n37\nS_R23\nResonant\nResonant 2,3\nPlanar\n\n\n37\n38\nS_R31\nResonant\nResonant 3,1\nPlanar\n\n\n38\n39\nS_R32\nResonant\nResonant 3,2\nPlanar\n\n\n39\n40\nS_R34\nResonant\nResonant 3,4\nPlanar\n\n\n40\n41\nS_R41\nResonant\nResonant 4,1\nPlanar\n\n\n41\n42\nS_R43\nResonant\nResonant 4,3\nPlanar",
    "crumbs": [
      "Constants"
    ]
  },
  {
    "objectID": "path_utils.html",
    "href": "path_utils.html",
    "title": "Path Utils",
    "section": "",
    "text": "source\n\n\n\n get_julia_file_path (filename:str)\n\nGet the absolute path to a Julia file.\n\nsource\n\n\n\n\n make_project_path (*parts)\n\nCreate a path relative to the project root.\n\nsource\n\n\n\n\n get_data_path ()\n\nGet the path to the data directory.\n\nsource\n\n\n\n\n get_project_root ()\n\nGet the project root directory.",
    "crumbs": [
      "Path Utils"
    ]
  },
  {
    "objectID": "path_utils.html#path-handling",
    "href": "path_utils.html#path-handling",
    "title": "Path Utils",
    "section": "",
    "text": "source\n\n\n\n get_julia_file_path (filename:str)\n\nGet the absolute path to a Julia file.\n\nsource\n\n\n\n\n make_project_path (*parts)\n\nCreate a path relative to the project root.\n\nsource\n\n\n\n\n get_data_path ()\n\nGet the path to the data directory.\n\nsource\n\n\n\n\n get_project_root ()\n\nGet the project root directory.",
    "crumbs": [
      "Path Utils"
    ]
  },
  {
    "objectID": "model_factory.html",
    "href": "model_factory.html",
    "title": "Model Factory",
    "section": "",
    "text": "source\n\nget_model\n\n get_model (params)",
    "crumbs": [
      "Model Factory"
    ]
  },
  {
    "objectID": "experiment.html",
    "href": "experiment.html",
    "title": "Experiment",
    "section": "",
    "text": "source\n\n\n\n\n setup_new_experiment (params:Dict[str,Any], experiments_folder:str,\n                       json_file:Optional[str]=None)\n\nSets up a new experiment by creating a new folder and updating the JSON file with experiment parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nDict\n\nDictionary of parameters for the new experiment.\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters.\n\n\nReturns\nstr\n\nThe path to the newly created experiment folder.\n\n\n\n\nsource\n\n\n\n\n create_experiments_json (parameter_sets, output_file='experiments.json')\n\n*Create an experiments.json file from given parameter sets.\nArgs: parameter_sets (list): List of dictionaries containing parameters for each experiment. output_file (str): Name of the output JSON file. Defaults to ‘experiments.json’.\nReturns: None*\n\n\n\n\nsource\n\n\n\n\n convert_numpy_types (obj)\n\nRecursively convert numpy types and tensors to native Python types for JSON serialization.\n\nsource\n\n\n\n\n add_experiment_metrics (experiments_folder:str,\n                         params:Optional[Dict[str,Any]]=None,\n                         experiment_id:Optional[int]=None,\n                         metrics:Optional[Dict[str,Any]]=None,\n                         json_file:Optional[str]=None)\n\nAdds metrics to an existing experiment in the JSON file based on the given parameters or ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nparams\nOptional\nNone\nOptional dictionary of parameters identifying the experiment.\n\n\nexperiment_id\nOptional\nNone\nOptional ID to identify the experiment.\n\n\nmetrics\nOptional\nNone\nOptional dictionary of metrics to be added to the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_parameters (experiments_folder:str, experiment_id:int,\n                            json_file:Optional[str]=None)\n\nRetrieves the parameters of an experiment from the JSON file based on the given ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nexperiment_id\nint\n\nID to identify the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nDict\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_data (experiments_folder:str, experiment_id:int,\n                      json_file:Optional[str]=None)\n\n*Retrieves all data for an experiment from the JSON file based on the given ID.\nArgs: experiments_folder (str): Path to the folder containing all experiments. experiment_id (int): ID to identify the experiment. json_file (Optional[str]): Optional path to the JSON file tracking experiment data.\nReturns: Dict[str, Any]: A dictionary containing all data for the specified experiment.\nRaises: FileNotFoundError: If the experiments folder or JSON file doesn’t exist. ValueError: If the experiment with the specified ID is not found.*\n\nsource\n\n\n\n\n read_json_to_dataframe (json_path:str)\n\n*Reads a JSON file containing experiment results and returns a DataFrame.\nArgs: - json_path (str): The path to the JSON file.\nReturns: - pd.DataFrame: A DataFrame containing the experiment results.*",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#json-management",
    "href": "experiment.html#json-management",
    "title": "Experiment",
    "section": "",
    "text": "source\n\n\n\n\n setup_new_experiment (params:Dict[str,Any], experiments_folder:str,\n                       json_file:Optional[str]=None)\n\nSets up a new experiment by creating a new folder and updating the JSON file with experiment parameters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nDict\n\nDictionary of parameters for the new experiment.\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters.\n\n\nReturns\nstr\n\nThe path to the newly created experiment folder.\n\n\n\n\nsource\n\n\n\n\n create_experiments_json (parameter_sets, output_file='experiments.json')\n\n*Create an experiments.json file from given parameter sets.\nArgs: parameter_sets (list): List of dictionaries containing parameters for each experiment. output_file (str): Name of the output JSON file. Defaults to ‘experiments.json’.\nReturns: None*\n\n\n\n\nsource\n\n\n\n\n convert_numpy_types (obj)\n\nRecursively convert numpy types and tensors to native Python types for JSON serialization.\n\nsource\n\n\n\n\n add_experiment_metrics (experiments_folder:str,\n                         params:Optional[Dict[str,Any]]=None,\n                         experiment_id:Optional[int]=None,\n                         metrics:Optional[Dict[str,Any]]=None,\n                         json_file:Optional[str]=None)\n\nAdds metrics to an existing experiment in the JSON file based on the given parameters or ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nparams\nOptional\nNone\nOptional dictionary of parameters identifying the experiment.\n\n\nexperiment_id\nOptional\nNone\nOptional ID to identify the experiment.\n\n\nmetrics\nOptional\nNone\nOptional dictionary of metrics to be added to the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_parameters (experiments_folder:str, experiment_id:int,\n                            json_file:Optional[str]=None)\n\nRetrieves the parameters of an experiment from the JSON file based on the given ID.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexperiments_folder\nstr\n\nPath to the folder containing all experiments.\n\n\nexperiment_id\nint\n\nID to identify the experiment.\n\n\njson_file\nOptional\nNone\nOptional path to the JSON file tracking experiment parameters and metrics.\n\n\nReturns\nDict\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n get_experiment_data (experiments_folder:str, experiment_id:int,\n                      json_file:Optional[str]=None)\n\n*Retrieves all data for an experiment from the JSON file based on the given ID.\nArgs: experiments_folder (str): Path to the folder containing all experiments. experiment_id (int): ID to identify the experiment. json_file (Optional[str]): Optional path to the JSON file tracking experiment data.\nReturns: Dict[str, Any]: A dictionary containing all data for the specified experiment.\nRaises: FileNotFoundError: If the experiments folder or JSON file doesn’t exist. ValueError: If the experiment with the specified ID is not found.*\n\nsource\n\n\n\n\n read_json_to_dataframe (json_path:str)\n\n*Reads a JSON file containing experiment results and returns a DataFrame.\nArgs: - json_path (str): The path to the JSON file.\nReturns: - pd.DataFrame: A DataFrame containing the experiment results.*",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#data-management",
    "href": "experiment.html#data-management",
    "title": "Experiment",
    "section": "Data Management",
    "text": "Data Management\n\nGet Paths\n\nsource\n\n\ngenerate_image_paths\n\n generate_image_paths (folder_prefix, unique_ids, file_suffix)\n\n\n\nGet Orbits\n\nsource\n\n\nconcatenate_orbits_from_experiment_folder\n\n concatenate_orbits_from_experiment_folder (experiments_folder, seq_len,\n                                            file_suffix='_generated_orbits\n                                            ')\n\n\n\nGet Tables\n\nsource\n\n\nconcatenate_csvs_from_experiment_folder\n\n concatenate_csvs_from_experiment_folder (experiments_folder, file_suffix)\n\n\n\nGet Orbit and Table\n\nsource\n\n\nconcatenate_and_check_orbits_from_experiment_folder\n\n concatenate_and_check_orbits_from_experiment_folder (experiments_folder,\n                                                      csv_file_name='_refi\n                                                      ned_orbits_df.csv', \n                                                      np_file_name='_refin\n                                                      ed_orbits')",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#parameters",
    "href": "experiment.html#parameters",
    "title": "Experiment",
    "section": "Parameters",
    "text": "Parameters\n\nsource\n\ngenerate_parameter_sets\n\n generate_parameter_sets (params, model_specific_params)",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#experiment-execution",
    "href": "experiment.html#experiment-execution",
    "title": "Experiment",
    "section": "Experiment Execution",
    "text": "Experiment Execution\n\nsource\n\nexecute_parameter_notebook\n\n execute_parameter_notebook (notebook_to_execute, output_dir, i, params,\n                             extra_parameters=None, checkpoint_file=None)\n\n\nsource\n\n\nparalelize_notebook_experiment\n\n paralelize_notebook_experiment (parameter_sets, notebook_to_execute,\n                                 output_dir, checkpoint_file,\n                                 max_workers=3, extra_parameters=None)",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "experiment.html#functions",
    "href": "experiment.html#functions",
    "title": "Experiment",
    "section": "Functions",
    "text": "Functions\n\nsource\n\ngenerate_file_paths\n\n generate_file_paths (experiment_id, images_folder, experiment_folder)\n\n*Generate a dictionary of file paths for an experiment.\nParameters: experiment_id (int or str): The unique ID of the experiment. images_folder (str): The folder path where image files are stored. experiment_folder (str): The folder path where experiment-related files are stored.\nReturns: dict: A dictionary containing all generated file paths.*\n\nsource\n\n\nprepare_experiment_data\n\n prepare_experiment_data (params, experiments_folder, data_path,\n                          want_to_discover)\n\n*Prepare the experiment data based on the provided parameters and configurations.\nParameters: params (dict): A dictionary containing all the experiment parameters. experiments_folder (str): The folder where experiments are stored. data_path (str): Path to the dataset file. want_to_discover (bool): Flag indicating whether to discover new families or use existing ones.\nReturns: tuple: Processed scaled data, orbit dataframe, family labels, and additional metadata.*\n\nsource\n\n\nprepare_and_train_model\n\n prepare_and_train_model (params, scaled_data, experiments_folder,\n                          experiment_id, file_paths, want_to_train)\n\n*Prepare the model and either train it or load a pre-trained version based on the provided parameters.\nParameters: params (dict): A dictionary containing all the experiment parameters. scaled_data (torch.Tensor): The scaled data to be used for training or validation. experiments_folder (str): The folder where experiments are stored. experiment_id (int): The unique ID of the current experiment. file_paths (dict): A dictionary containing file paths for saving/loading model and metrics. want_to_train (bool): Flag indicating whether to train the model or load a pre-trained one.\nReturns: object: The trained or loaded model.*",
    "crumbs": [
      "Experiment"
    ]
  },
  {
    "objectID": "propagation.html",
    "href": "propagation.html",
    "title": "Propagation",
    "section": "",
    "text": "Propagation implemented by Walther Litteri",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#tolerance-constants",
    "href": "propagation.html#tolerance-constants",
    "title": "Propagation",
    "section": "Tolerance Constants",
    "text": "Tolerance Constants",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#jacobi-constant",
    "href": "propagation.html#jacobi-constant",
    "title": "Propagation",
    "section": "Jacobi Constant",
    "text": "Jacobi Constant\n\nsource\n\njacobi_constant\n\n jacobi_constant (X:numpy.ndarray, mu:float)\n\n*State-dependent Jacobi constant for a given state vector X and gravitational parameter mu.\nParameters: X (np.ndarray): Cartesian state vector with 6 components (x, y, z, xp, yp, zp). mu (float): Gravitational parameter.\nReturns: Tuple[float, float]: Jacobi constant (J) and total energy (E).*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nCartesian state vector with 6 components (x, y, z, xp, yp, zp)\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nTuple\n\n\n\n\n\norbit_data = get_example_orbit_data()\norbit_data.shape\n\n(200, 6, 300)\n\n\n\n# Calculate Jacobi constants and energies for all orbits at all time points\njacobi_constants = np.zeros((200, 300))\ntotal_energies = np.zeros((200, 300))\n\nfor orbit_index in range(200):\n    for time_index in range(300):\n        X = orbit_data[orbit_index, :, time_index]\n        J, E = jacobi_constant(X, EM_MU)\n        jacobi_constants[orbit_index, time_index] = J\n        total_energies[orbit_index, time_index] = E\n\n# Flatten the Jacobi constants array to plot the histogram of all values\njacobi_constants_all = jacobi_constants.flatten()\n\n# Plot histogram of Jacobi constants for all orbits\nplt.figure(figsize=(10, 5))\nplt.hist(jacobi_constants_all, bins=50, color='blue', alpha=0.7)\nplt.title('Histogram of Jacobi Constants for All Orbits')\nplt.xlabel('Jacobi Constant')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n# Plot histogram of Jacobi constants for the first orbit\njacobi_constants_first_orbit = jacobi_constants[0, :]\n\nplt.figure(figsize=(10, 5))\nplt.hist(jacobi_constants_first_orbit, bins=50, color='green', alpha=0.7)\nplt.title('Histogram of Jacobi Constants for the First Orbit')\nplt.xlabel('Jacobi Constant')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#equations-of-motion-cr3bp",
    "href": "propagation.html#equations-of-motion-cr3bp",
    "title": "Propagation",
    "section": "Equations of motion CR3BP",
    "text": "Equations of motion CR3BP\n\nsource\n\neom_cr3bp\n\n eom_cr3bp (t:float, X:numpy.ndarray, mu:float)\n\n*Equations of motion for the Circular Restricted 3 Body Problem (CR3BP). The form is X_dot = f(t, X, (parameters,)). This formulation is time-independent as it does not depend explicitly on t.\nParameters: t (float): Time variable (not used in this formulation). X (np.ndarray): State vector with 6 components (x, y, z, v_x, v_y, v_z). mu (float): Gravitational parameter.\nReturns: List[float]: Derivatives of the state vector.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nt\nfloat\nTime variable (not used in this formulation)\n\n\nX\nndarray\nState vector with 6 components (x, y, z, v_x, v_y, v_z)\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nList\n\n\n\n\n\n# Select a random orbit from the dataset\nnum_orbits, num_components, num_time_points = orbit_data.shape\nrandom_orbit_index = np.random.randint(0, num_orbits)\nX0 = orbit_data[random_orbit_index, :, 0]\nmu = 0.01215058560962404\nT0 = 2.7430007981241529E+0  # Total time for the propagation, can be adjusted as needed\n\n# Propagate the orbit using solve_ivp\nsol = solve_ivp(eom_cr3bp, [0, T0], X0, args=(mu,), dense_output=True, rtol=1e-9, atol=1e-9, method='Radau')\ntvec = np.linspace(0, T0, num_time_points)\nz = sol.sol(tvec)\n\n# Compute derivatives using eom_cr3bp for a specific state in the propagated orbit\ntime_index = np.random.randint(0, num_time_points - 1)  # Choose a random time index\nt = tvec[time_index]\nX = z[:, time_index]\ncomputed_derivatives = eom_cr3bp(t, X, mu)\n\n# Compare with actual changes in state vector\ndelta_t = tvec[1] - tvec[0]\nactual_derivatives = (z[:, time_index + 1] - z[:, time_index]) / delta_t\n\n# Visualize the actual trajectory and computed derivatives\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\n\n# Plot the actual trajectory\naxs[0].plot(z[0], z[1], label='Trajectory')\naxs[0].scatter(z[0, time_index], z[1, time_index], color='red', label='Point of Interest')\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].set_title('Trajectory in the XY plane')\naxs[0].legend()\naxs[0].grid(True)\n\n# Plot computed vs. actual derivatives\nlabels = ['x_dot', 'y_dot', 'z_dot', 'x_ddot', 'y_ddot', 'z_ddot']\nwidth = 0.3  # width of the bars\nx = np.arange(len(labels))  # the label locations\n\naxs[1].bar(x - width/2, computed_derivatives, width, label='Computed')\naxs[1].bar(x + width/2, actual_derivatives, width, label='Actual')\naxs[1].set_xticks(x)\naxs[1].set_xticklabels(labels)\naxs[1].set_xlabel('Derivative')\naxs[1].set_ylabel('Value')\naxs[1].set_title('Computed vs Actual Derivatives')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#propagation",
    "href": "propagation.html#propagation",
    "title": "Propagation",
    "section": "Propagation",
    "text": "Propagation\n\nsource\n\nprop_node\n\n prop_node (X:numpy.ndarray, dt:float, mu:float)\n\n*Return the state X after a given time step dt = T_end - T_start.\nParameters: X (np.ndarray): Initial state vector with 6 components (x, y, z, v_x, v_y, v_z). dt (float): Time step for propagation. mu (float): Gravitational parameter.\nReturns: np.ndarray: Final state vector after time step dt.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nInitial state vector with 6 components (x, y, z, v_x, v_y, v_z)\n\n\ndt\nfloat\nTime step for propagation\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nndarray\n\n\n\n\n\n# Select a random orbit from the dataset\nnum_orbits, num_components, num_time_points = orbit_data.shape\nrandom_orbit_index = np.random.randint(0, num_orbits)\nX0 = orbit_data[random_orbit_index, :, 0]\nmu = 0.01215058560962404\ndt = 0.1  # Small time step for propagation\n\n# Propagate the state vector using prop_node\nX_final = prop_node(X0, dt, mu)\n\n# Print the initial and final state vectors\nprint(\"Initial state vector:\", X0)\nprint(\"Final state vector after time step dt:\", X_final)\n\n# To visualize the propagation, we can propagate over multiple time steps and plot the trajectory\nT_total = 2.0  # Total time for propagation\ntime_steps = int(T_total / dt)\ntrajectory = np.zeros((time_steps + 1, 6))\ntrajectory[0] = X0\n\n# Propagate step by step\nX_current = X0\nfor i in range(1, time_steps + 1):\n    X_current = prop_node(X_current, dt, mu)\n    trajectory[i] = X_current\n\n# Plot the trajectory\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2], label='Propagated trajectory')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Trajectory Propagation using prop_node')\nax.legend()\nplt.show()\n\nInitial state vector: [ 1.02948208  0.         -0.1867957   0.         -0.11894369  0.        ]\nFinal state vector after time step dt: [ 1.02873876 -0.01181343 -0.18439385 -0.01485109 -0.11651009  0.04812437]",
    "crumbs": [
      "Propagation"
    ]
  },
  {
    "objectID": "propagation.html#compute-error",
    "href": "propagation.html#compute-error",
    "title": "Propagation",
    "section": "Compute Error",
    "text": "Compute Error\n\nsource\n\njacobi_test\n\n jacobi_test (X:numpy.ndarray, mu:float)\n\n*Compute the energy error. X can have either 6 columns (state vector) or 7 columns (time + state vector). The returned quantity is the cumulative error with respect to the initial value. If propagation is perfect, err = 0 (or very small).\nParameters: X (np.ndarray): State vector with shape (n, 6) or (n, 7), where n is the number of samples. mu (float): Gravitational parameter.\nReturns: float: Cumulative energy error with respect to the initial value.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nState vector with shape (n, 6) or (n, 7), where n is the number of samples\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nfloat\n\n\n\n\n\nsource\n\n\ndynamics_defect\n\n dynamics_defect (X:numpy.ndarray, mu:float)\n\n*Compute the dynamical defect for the generated time-state sequence. The returned quantity is the cumulative error on the position and velocity components. The overall metrics can be a combination of these two last errors.\nParameters: X (np.ndarray): Time-state vector with shape (n, 7), where the first column is the time vector. mu (float): Gravitational parameter.\nReturns: Tuple[float, float]: Cumulative errors in position and velocity.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nTime-state vector with shape (n, 7), where the first column is the time vector\n\n\nmu\nfloat\nGravitational parameter\n\n\nReturns\nTuple\n\n\n\n\n\n# Select a random orbit from the dataset\nnum_orbits, num_components, num_time_points = orbit_data.shape\nrandom_orbit_index = np.random.randint(0, num_orbits)\nselected_orbit = orbit_data[random_orbit_index, :, :]\n\n# Add a time column to the state vector for dynamics_defect function\n# Assuming the time steps are evenly spaced and given by the array tvec\ntvec = np.linspace(0, 2.7430007981241529E+0, num_time_points)\ntime_state_vector = np.hstack((tvec.reshape(-1, 1), selected_orbit.T))\n\n# Test jacobi_test function\nenergy_error = jacobi_test(selected_orbit.T, mu)\nprint(\"Cumulative energy error for the selected orbit:\", energy_error)\n\n# Test dynamics_defect function\npos_error, vel_error = dynamics_defect(time_state_vector, mu)\nprint(\"Cumulative position error for the selected orbit:\", pos_error)\nprint(\"Cumulative velocity error for the selected orbit:\", vel_error)\n\n# Visualize the numerically propagated orbit\nvisualize_static_orbits(orbit_data, time_instants=[0, 100, 200, 295], orbit_indices=[random_orbit_index])\n\n# Visualize the cumulative errors calculated\nfig, ax = plt.subplots(figsize=(10, 6))\n\nlabels = ['Cumulative Position Error', 'Cumulative Velocity Error']\nerrors = [pos_error, vel_error]\n\nax.bar(labels, errors, color=['blue', 'green'])\nax.set_ylabel('Error')\nax.set_title('Cumulative Position and Velocity Errors')\nax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nCumulative energy error for the selected orbit: 0.00022752154970095972\nCumulative position error for the selected orbit: 0.07835299756599234\nCumulative velocity error for the selected orbit: 0.147666860309554\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ncalculate_errors\n\n calculate_errors (orbit_data:numpy.ndarray, mu:float,\n                   orbit_indices:List[int]=None,\n                   error_types:List[str]=['position', 'velocity',\n                   'energy'], time_step:Optional[float]=None,\n                   display_results:bool=True, cumulative:bool=False)\n\nCalculate and return the cumulative error and the average error per time step for the selected orbits together. Optionally, display the evolution of each error as a chart.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\n3D array of orbit data\n\n\nmu\nfloat\n\nGravitational parameter\n\n\norbit_indices\nList\nNone\nList of integers referring to the orbits to analyze\n\n\nerror_types\nList\n[‘position’, ‘velocity’, ‘energy’]\nTypes of errors to calculate\n\n\ntime_step\nOptional\nNone\nOptional time step if time dimension is not included\n\n\ndisplay_results\nbool\nTrue\nBoolean to control whether to display the results\n\n\ncumulative\nbool\nFalse\nBoolean to control cumulative or average error\n\n\nReturns\nDict\n\n\n\n\n\n\nerrors = calculate_errors(orbit_data, EM_MU, orbit_indices = [0, 1, 2], time_step=0.00917391571278981)\n\nCumulative position error for selected orbits: 0.234824159550522\nAverage position error per time step: 0.0002617883607029231\n\n\n\n\n\n\n\n\n\nCumulative velocity error for selected orbits: 0.44235011161515836\nAverage velocity error per time step: 0.0004931439371406447\n\n\n\n\n\n\n\n\n\nCumulative energy error for selected orbits: 0.0006821222552635398\nAverage energy error per time step: 7.604484451098548e-07\n\n\n\n\n\n\n\n\n\n\nsource\n\n\ncalculate_errors_per_orbit\n\n calculate_errors_per_orbit (orbit_data:numpy.ndarray, mu:float,\n                             error_types:List[str]=['position',\n                             'velocity', 'energy'],\n                             time_step:Optional[float]=None,\n                             display_results:bool=False)\n\nCalculate and return the average error per orbit for the selected error types. Optionally, display the evolution of each error as a chart.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norbit_data\nndarray\n\n3D array of orbit data\n\n\nmu\nfloat\n\nGravitational parameter\n\n\nerror_types\nList\n[‘position’, ‘velocity’, ‘energy’]\nTypes of errors to calculate\n\n\ntime_step\nOptional\nNone\nOptional time step if time dimension is not included\n\n\ndisplay_results\nbool\nFalse\nBoolean to control whether to display the results\n\n\nReturns\nDict\n\n\n\n\n\n\nerrors = calculate_errors_per_orbit(orbit_data[0:3], EM_MU, time_step=0.00917391571278981, display_results=True)\n\nAverage Position Error per Orbit:\n[0.00026157 0.00026175 0.00026205]\nAverage Velocity Error per Orbit:\n[0.00049252 0.00049305 0.00049387]\nAverage Energy Error per Orbit:\n[7.60028806e-07 7.60374891e-07 7.60941638e-07]",
    "crumbs": [
      "Propagation"
    ]
  }
]