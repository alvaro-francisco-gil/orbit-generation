{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Scripts to build the different datasets used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "\n",
    "from orbit_generation.processing import pad_and_convert_to_3d, segment_and_convert_to_3d, add_time_vector_to_orbits, resample_3d_array\n",
    "from orbit_generation.constants import EXTENDED_ORBIT_CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_orbit_data_from_hdf5(file_path: str                   # Path to the HDF5 file.\n",
    "                            ) -> Tuple[Dict[int, np.ndarray], # Dictionary of orbits with numerical keys.\n",
    "                                    pd.DataFrame,             # DataFrame containing orbit features.\n",
    "                                    Dict[str, float]]:        # Dictionary containing system features.\n",
    "    \"\"\"\n",
    "    Load orbit data from an HDF5 file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Check if not_propagated_orbits is a scalar\n",
    "        if file['not_propagated_orbits'].shape == ():\n",
    "            # Handle scalar case\n",
    "            not_propagated_orbits = []\n",
    "        else:\n",
    "            # Extract not_propagated_orbits and store in a list of integers\n",
    "            not_propagated_orbits = [index - 1 for index in file['not_propagated_orbits'][0].tolist()]\n",
    "        \n",
    "        # Extract system features and labels\n",
    "        system_features = file['system_features'][:]\n",
    "        system_labels = file['system_labels'][:].astype(str)\n",
    "        \n",
    "        # Create a dictionary for system\n",
    "        system_dict = {label: feature[0] for label, feature in zip(system_labels.flatten().tolist(), system_features)}\n",
    "        \n",
    "        # Extract orbit features and labels\n",
    "        orbit_features = file['orbit_features'][:]\n",
    "        orbit_labels = file['orbit_labels'][:].astype(str)\n",
    "        \n",
    "        # Create a dataframe for orbits\n",
    "        orbit_df = pd.DataFrame(orbit_features.T, columns=orbit_labels.flatten().tolist())\n",
    "        \n",
    "        # Remove rows in orbit_df based on not_propagated_orbits\n",
    "        if not_propagated_orbits:\n",
    "            orbit_df = orbit_df.drop(not_propagated_orbits).reset_index(drop=True)\n",
    "        \n",
    "        # Extract numpy arrays with numerical keys\n",
    "        orbits = {int(key): file[key][:] for key in file.keys() if key.isdigit()}\n",
    "        \n",
    "        # Reset the index of the dictionary to start on 0\n",
    "        orbits = {i: orbits[key] for i, key in enumerate(sorted(orbits.keys()))}\n",
    "                \n",
    "    return orbits, orbit_df, system_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orbit Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_orbit_features_from_hdf5(file_path: str          # Path to the HDF5 file.\n",
    "                                ) -> pd.DataFrame:       # DataFrame containing orbit features.\n",
    "    \"\"\"\n",
    "    Load orbit DataFrame from an HDF5 file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Check if not_propagated_orbits is a scalar\n",
    "        if file['not_propagated_orbits'].shape == ():\n",
    "            # Handle scalar case\n",
    "            not_propagated_orbits = []\n",
    "        else:\n",
    "            # Extract not_propagated_orbits and store in a list of integers\n",
    "            not_propagated_orbits = [index - 1 for index in file['not_propagated_orbits'][0].tolist()]\n",
    "        \n",
    "        # Extract orbit features and labels\n",
    "        orbit_features = file['orbit_features'][:]\n",
    "        orbit_labels = file['orbit_labels'][:].astype(str)\n",
    "        \n",
    "        # Create a dataframe for orbits\n",
    "        orbit_df = pd.DataFrame(orbit_features.T, columns=orbit_labels.flatten().tolist())\n",
    "        \n",
    "        # Remove rows in orbit_df based on not_propagated_orbits\n",
    "        if not_propagated_orbits:\n",
    "            orbit_df = orbit_df.drop(not_propagated_orbits).reset_index(drop=True)\n",
    "                \n",
    "    return orbit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_orbit_features_from_folder(folder_path: str    # Path to the folder\n",
    "                                  ) -> pd.DataFrame:   # DataFrame containing concatenated orbit features.\n",
    "    \"\"\"\n",
    "    Concatenate orbit DataFrames from all HDF5 files in a folder, preserving original index and adding system column.\n",
    "    \"\"\"\n",
    "    all_dfs = []  # List to store individual DataFrames\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Check if the file is an HDF5 file\n",
    "        if file_name.endswith('.h5') or file_name.endswith('.hdf5'):\n",
    "            # Get the orbit DataFrame from the HDF5 file\n",
    "            orbit_df = get_orbit_features_from_hdf5(file_path)\n",
    "            \n",
    "            # Preserve the original index as a new column\n",
    "            orbit_df['original_index'] = orbit_df.index\n",
    "            \n",
    "            # Add a new column called 'system' with the name of the file (without extension)\n",
    "            orbit_df['system'] = os.path.splitext(file_name)[0].split('_')[0]\n",
    "            \n",
    "            # Append the DataFrame to the list\n",
    "            all_dfs.append(orbit_df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    concatenated_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_data_from_hdf5(file_path: str              # Path to the HDF5 file.\n",
    "                             ) -> Dict[str, float]:       # Dictionary containing system features.\n",
    "    \"\"\"\n",
    "    Load system data from an HDF5 file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Extract system features and labels\n",
    "        system_features = file['system_features'][:]\n",
    "        system_labels = file['system_labels'][:].astype(str)\n",
    "        \n",
    "        # Create a dictionary for system\n",
    "        system_dict = {label: feature[0] for label, feature in zip(system_labels.flatten().tolist(), system_features)}\n",
    "        \n",
    "    return system_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_features_from_folder(folder_path: str    # Path to the folder\n",
    "                                   ) -> pd.DataFrame:   # DataFrame containing concatenated system features.\n",
    "    \"\"\"\n",
    "    Concatenate system DataFrames from all HDF5 files in a folder, preserving original index and adding system column.\n",
    "    \"\"\"\n",
    "    all_systems = []  # List to store individual system dictionaries\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Check if the file is an HDF5 file\n",
    "        if file_name.endswith('.h5') or file_name.endswith('.hdf5'):\n",
    "            # Get the system dictionary from the HDF5 file\n",
    "            system_dict = get_system_data_from_hdf5(file_path)\n",
    "            \n",
    "            # Add a new entry to the dictionary for the system name\n",
    "            system_dict['system'] = os.path.splitext(file_name)[0].split('_')[0]\n",
    "            \n",
    "            # Append the dictionary to the list\n",
    "            all_systems.append(system_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    concatenated_df = pd.DataFrame(all_systems)\n",
    "    \n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def substitute_values_from_df(values: List[Any],         # List of values to be substituted.\n",
    "                              df: pd.DataFrame,          # DataFrame containing the mapping.\n",
    "                              goal_column: str,          # Column in the DataFrame to get the substitution values from.\n",
    "                              id_column: str = 'Id'      # Column in the DataFrame to match the values with. Default is 'Id'.\n",
    "                             ) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Substitute values in the given list based on the mapping from a DataFrame's id column to goal column.\n",
    "\n",
    "    Parameters:\n",
    "    values (List[Any]): List of values to be substituted.\n",
    "    df (pd.DataFrame): DataFrame containing the mapping from id_column to goal_column.\n",
    "    goal_column (str): Column in the DataFrame to get the substitution values from.\n",
    "    id_column (str, optional): Column in the DataFrame to match the values with. Default is 'Id'.\n",
    "\n",
    "    Returns:\n",
    "    List[Any]: A list with substituted values from the DataFrame's goal_column.\n",
    "    \"\"\"\n",
    "    # Create a dictionary for substitution from the DataFrame\n",
    "    substitution_dict = df.set_index(id_column)[goal_column].to_dict()\n",
    "\n",
    "    # Substitute the values in the list using the dictionary\n",
    "    substituted_values = [substitution_dict.get(value, value) for value in values]\n",
    "\n",
    "    return substituted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_orbit_classes(values: List[Any]) -> Tuple[List[Any], List[Any], List[Any], List[Any]]:\n",
    "    \"\"\"\n",
    "    Get orbit classes based on the given values and DataFrame. Returns four lists corresponding\n",
    "    to 'Label', 'Type', 'Subtype', and 'Direction' columns.\n",
    "\n",
    "    Parameters:\n",
    "    values (List[Any]): List of values to be substituted.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[Any], List[Any], List[Any], List[Any]]: Four lists with substituted values from 'Label', 'Type', 'Subtype', and 'Direction' columns.\n",
    "    \"\"\"\n",
    "    labels = substitute_values_from_df(values, EXTENDED_ORBIT_CLASSIFICATION, 'Label')\n",
    "    types = substitute_values_from_df(values, EXTENDED_ORBIT_CLASSIFICATION, 'Type')\n",
    "    subtypes = substitute_values_from_df(values, EXTENDED_ORBIT_CLASSIFICATION, 'Subtype')\n",
    "    directions = substitute_values_from_df(values, EXTENDED_ORBIT_CLASSIFICATION, 'Direction')\n",
    "\n",
    "    return labels, types, subtypes, directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['S_BN', 'S_L1_A', 'S_L4_LP'],\n",
       " ['System-wide', 'L1', 'L4'],\n",
       " ['Butterfly', 'Axial', 'Long Period'],\n",
       " ['North', 'No specification', 'No specification'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [1,7,23]\n",
    "get_orbit_classes(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_periods_of_orbit_dict(orbits: Dict[int, np.ndarray],         # Dictionary of orbits with numerical keys.\n",
    "                              propagated_periods: Dict[int, int],    # Dictionary of propagated periods for each orbit.\n",
    "                              desired_periods: int                   # Desired number of periods.\n",
    "                             ) -> Dict[int, np.ndarray]:             # Processed dictionary of orbits.\n",
    "    \"\"\"\n",
    "    Process the orbits to extract the desired periods and print the percentage of the dataset returned.\n",
    "    \"\"\"\n",
    "    processed_orbits = {}\n",
    "    total_length_before = 0\n",
    "    total_length_after = 0\n",
    "    \n",
    "    for key, orbit in orbits.items():\n",
    "        total_length_before += orbit.shape[1]\n",
    "        if key in propagated_periods:\n",
    "            num_propagated = propagated_periods[key]\n",
    "            if num_propagated >= desired_periods:\n",
    "                # Calculate the length to take\n",
    "                length_per_period = orbit.shape[1] // num_propagated\n",
    "                length_to_take = length_per_period * desired_periods\n",
    "                # Take the desired periods from the beginning\n",
    "                processed_orbits[key] = orbit[:, :int(length_to_take) + 1]\n",
    "                total_length_after += length_to_take + 1\n",
    "            else:\n",
    "                # Raise an error if the number of propagated periods is less than desired\n",
    "                raise ValueError(f\"The number of propagated periods ({num_propagated}) for orbit {key} is less than the desired periods ({desired_periods}).\")\n",
    "        else:\n",
    "            # Raise an error if the key is not in propagated_periods\n",
    "            raise KeyError(f\"Key {key} is not found in propagated_periods.\")\n",
    "    \n",
    "    # Calculate and print the percentage of the dataset returned\n",
    "    percentage_returned = (total_length_after / total_length_before) * 100\n",
    "    print(f\"Percentage of the dataset returned: {percentage_returned:.2f}%\")\n",
    "    \n",
    "    return processed_orbits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_first_period_of_fixed_period_dataset(file_path: str              # Path to the HDF5 file.\n",
    "                                            ) -> Tuple[np.ndarray,       # 3D numpy array of padded orbits.\n",
    "                                                    pd.DataFrame,        # DataFrame containing orbit features.\n",
    "                                                    Dict[str, float]]:   # Dictionary containing system features.\n",
    "    \"\"\"\n",
    "    Load and process orbit data from an HDF5 file for the first period.\n",
    "    \"\"\"\n",
    "    # Load the orbit data, features dataframe, and system dictionary from the HDF5 file\n",
    "    orbits, orbit_df, system_dict = get_orbit_data_from_hdf5(file_path)\n",
    "\n",
    "    # Extract propagated periods and periods from the DataFrame\n",
    "    propagated_periods = orbit_df['propagated_periods'].tolist()\n",
    "    periods = orbit_df['period'].tolist()\n",
    "\n",
    "    # Remove the file type and extract parts of the file name to determine processing steps\n",
    "    file_name = os.path.basename(file_path).split('.')[0]\n",
    "    file_parts = file_name.split('_')\n",
    "\n",
    "    # Check if the second part of the file name is 'N'\n",
    "    if file_parts[1] == 'N':\n",
    "        # Add time vectors to the orbits\n",
    "        orbits = add_time_vector_to_orbits(orbits, propagated_periods, periods)\n",
    "        # Pad and convert the orbits to a 3D array using the fourth part of the file name as timesteps\n",
    "        orbits = pad_and_convert_to_3d(orbits, int(file_parts[3]))\n",
    "\n",
    "    return orbits, orbit_df, system_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_full_fixed_step_dataset(file_path: str,                   # Path to the HDF5 file.\n",
    "                                segment_length: int               # Desired length of each segment.\n",
    "                                ) -> Tuple[np.ndarray,            # 3D numpy array of segmented orbits.\n",
    "                                        pd.DataFrame,             # DataFrame containing orbit features.\n",
    "                                        np.ndarray,               # NumPy array of IDs representing each new segment.\n",
    "                                        Dict[str, float]]:        # Dictionary containing system features.\n",
    "    \"\"\"\n",
    "    Load and process orbit data from an HDF5 file, segmenting each orbit into specified length.\n",
    "    \"\"\"\n",
    "    # Load the orbit data, features dataframe, and system dictionary from the HDF5 file\n",
    "    orbits, orbit_df, system_dict = get_orbit_data_from_hdf5(file_path)\n",
    "\n",
    "    # Check if the second part of the file name is 'dt'\n",
    "    if os.path.basename(file_path).split('_')[1] == 'dt':\n",
    "        # Segment the orbits and get the corresponding segment IDs\n",
    "        orbits, orbits_ids = segment_and_convert_to_3d(orbits, segment_length)\n",
    "        orbits_ids = np.array(orbits_ids)  # Convert IDs to a NumPy array\n",
    "\n",
    "    return orbits, orbit_df, orbits_ids, system_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_first_period_fixed_step_dataset(file_path: str,                  # Path to the HDF5 file.\n",
    "                                        segment_length: int              # Desired length of each segment.\n",
    "                                       ) -> Tuple[np.ndarray,            # 3D numpy array of segmented orbits.\n",
    "                                               pd.DataFrame,             # DataFrame containing orbit features.\n",
    "                                               np.ndarray,               # NumPy array of IDs representing each new segment.\n",
    "                                               Dict[str, float]]:        # Dictionary containing system features.\n",
    "    \"\"\"\n",
    "    Load and process orbit data from an HDF5 file, segmenting each orbit into specified length.\n",
    "    \"\"\"\n",
    "    # Load the orbit data, features dataframe, and system dictionary from the HDF5 file\n",
    "    orbits, orbit_df, system_dict = get_orbit_data_from_hdf5(file_path)\n",
    "\n",
    "    # Get the propagated periods from the orbit_df\n",
    "    propagated_periods = orbit_df['propagated_periods'].to_dict()\n",
    "\n",
    "    # Process orbits to extract the first desired period\n",
    "    orbits = get_periods_of_orbit_dict(orbits, propagated_periods, desired_periods=1)\n",
    "    \n",
    "    # Check if the second part of the file name is 'dt'\n",
    "    if os.path.basename(file_path).split('_')[1] == 'dt':\n",
    "        # Segment the orbits and get the corresponding segment IDs\n",
    "        orbits, orbits_ids = segment_and_convert_to_3d(orbits, segment_length)\n",
    "        orbits_ids = np.array(orbits_ids)  # Convert IDs to a NumPy array\n",
    "\n",
    "    return orbits, orbit_df, orbits_ids, system_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_first_period_dataset(file_path: str,                             # Path to the HDF5 file.\n",
    "                             segment_length: Optional[int] = 100        # Desired length of each segment, optional.\n",
    "                            ) -> Tuple[np.ndarray,                      # 3D numpy array of orbits.\n",
    "                                    pd.DataFrame,                       # DataFrame containing orbit features.\n",
    "                                    np.ndarray,                         # NumPy array of IDs representing each new segment.\n",
    "                                    Dict[str, float]]:                  # Dictionary containing system features.\n",
    "    \"\"\"\n",
    "    Load orbit data based on the file path. Calls the appropriate function\n",
    "    depending on the name of the file.\n",
    "    \"\"\"\n",
    "    # Extract the base name and split by underscores to analyze the parts\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_parts = file_name.split('_')\n",
    "\n",
    "    # Check if the second part of the file name is 'dt'\n",
    "    if file_parts[1] == 'dt':\n",
    "        # If 'dt' is present and segment_length is provided\n",
    "        if segment_length is not None:\n",
    "            orbits, orbit_df, orbits_ids, system_dict = get_first_period_fixed_step_dataset(file_path, segment_length)\n",
    "            return orbits, orbit_df, orbits_ids, system_dict\n",
    "        else:\n",
    "            raise ValueError(\"Segment length must be provided for 'dt' files.\")\n",
    "    elif file_parts[1] == 'N':\n",
    "        # If 'N' is present\n",
    "        orbits, orbit_df, system_dict = get_first_period_of_fixed_period_dataset(file_path)\n",
    "        orbits_ids = np.arange(0, orbits.shape[0])  # Create an array of orbit IDs from 1 to the number of orbits\n",
    "        # Resample the orbits\n",
    "        if segment_length is not None:\n",
    "            orbits = resample_3d_array(data=orbits, axis=2, target_size=segment_length)\n",
    "        return orbits, orbit_df, orbits_ids, system_dict\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. File name must contain either 'dt' or 'N'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
