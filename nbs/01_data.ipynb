{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Utils\n",
    "\n",
    "> Necessary scripts to read orbits from different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional, Any, Union, List, Dict, Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from orbit_generation.path import get_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from unittest.mock import patch, MagicMock\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_orbit_data(file_path: str,  # The path to the .mat, .h5, or .npy file.\n",
    "                    variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.\n",
    "                    dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.\n",
    "                   ) -> Any:  # The loaded orbit data.\n",
    "    \"\"\"\n",
    "    Load orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.mat'):\n",
    "        if variable_name is None:\n",
    "            raise ValueError(\"variable_name must be provided for .mat files\")\n",
    "        mat = loadmat(file_path)\n",
    "        if variable_name in mat:\n",
    "            data = mat[variable_name]\n",
    "        else:\n",
    "            raise ValueError(f\"{variable_name} not found in {file_path}\")\n",
    "\n",
    "    elif file_path.endswith('.h5'):\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            if dataset_path is None:\n",
    "                raise ValueError(\"dataset_path must be provided for .h5 files\")\n",
    "            if dataset_path in file:\n",
    "                data = np.array(file[dataset_path])\n",
    "            else:\n",
    "                raise ValueError(f\"{dataset_path} not found in {file_path}\")\n",
    "\n",
    "    elif file_path.endswith('.npy'):\n",
    "        data = np.load(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .mat, .h5, or .npy file.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test load_orbit_data\n",
    "#| hide\n",
    "mock_mat_data = {'Xarray': np.array([1, 2, 3])}\n",
    "mock_h5_data = np.array([4, 5, 6])\n",
    "mock_npy_data = np.array([7, 8, 9])\n",
    "\n",
    "# Test for load_orbit_data with .mat file\n",
    "with patch('__main__.loadmat', return_value=mock_mat_data) as mock_loadmat:\n",
    "    result = load_orbit_data('test_data.mat', variable_name='Xarray')\n",
    "    assert (result == mock_mat_data['Xarray']).all(), \"MAT file loading failed or data mismatch\"\n",
    "    mock_loadmat.assert_called_once_with('test_data.mat')\n",
    "\n",
    "# Test for load_orbit_data with .h5 file\n",
    "with patch('__main__.h5py.File') as mock_h5py:\n",
    "    mock_file = MagicMock()\n",
    "    mock_file.__enter__.return_value = {'/files/PERIODIC ORBITS': mock_h5_data}\n",
    "    mock_h5py.return_value = mock_file\n",
    "    result = load_orbit_data('test_data.h5', dataset_path='/files/PERIODIC ORBITS')\n",
    "    assert (result == mock_h5_data).all(), \"H5 file loading failed or data mismatch\"\n",
    "\n",
    "# Test for load_orbit_data with .npy file\n",
    "with patch('numpy.load', return_value=mock_npy_data) as mock_load:\n",
    "    result = load_orbit_data('test_data.npy')\n",
    "    assert (result == mock_npy_data).all(), \"NPY file loading failed or data mismatch\"\n",
    "    mock_load.assert_called_once_with('test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_memmap_array(file_path: str,  # The path to the .npy file as a string.\n",
    "                      mode: str = 'c'  # Mode for memory-mapping ('r', 'r+', 'w+', 'c').\n",
    "                     ) -> np.memmap:   # Returns a memory-mapped array.\n",
    "    \"\"\"\n",
    "    Load a .npy file as a memory-mapped array using numpy.memmap.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists at the specified path\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No file found at the specified path: {file_path}\")\n",
    "    \n",
    "    # Load the .npy file as a memmap object with the specified mode\n",
    "    return np.load(file_path, mmap_mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_orbit_features(file_path: str,  # The path to the file (can be .mat, .h5, or .npy).\n",
    "                       variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.\n",
    "                       dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.\n",
    "                      ) -> pd.DataFrame:  # DataFrame with detailed orbit features.\n",
    "    \"\"\"\n",
    "    Load orbit feature data from a specified file and convert it to a DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data using the previously defined function that supports .mat, .h5, and .npy files\n",
    "    orbit_data = load_orbit_data(file_path, variable_name=variable_name, dataset_path=dataset_path)\n",
    "    \n",
    "    # Define column labels for the DataFrame\n",
    "    column_labels = [\n",
    "        'Orbit Family', 'Initial Position X', 'Initial Position Y', 'Initial Position Z',\n",
    "        'Initial Velocity X', 'Initial Velocity Y', 'Initial Velocity Z',\n",
    "        'Jacobi Constant', 'Period', 'Stability Index'\n",
    "    ]\n",
    "    \n",
    "    # Create a DataFrame from the loaded data\n",
    "    features = pd.DataFrame(orbit_data, columns=column_labels)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test get_orbit_features\n",
    "#| hide\n",
    "def test_get_orbit_features():\n",
    "    # Sample data simulating what might be returned by load_orbit_data\n",
    "    mock_data = np.array([\n",
    "        [1, 0, 0, 0, 1, 0, 0, 3.0, 2.0, 1.0],\n",
    "        [2, 1, 1, 1, 0, 1, 0, 2.5, 1.5, 0.5]\n",
    "    ])\n",
    "    \n",
    "    # Expected DataFrame structure\n",
    "    expected_columns = [\n",
    "        'Orbit Family', 'Initial Position X', 'Initial Position Y', 'Initial Position Z',\n",
    "        'Initial Velocity X', 'Initial Velocity Y', 'Initial Velocity Z',\n",
    "        'Jacobi Constant', 'Period', 'Stability Index'\n",
    "    ]\n",
    "    expected_df = pd.DataFrame(mock_data, columns=expected_columns)\n",
    "    \n",
    "    # Patch the load_orbit_data function to return mock_data\n",
    "    with patch('__main__.load_orbit_data', return_value=mock_data) as mock_load_orbit_data:\n",
    "        # Test for .mat file\n",
    "        result_df = get_orbit_features('dummy_path.mat', variable_name='dummy_var')\n",
    "        test_eq(result_df.equals(expected_df), True)\n",
    "        \n",
    "        # Ensure the mock was called correctly\n",
    "        mock_load_orbit_data.assert_called_once_with('dummy_path.mat', variable_name='dummy_var', dataset_path=None)\n",
    "\n",
    "        # Test for .h5 file with dataset_path\n",
    "        mock_load_orbit_data.reset_mock()\n",
    "        result_df = get_orbit_features('dummy_path.h5', dataset_path='dummy_dataset')\n",
    "        test_eq(result_df.equals(expected_df), True)\n",
    "        \n",
    "        # Ensure the mock was called correctly\n",
    "        mock_load_orbit_data.assert_called_once_with('dummy_path.h5', variable_name=None, dataset_path='dummy_dataset')\n",
    "\n",
    "        # Test for .npy file\n",
    "        mock_load_orbit_data.reset_mock()\n",
    "        result_df = get_orbit_features('dummy_path.npy')\n",
    "        test_eq(result_df.equals(expected_df), True)\n",
    "        \n",
    "        # Ensure the mock was called correctly\n",
    "        mock_load_orbit_data.assert_called_once_with('dummy_path.npy', variable_name=None, dataset_path=None)\n",
    "\n",
    "# Call the test function to execute tests\n",
    "test_get_orbit_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_data(data: np.ndarray,  # The numpy array data to save.\n",
    "              file_name: str  # The name of the file to save the data in, including the extension.\n",
    "             ) -> None:\n",
    "    \"\"\"\n",
    "    Save a numpy array to a file based on the file extension specified in `file_name`.\n",
    "    Supports saving to HDF5 (.hdf5) or NumPy (.npy) file formats.\n",
    "    \"\"\"\n",
    "    # Extract file extension from file name\n",
    "    _, file_extension = os.path.splitext(file_name)\n",
    "    \n",
    "    if file_extension == '.hdf5':\n",
    "        # Open a new HDF5 file\n",
    "        with h5py.File(file_name, 'w') as f:\n",
    "            # Create a dataset in the file\n",
    "            f.create_dataset('data', data=data, compression='gzip', compression_opts=9)\n",
    "    elif file_extension == '.npy':\n",
    "        # Save the array to a NumPy .npy file\n",
    "        np.save(file_name, data)\n",
    "    else:\n",
    "        # Raise an error for unsupported file types\n",
    "        raise ValueError(\"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test save_data\n",
    "#| hide\n",
    "# Test for NPY saving functionality\n",
    "# Test for NPY saving functionality\n",
    "def test_save_data_npy():\n",
    "    data = np.random.rand(5, 5)\n",
    "    file_name = 'test_data.npy'\n",
    "    \n",
    "    with patch('numpy.save', autospec=True) as mock_save:\n",
    "        save_data(data, file_name)\n",
    "        mock_save.assert_called_once_with(file_name, data)\n",
    "\n",
    "# Test for HDF5 saving functionality\n",
    "def test_save_data_hdf5():\n",
    "    data = np.random.rand(5, 5)\n",
    "    file_name = 'test_data.hdf5'\n",
    "    \n",
    "    with patch('h5py.File', autospec=True) as mock_file:\n",
    "        save_data(data, file_name)\n",
    "        mock_file.assert_called_once_with(file_name, 'w')\n",
    "\n",
    "# Test for handling invalid file type\n",
    "def test_save_data_invalid_type():\n",
    "    data = np.random.rand(5, 5)\n",
    "    file_name = 'test_data.unknown'\n",
    "    \n",
    "    try:\n",
    "        save_data(data, file_name)\n",
    "        assert False, \"ValueError expected but not raised\"\n",
    "    except ValueError as e:\n",
    "        assert str(e) == \"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\", \"Incorrect error message\"\n",
    "\n",
    "test_save_data_invalid_type()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_example_orbit_data(\n",
    "    ) -> np.ndarray:  # Return type annotation added\n",
    "    \"\"\"\n",
    "    Load example orbit data from a numpy file located in the example_data directory.\n",
    "    \"\"\"\n",
    "    # Construct path to example data file\n",
    "    data_path = get_data_path() / \"example_training_data\" / \"example_orbits.npy\"\n",
    "    \n",
    "    # Convert Path to string before passing to load_orbit_data\n",
    "    data = load_orbit_data(str(data_path))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 7, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | test\n",
    "data = get_example_orbit_data()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order labels and array given target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def order_labels_and_array_with_target(\n",
    "    labels: np.ndarray,  # Array of labels to be ordered\n",
    "    array: np.ndarray,  # Array to be ordered according to labels\n",
    "    target_label: str,  # Label to order by\n",
    "    place_at_end: bool = False,  # Whether to place target label at end\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:  # Returns ordered labels and array\n",
    "    \"\"\"\n",
    "    Orders labels and array by placing entries with target_label either at start or end.\n",
    "    \"\"\"\n",
    "    # Convert labels to a numpy array if it's not already\n",
    "    labels = np.array(labels)\n",
    "    n = len(labels)\n",
    "    \n",
    "    # Create index arrays to sort based on target label\n",
    "    primary_indices = [i for i in range(n) if labels[i] == target_label]\n",
    "    secondary_indices = [i for i in range(n) if labels[i] != target_label]\n",
    "\n",
    "    # If place_at_end is True, reorder the indices\n",
    "    if place_at_end:\n",
    "        combined_indices = secondary_indices + primary_indices\n",
    "    else:\n",
    "        combined_indices = primary_indices + secondary_indices\n",
    "    \n",
    "    # Use indices to sort labels and array\n",
    "    ordered_labels = labels[combined_indices]\n",
    "    ordered_array = array[combined_indices]\n",
    "    \n",
    "    return ordered_labels, ordered_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple' 'apple' 'banana' 'orange' 'banana' 'grape']\n",
      "[[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]]\n",
      "\n",
      " [[21 22]\n",
      "  [23 24]]]\n"
     ]
    }
   ],
   "source": [
    "# Sample labels and a sample 3D array\n",
    "labels = np.array(['apple', 'banana', 'apple', 'orange', 'banana', 'grape'])\n",
    "array = np.array([[[1, 2], [3, 4]], \n",
    "                  [[5, 6], [7, 8]], \n",
    "                  [[9, 10], [11, 12]], \n",
    "                  [[13, 14], [15, 16]], \n",
    "                  [[17, 18], [19, 20]], \n",
    "                  [[21, 22], [23, 24]]])\n",
    "target_label = 'apple'\n",
    "\n",
    "ordered_labels, ordered_array = order_labels_and_array_with_target(labels, array, target_label)\n",
    "\n",
    "print(ordered_labels)\n",
    "print(ordered_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_orbits(orbit_data: np.ndarray,  # Array of orbit data with shape (num_orbits, 6, num_time_points)\n",
    "                  sample_spec: Union[dict, int],  # Number of samples per class (dict) or total samples (int)\n",
    "                  labels: Optional[np.ndarray] = None,  # Array of labels for each orbit\n",
    "                  ) -> tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Randomly sample orbits from the provided dataset.\n",
    "    \"\"\"\n",
    "    if labels is not None and isinstance(sample_spec, dict):\n",
    "        # Sampling specified number of orbits for each class\n",
    "        indices = []\n",
    "        for label, count in sample_spec.items():\n",
    "            class_indices = np.where(labels == label)[0]\n",
    "            if len(class_indices) < count:\n",
    "                raise ValueError(f\"Not enough samples for class {label}. Requested {count}, available {len(class_indices)}.\")\n",
    "            selected_indices = np.random.choice(class_indices, size=count, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        indices = np.array(indices)\n",
    "    else:\n",
    "        # Random sampling without considering classes\n",
    "        indices = np.random.choice(orbit_data.shape[0], size=sample_spec, replace=False)\n",
    "    \n",
    "    # Select the sampled data and labels\n",
    "    sampled_data = orbit_data[indices]\n",
    "    sampled_labels = labels[indices] if labels is not None else None\n",
    "    \n",
    "    return sampled_data, sampled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| test sample_orbits_random_sampling_without_labels\n",
    "def test_sample_orbits_random_sampling_without_labels():\n",
    "    # Setup a mock dataset\n",
    "    orbit_data = np.random.rand(100, 6, 10)  # 100 orbits, 6 values, 10 time points\n",
    "    \n",
    "    # Perform random sampling without labels\n",
    "    sampled_data, sampled_labels = sample_orbits(orbit_data, 10)\n",
    "    \n",
    "    # Test outcomes\n",
    "    assert sampled_data.shape == (10, 6, 10), \"Shape of sampled data should match the requested sample size\"\n",
    "    assert sampled_labels is None, \"Labels should be None when not provided\"\n",
    "\n",
    "#| test sample_orbits_class_specific_sampling\n",
    "def test_sample_orbits_class_specific_sampling():\n",
    "    # Setup a mock dataset and labels\n",
    "    orbit_data = np.random.rand(100, 6, 10)\n",
    "    labels = np.random.randint(0, 3, size=100)  # 100 labels in 3 classes\n",
    "    sample_spec = {0: 5, 1: 5}\n",
    "    \n",
    "    # Perform class-specific sampling\n",
    "    sampled_data, sampled_labels = sample_orbits(orbit_data, sample_spec, labels)\n",
    "    \n",
    "    # Test outcomes\n",
    "    assert sampled_data.shape == (10, 6, 10), \"Shape of sampled data should match the total requested sample size\"\n",
    "    assert len(sampled_labels) == 10, \"Number of labels should match the total requested sample size\"\n",
    "    assert all(label in sample_spec for label in sampled_labels), \"All labels should be from requested classes\"\n",
    "\n",
    "#| test sample_orbits_insufficient_class_samples\n",
    "def test_sample_orbits_insufficient_class_samples():\n",
    "    # Setup a mock dataset and labels\n",
    "    orbit_data = np.random.rand(100, 6, 10)\n",
    "    labels = np.random.randint(0, 1, size=100)  # 100 labels in 1 class only\n",
    "    sample_spec = {0: 50, 1: 50}  # Requesting 50 samples each from class 0 and 1\n",
    "    \n",
    "    # Perform class-specific sampling with expectation of failure\n",
    "    try:\n",
    "        sample_orbits(orbit_data, sample_spec, labels)\n",
    "        assert False, \"Expected ValueError due to insufficient samples for class 1\"\n",
    "    except ValueError as e:\n",
    "        assert str(e) == \"Not enough samples for class 1. Requested 50, available 0.\", \"Error message should indicate insufficient samples for class 1\"\n",
    "\n",
    "# Execute tests to verify the behavior\n",
    "test_sample_orbits_random_sampling_without_labels()\n",
    "test_sample_orbits_class_specific_sampling()\n",
    "test_sample_orbits_insufficient_class_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Discarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def discard_random_labels(data: np.ndarray,  # Dataset to filter\n",
    "                         labels: np.ndarray,  # Labels corresponding to the data\n",
    "                         discard_labels: Union[List, Dict, int],  # Labels to discard - list, dict or number\n",
    "                         ) -> Tuple[List, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Discards random or specified labels from the dataset.\n",
    "    \n",
    "    Returns tuple of (discarded labels, filtered data, filtered labels).\n",
    "    \"\"\"\n",
    "    # Handle empty dictionary or empty list\n",
    "    if isinstance(discard_labels, dict) and not discard_labels:\n",
    "        return [], data, labels  # Return everything as is if dictionary is empty\n",
    "    elif isinstance(discard_labels, list) and not discard_labels:\n",
    "        return [], data, labels  # Return everything as is if list is empty\n",
    "    \n",
    "    # Check if discard_labels is a list\n",
    "    if isinstance(discard_labels, list):\n",
    "        # Use the provided list of labels to discard\n",
    "        discarded = np.array(discard_labels)\n",
    "    elif isinstance(discard_labels, dict):\n",
    "        # If it's a dictionary, use its keys as labels to discard\n",
    "        discarded = np.array(list(discard_labels.keys()))\n",
    "    else:\n",
    "        # Get unique labels\n",
    "        unique_labels = np.unique(labels)\n",
    "        # Randomly select labels to discard\n",
    "        discarded = np.random.choice(unique_labels, size=discard_labels, replace=False)\n",
    "    \n",
    "    # Create a mask for samples that are not discarded\n",
    "    mask = ~np.isin(labels, discarded)\n",
    "    \n",
    "    # Return the discarded labels and the filtered dataset\n",
    "    return discarded.tolist(), data[mask], labels[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates preserve Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_duplicates_preserve_order(input_list: List,  # Input list that may contain duplicates\n",
    "                                   ) -> List:  # Returns list with duplicates removed while preserving order\n",
    "    \"\"\"\n",
    "    Removes duplicate items from a list while preserving the original order.\n",
    "    \"\"\"\n",
    "    unique_items = []\n",
    "    seen = set()\n",
    "    for item in input_list:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            unique_items.append(item)\n",
    "    return unique_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_dataloaders(scaled_data: torch.Tensor,  # Input tensor of scaled data\n",
    "                      val_split: float = 0.2,  # Fraction of data to use for validation\n",
    "                      batch_size: int = 32,  # Batch size for dataloaders\n",
    "                      ) -> Tuple[DataLoader, Optional[DataLoader]]:  # Returns train and optional val dataloaders\n",
    "    \"\"\"\n",
    "    Creates train and validation dataloaders from input tensor data.\n",
    "    \"\"\"\n",
    "    if val_split > 0:\n",
    "        X_train, X_val = train_test_split(\n",
    "            scaled_data,\n",
    "            test_size=val_split,\n",
    "            shuffle=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        train_dataset = TensorDataset(X_train)\n",
    "        val_dataset = TensorDataset(X_val)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    else:\n",
    "        train_dataset = TensorDataset(scaled_data)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "        val_dataloader = None\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EPS = 1e-18  # A small epsilon to prevent division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSFeatureWiseScaler():\n",
    "    \"\"\"\n",
    "    Scales time series data feature-wise using PyTorch tensors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_range : tuple(float, float), optional\n",
    "        Tuple representing the minimum and maximum feature values (default is (0, 1)).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    _min_v : float\n",
    "        Minimum feature value.\n",
    "    _max_v : float\n",
    "        Maximum feature value.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_range: tuple = (0, 1)) -> None:\n",
    "        assert len(feature_range) == 2\n",
    "        self._min_v, self._max_v = feature_range\n",
    "\n",
    "    def fit(self, X: torch.Tensor) -> \"TSFeatureWiseScaler\":\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data.\n",
    "        \n",
    "        :param X: Input data. Shape: (N, F, T) where N is number of data points,\n",
    "                  F is number of features, and T is number of time steps.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: The fitted scaler object.\n",
    "        :rtype: TSFeatureWiseScaler\n",
    "        \"\"\"\n",
    "        F = X.shape[1]\n",
    "        self.mins = torch.zeros(F, device=X.device)\n",
    "        self.maxs = torch.zeros(F, device=X.device)\n",
    "\n",
    "        for i in range(F):\n",
    "            self.mins[i] = torch.min(X[:, i, :])\n",
    "            self.maxs[i] = torch.max(X[:, i, :])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transforms the data.\n",
    "        \n",
    "        :param X: Input data. Shape: (N, F, T)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        X_scaled = X.clone()\n",
    "        for i in range(X.shape[1]):\n",
    "            X_scaled[:, i, :] = ((X[:, i, :] - self.mins[i]) / (self.maxs[i] - self.mins[i] + 1e-8)) * (self._max_v - self._min_v) + self._min_v\n",
    "        return X_scaled\n",
    "\n",
    "    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inverse-transforms the data.\n",
    "        \n",
    "        :param X: Scaled data. Shape: (N, F, T)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Original data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        X_inv = X.clone()\n",
    "        for i in range(X.shape[1]):\n",
    "            X_inv[:, i, :] = (X[:, i, :] - self._min_v) / (self._max_v - self._min_v)\n",
    "            X_inv[:, i, :] = X_inv[:, i, :] * (self.maxs[i] - self.mins[i] + 1e-8) + self.mins[i]\n",
    "        return X_inv\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data and transforms it.\n",
    "        \n",
    "        :param X: Input data. Shape: (N, F, T)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSGlobalScaler():\n",
    "    \"\"\"\n",
    "    Scales time series data globally using PyTorch tensors.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    min : float\n",
    "        Minimum value encountered in the data.\n",
    "    max : float\n",
    "        Maximum value encountered in the data.\n",
    "    \"\"\"\n",
    "    def fit(self, X: torch.Tensor) -> \"TSGlobalScaler\":\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data.\n",
    "        \n",
    "        :param X: Input data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: The fitted scaler object.\n",
    "        :rtype: TSGlobalScaler\n",
    "        \"\"\"\n",
    "        self.min = torch.min(X)\n",
    "        self.max = torch.max(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transforms the data.\n",
    "        \n",
    "        :param X: Input data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled X.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        return (X - self.min) / (self.max - self.min + EPS)\n",
    "\n",
    "    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inverse-transforms the data.\n",
    "        \n",
    "        :param X: Scaled data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Original data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        X = X * (self.max - self.min + EPS)\n",
    "        X = X + self.min\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data and transforms it.\n",
    "        \n",
    "        :param X: Input data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled input data X.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
