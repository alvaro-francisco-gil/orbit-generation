{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Necessary scripts to read orbits from different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional, Any\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from unittest.mock import patch, MagicMock\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_orbit_data(file_path: str,  # The path to the .mat, .h5, or .npy file.\n",
    "                    variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.\n",
    "                    dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.\n",
    "                   ) -> Any:  # The loaded orbit data.\n",
    "    \"\"\"\n",
    "    Load orbit data from MATLAB .mat files, HDF5 .h5 files, or NumPy .npy files.\n",
    "    \"\"\"\n",
    "    if file_path.endswith('.mat'):\n",
    "        if variable_name is None:\n",
    "            raise ValueError(\"variable_name must be provided for .mat files\")\n",
    "        mat = loadmat(file_path)\n",
    "        if variable_name in mat:\n",
    "            data = mat[variable_name]\n",
    "        else:\n",
    "            raise ValueError(f\"{variable_name} not found in {file_path}\")\n",
    "\n",
    "    elif file_path.endswith('.h5'):\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            if dataset_path is None:\n",
    "                raise ValueError(\"dataset_path must be provided for .h5 files\")\n",
    "            if dataset_path in file:\n",
    "                data = np.array(file[dataset_path])\n",
    "            else:\n",
    "                raise ValueError(f\"{dataset_path} not found in {file_path}\")\n",
    "\n",
    "    elif file_path.endswith('.npy'):\n",
    "        data = np.load(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .mat, .h5, or .npy file.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test load_orbit_data\n",
    "#| hide\n",
    "mock_mat_data = {'Xarray': np.array([1, 2, 3])}\n",
    "mock_h5_data = np.array([4, 5, 6])\n",
    "mock_npy_data = np.array([7, 8, 9])\n",
    "\n",
    "# Test for load_orbit_data with .mat file\n",
    "with patch('__main__.loadmat', return_value=mock_mat_data) as mock_loadmat:\n",
    "    result = load_orbit_data('test_data.mat', variable_name='Xarray')\n",
    "    assert (result == mock_mat_data['Xarray']).all(), \"MAT file loading failed or data mismatch\"\n",
    "    mock_loadmat.assert_called_once_with('test_data.mat')\n",
    "\n",
    "# Test for load_orbit_data with .h5 file\n",
    "with patch('__main__.h5py.File') as mock_h5py:\n",
    "    mock_file = MagicMock()\n",
    "    mock_file.__enter__.return_value = {'/files/PERIODIC ORBITS': mock_h5_data}\n",
    "    mock_h5py.return_value = mock_file\n",
    "    result = load_orbit_data('test_data.h5', dataset_path='/files/PERIODIC ORBITS')\n",
    "    assert (result == mock_h5_data).all(), \"H5 file loading failed or data mismatch\"\n",
    "\n",
    "# Test for load_orbit_data with .npy file\n",
    "with patch('numpy.load', return_value=mock_npy_data) as mock_load:\n",
    "    result = load_orbit_data('test_data.npy')\n",
    "    assert (result == mock_npy_data).all(), \"NPY file loading failed or data mismatch\"\n",
    "    mock_load.assert_called_once_with('test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_memmap_array(file_path: str,  # The path to the .npy file as a string.\n",
    "                      mode: str = 'c'  # Mode for memory-mapping ('r', 'r+', 'w+', 'c').\n",
    "                     ) -> np.memmap:   # Returns a memory-mapped array.\n",
    "    \"\"\"\n",
    "    Load a .npy file as a memory-mapped array using numpy.memmap.\n",
    "    \n",
    "    Args:\n",
    "    file_path: A string representing the path to the .npy file.\n",
    "    mode: The mode in which the file is to be opened. Valid options are:\n",
    "          - 'r'  : Read-only, no data can be modified.\n",
    "          - 'r+' : Read/write, modifications to the data are written to the file.\n",
    "          - 'w+' : Read/write, file is created if it does not exist, overwritten if it does.\n",
    "          - 'c'  : Copy-on-write, data can be modified in memory but changes are not saved to the file.\n",
    "\n",
    "    Returns:\n",
    "    A numpy.memmap object that behaves like a numpy array but with data stored on disk instead of in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists at the specified path\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"No file found at the specified path: {file_path}\")\n",
    "    \n",
    "    # Load the .npy file as a memmap object with the specified mode\n",
    "    return np.load(file_path, mmap_mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_orbit_features(file_path: str,  # The path to the file (can be .mat, .h5, or .npy).\n",
    "                       variable_name: Optional[str] = None,  # Name of the variable in the .mat file, optional.\n",
    "                       dataset_path: Optional[str] = None  # Path to the dataset in the .h5 file, optional.\n",
    "                      ) -> pd.DataFrame:  # DataFrame with detailed orbit features.\n",
    "    \"\"\"\n",
    "    Load orbit feature data from a specified file and convert it to a DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data using the previously defined function that supports .mat, .h5, and .npy files\n",
    "    orbit_data = load_orbit_data(file_path, variable_name=variable_name, dataset_path=dataset_path)\n",
    "    \n",
    "    # Define column labels for the DataFrame\n",
    "    column_labels = [\n",
    "        'Orbit Family', 'Initial Position X', 'Initial Position Y', 'Initial Position Z',\n",
    "        'Initial Velocity X', 'Initial Velocity Y', 'Initial Velocity Z',\n",
    "        'Jacobi Constant', 'Period', 'Stability Index'\n",
    "    ]\n",
    "    \n",
    "    # Create a DataFrame from the loaded data\n",
    "    features = pd.DataFrame(orbit_data, columns=column_labels)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test get_orbit_features\n",
    "#| hide\n",
    "def test_get_orbit_features():\n",
    "    # Sample data simulating what might be returned by load_orbit_data\n",
    "    mock_data = np.array([\n",
    "        [1, 0, 0, 0, 1, 0, 0, 3.0, 2.0, 1.0],\n",
    "        [2, 1, 1, 1, 0, 1, 0, 2.5, 1.5, 0.5]\n",
    "    ])\n",
    "    \n",
    "    # Expected DataFrame structure\n",
    "    expected_columns = [\n",
    "        'Orbit Family', 'Initial Position X', 'Initial Position Y', 'Initial Position Z',\n",
    "        'Initial Velocity X', 'Initial Velocity Y', 'Initial Velocity Z',\n",
    "        'Jacobi Constant', 'Period', 'Stability Index'\n",
    "    ]\n",
    "    expected_df = pd.DataFrame(mock_data, columns=expected_columns)\n",
    "    \n",
    "    # Patch the load_orbit_data function to return mock_data\n",
    "    with patch('__main__.load_orbit_data', return_value=mock_data) as mock_load_orbit_data:\n",
    "        # Test for .mat file\n",
    "        result_df = get_orbit_features('dummy_path.mat', variable_name='dummy_var')\n",
    "        test_eq(result_df.equals(expected_df), True)\n",
    "        \n",
    "        # Ensure the mock was called correctly\n",
    "        mock_load_orbit_data.assert_called_once_with('dummy_path.mat', variable_name='dummy_var', dataset_path=None)\n",
    "\n",
    "        # Test for .h5 file with dataset_path\n",
    "        mock_load_orbit_data.reset_mock()\n",
    "        result_df = get_orbit_features('dummy_path.h5', dataset_path='dummy_dataset')\n",
    "        test_eq(result_df.equals(expected_df), True)\n",
    "        \n",
    "        # Ensure the mock was called correctly\n",
    "        mock_load_orbit_data.assert_called_once_with('dummy_path.h5', variable_name=None, dataset_path='dummy_dataset')\n",
    "\n",
    "        # Test for .npy file\n",
    "        mock_load_orbit_data.reset_mock()\n",
    "        result_df = get_orbit_features('dummy_path.npy')\n",
    "        test_eq(result_df.equals(expected_df), True)\n",
    "        \n",
    "        # Ensure the mock was called correctly\n",
    "        mock_load_orbit_data.assert_called_once_with('dummy_path.npy', variable_name=None, dataset_path=None)\n",
    "\n",
    "# Call the test function to execute tests\n",
    "test_get_orbit_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_data(data: np.ndarray,  # The numpy array data to save.\n",
    "              file_name: str  # The name of the file to save the data in, including the extension.\n",
    "             ) -> None:\n",
    "    \"\"\"\n",
    "    Save a numpy array to a file based on the file extension specified in `file_name`.\n",
    "    Supports saving to HDF5 (.hdf5) or NumPy (.npy) file formats.\n",
    "    \"\"\"\n",
    "    # Extract file extension from file name\n",
    "    _, file_extension = os.path.splitext(file_name)\n",
    "    \n",
    "    if file_extension == '.hdf5':\n",
    "        # Open a new HDF5 file\n",
    "        with h5py.File(file_name, 'w') as f:\n",
    "            # Create a dataset in the file\n",
    "            f.create_dataset('data', data=data, compression='gzip', compression_opts=9)\n",
    "    elif file_extension == '.npy':\n",
    "        # Save the array to a NumPy .npy file\n",
    "        np.save(file_name, data)\n",
    "    else:\n",
    "        # Raise an error for unsupported file types\n",
    "        raise ValueError(\"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| test save_data\n",
    "#| hide\n",
    "# Test for NPY saving functionality\n",
    "# Test for NPY saving functionality\n",
    "def test_save_data_npy():\n",
    "    data = np.random.rand(5, 5)\n",
    "    file_name = 'test_data.npy'\n",
    "    \n",
    "    with patch('numpy.save', autospec=True) as mock_save:\n",
    "        save_data(data, file_name)\n",
    "        mock_save.assert_called_once_with(file_name, data)\n",
    "\n",
    "# Test for HDF5 saving functionality\n",
    "def test_save_data_hdf5():\n",
    "    data = np.random.rand(5, 5)\n",
    "    file_name = 'test_data.hdf5'\n",
    "    \n",
    "    with patch('h5py.File', autospec=True) as mock_file:\n",
    "        save_data(data, file_name)\n",
    "        mock_file.assert_called_once_with(file_name, 'w')\n",
    "\n",
    "# Test for handling invalid file type\n",
    "def test_save_data_invalid_type():\n",
    "    data = np.random.rand(5, 5)\n",
    "    file_name = 'test_data.unknown'\n",
    "    \n",
    "    try:\n",
    "        save_data(data, file_name)\n",
    "        assert False, \"ValueError expected but not raised\"\n",
    "    except ValueError as e:\n",
    "        assert str(e) == \"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\", \"Incorrect error message\"\n",
    "\n",
    "test_save_data_invalid_type()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_example_orbit_data():\n",
    "    \"\"\"\n",
    "    Load orbit data from a hardcoded MAT file located in the `data` directory.\n",
    "    \n",
    "    The function is specifically designed to load the 'Xarray' variable \n",
    "    from the '1_L2_S_200_EM_CR3BP.mat' file. This setup is intended for \n",
    "    demonstration or testing purposes, where the data file and the variable \n",
    "    of interest are known ahead of time.\n",
    "\n",
    "    :return: A numpy.ndarray containing the transposed data from the MAT file.\n",
    "    \"\"\"\n",
    "    # Hardcoded file name and variable name\n",
    "    filename = \"example_orbits_1_L2_S_200_EM_CR3BP.mat\"\n",
    "    variable_name = 'Xarray'\n",
    "    \n",
    "    # Assuming the notebook or script is executed in a directory at the same level as the `data` folder\n",
    "    matlab_file_path = '..' + \"/data/example_data/\" + filename\n",
    "    \n",
    "    # Assuming `load_orbit_data` is a predefined function that loads and returns data from the .mat file\n",
    "    data = load_orbit_data(str(matlab_file_path), variable_name=variable_name)\n",
    "    # Transpose the data for further use\n",
    "    data = np.transpose(data, (2, 1, 0))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | test\n",
    "data = get_example_orbit_data()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_orbits(orbit_data: np.ndarray,  # Orbit data array\n",
    "                  sample_spec: dict or int, # Number of samples per class (dict) or total number of samples (int)\n",
    "                  labels: np.ndarray = None # Optional: Array of labels corresponding to each orbit\n",
    "                 ) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Randomly sample orbits from the provided dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        orbit_data (np.ndarray): Array of orbit data with shape (num_orbits, 6, num_time_points).\n",
    "        sample_spec (dict or int): If int, it is the total number of orbits to sample.\n",
    "                                   If dict, it specifies the number of samples for each class.\n",
    "        labels (np.ndarray, optional): Array of labels for each orbit.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the sampled orbit data and corresponding labels (if provided).\n",
    "    \"\"\"\n",
    "    if labels is not None and isinstance(sample_spec, dict):\n",
    "        # Sampling specified number of orbits for each class\n",
    "        indices = []\n",
    "        for label, count in sample_spec.items():\n",
    "            class_indices = np.where(labels == label)[0]\n",
    "            if len(class_indices) < count:\n",
    "                raise ValueError(f\"Not enough samples for class {label}. Requested {count}, available {len(class_indices)}.\")\n",
    "            selected_indices = np.random.choice(class_indices, size=count, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        indices = np.array(indices)\n",
    "    else:\n",
    "        # Random sampling without considering classes\n",
    "        indices = np.random.choice(orbit_data.shape[0], size=sample_spec, replace=False)\n",
    "    \n",
    "    # Select the sampled data and labels\n",
    "    sampled_data = orbit_data[indices]\n",
    "    sampled_labels = labels[indices] if labels is not None else None\n",
    "    \n",
    "    return sampled_data, sampled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| test sample_orbits_random_sampling_without_labels\n",
    "def test_sample_orbits_random_sampling_without_labels():\n",
    "    # Setup a mock dataset\n",
    "    orbit_data = np.random.rand(100, 6, 10)  # 100 orbits, 6 values, 10 time points\n",
    "    \n",
    "    # Perform random sampling without labels\n",
    "    sampled_data, sampled_labels = sample_orbits(orbit_data, 10)\n",
    "    \n",
    "    # Test outcomes\n",
    "    assert sampled_data.shape == (10, 6, 10), \"Shape of sampled data should match the requested sample size\"\n",
    "    assert sampled_labels is None, \"Labels should be None when not provided\"\n",
    "\n",
    "#| test sample_orbits_class_specific_sampling\n",
    "def test_sample_orbits_class_specific_sampling():\n",
    "    # Setup a mock dataset and labels\n",
    "    orbit_data = np.random.rand(100, 6, 10)\n",
    "    labels = np.random.randint(0, 3, size=100)  # 100 labels in 3 classes\n",
    "    sample_spec = {0: 5, 1: 5}\n",
    "    \n",
    "    # Perform class-specific sampling\n",
    "    sampled_data, sampled_labels = sample_orbits(orbit_data, sample_spec, labels)\n",
    "    \n",
    "    # Test outcomes\n",
    "    assert sampled_data.shape == (10, 6, 10), \"Shape of sampled data should match the total requested sample size\"\n",
    "    assert len(sampled_labels) == 10, \"Number of labels should match the total requested sample size\"\n",
    "    assert all(label in sample_spec for label in sampled_labels), \"All labels should be from requested classes\"\n",
    "\n",
    "#| test sample_orbits_insufficient_class_samples\n",
    "def test_sample_orbits_insufficient_class_samples():\n",
    "    # Setup a mock dataset and labels\n",
    "    orbit_data = np.random.rand(100, 6, 10)\n",
    "    labels = np.random.randint(0, 1, size=100)  # 100 labels in 1 class only\n",
    "    sample_spec = {0: 50, 1: 50}  # Requesting 50 samples each from class 0 and 1\n",
    "    \n",
    "    # Perform class-specific sampling with expectation of failure\n",
    "    try:\n",
    "        sample_orbits(orbit_data, sample_spec, labels)\n",
    "        assert False, \"Expected ValueError due to insufficient samples for class 1\"\n",
    "    except ValueError as e:\n",
    "        assert str(e) == \"Not enough samples for class 1. Requested 50, available 0.\", \"Error message should indicate insufficient samples for class 1\"\n",
    "\n",
    "# Execute tests to verify the behavior\n",
    "test_sample_orbits_random_sampling_without_labels()\n",
    "test_sample_orbits_class_specific_sampling()\n",
    "test_sample_orbits_insufficient_class_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EPS = 1e-18  # A small epsilon to prevent division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSFeatureWiseScaler():\n",
    "    \"\"\"\n",
    "    Scales time series data feature-wise using PyTorch tensors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_range : tuple(float, float), optional\n",
    "        Tuple representing the minimum and maximum feature values (default is (0, 1)).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    _min_v : float\n",
    "        Minimum feature value.\n",
    "    _max_v : float\n",
    "        Maximum feature value.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_range: tuple = (0, 1)) -> None:\n",
    "        assert len(feature_range) == 2\n",
    "        self._min_v, self._max_v = feature_range\n",
    "\n",
    "    # X: N x T x D\n",
    "    def fit(self, X: torch.Tensor) -> \"TSFeatureWiseScaler\":\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data.\n",
    "        \n",
    "        :param X: Input data. Shape: (N, T, D)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: The fitted scaler object.\n",
    "        :rtype: TSFeatureWiseScaler\n",
    "        \"\"\"\n",
    "        D = X.shape[2]\n",
    "        self.mins = torch.zeros(D, device=X.device)\n",
    "        self.maxs = torch.zeros(D, device=X.device)\n",
    "\n",
    "        for i in range(D):\n",
    "            self.mins[i] = torch.min(X[:, :, i])\n",
    "            self.maxs[i] = torch.max(X[:, :, i])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transforms the data.\n",
    "        \n",
    "        :param X: Input data. Shape: (N, T, D)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        return ((X - self.mins) / (self.maxs - self.mins + EPS)) * (self._max_v - self._min_v) + self._min_v\n",
    "\n",
    "    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inverse-transforms the data.\n",
    "        \n",
    "        :param X: Scaled data. Shape: (N, T, D)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Original data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        X = X - self._min_v\n",
    "        X = X / (self._max_v - self._min_v)\n",
    "        X = X * (self.maxs - self.mins + EPS)\n",
    "        X = X + self.mins\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data and transforms it.\n",
    "        \n",
    "        :param X: Input data. Shape: (N, T, D)\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSGlobalScaler():\n",
    "    \"\"\"\n",
    "    Scales time series data globally using PyTorch tensors.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    min : float\n",
    "        Minimum value encountered in the data.\n",
    "    max : float\n",
    "        Maximum value encountered in the data.\n",
    "    \"\"\"\n",
    "    def fit(self, X: torch.Tensor) -> \"TSGlobalScaler\":\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data.\n",
    "        \n",
    "        :param X: Input data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: The fitted scaler object.\n",
    "        :rtype: TSGlobalScaler\n",
    "        \"\"\"\n",
    "        self.min = torch.min(X)\n",
    "        self.max = torch.max(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Transforms the data.\n",
    "        \n",
    "        :param X: Input data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled X.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        return (X - self.min) / (self.max - self.min + EPS)\n",
    "\n",
    "    def inverse_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inverse-transforms the data.\n",
    "        \n",
    "        :param X: Scaled data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Original data.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        X = X * (self.max - self.min + EPS)\n",
    "        X = X + self.min\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fits the scaler to the data and transforms it.\n",
    "        \n",
    "        :param X: Input data.\n",
    "        :type X: torch.Tensor\n",
    "        \n",
    "        :returns: Scaled input data X.\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
