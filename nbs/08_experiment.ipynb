{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "> Scripts to perform the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "import torch\n",
    "import itertools\n",
    "import nbformat\n",
    "import papermill as pm\n",
    "import re\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_new_experiment(params: Dict[str, Any],              # Dictionary of parameters for the new experiment.\n",
    "                         experiments_folder: str,             # Path to the folder containing all experiments.\n",
    "                         json_file: Optional[str] = None      # Optional path to the JSON file tracking experiment parameters.\n",
    "                        ) -> str:                             # The path to the newly created experiment folder.\n",
    "    \"\"\"\n",
    "    Sets up a new experiment by creating a new folder and updating the JSON file with experiment parameters.\n",
    "    \"\"\"\n",
    "    # Ensure the experiments folder exists\n",
    "    if not os.path.exists(experiments_folder):\n",
    "        os.makedirs(experiments_folder)\n",
    "\n",
    "    # Default JSON file to 'experiments.json' in the experiments_folder if not provided\n",
    "    if json_file is None:\n",
    "        json_file = os.path.join(experiments_folder, 'experiments.json')\n",
    "\n",
    "    # Load existing experiments from the JSON file if it exists\n",
    "    if os.path.isfile(json_file):\n",
    "        with open(json_file, mode='r') as file:\n",
    "            experiments = json.load(file)\n",
    "    else:\n",
    "        experiments = []\n",
    "\n",
    "    # Check if the parameters already exist in the JSON file\n",
    "    for experiment in experiments:\n",
    "        if all(experiment['parameters'].get(key) == value for key, value in params.items()):\n",
    "            candidate_folder = os.path.join(experiments_folder, f\"experiment_{experiment['id']}\")\n",
    "            if os.path.exists(candidate_folder):\n",
    "                print(f'Parameters already exist for experiment: {candidate_folder}')\n",
    "                return candidate_folder\n",
    "\n",
    "    # Determine the next experiment number\n",
    "    next_experiment_number = max((experiment['id'] for experiment in experiments), default=0) + 1\n",
    "\n",
    "    # Create a new folder for the next experiment\n",
    "    new_experiment_folder = os.path.join(experiments_folder, f'experiment_{next_experiment_number}')\n",
    "    os.makedirs(new_experiment_folder, exist_ok=True)\n",
    "\n",
    "    # Add the new experiment to the list and save to JSON file\n",
    "    new_experiment = {\n",
    "        'id': next_experiment_number,\n",
    "        'parameters': params\n",
    "    }\n",
    "    experiments.append(new_experiment)\n",
    "    with open(json_file, mode='w') as file:\n",
    "        json.dump(experiments, file, indent=4)\n",
    "\n",
    "    print(f'New experiment setup complete: {new_experiment_folder}')\n",
    "    print(f'Parameters saved to {json_file}.')\n",
    "\n",
    "    return new_experiment_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_experiments_json(parameter_sets, output_file='experiments.json'):\n",
    "    \"\"\"\n",
    "    Create an experiments.json file from given parameter sets.\n",
    "    \n",
    "    Args:\n",
    "    parameter_sets (list): List of dictionaries containing parameters for each experiment.\n",
    "    output_file (str): Name of the output JSON file. Defaults to 'experiments.json'.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    experiments = []\n",
    "    \n",
    "    for i, params in enumerate(parameter_sets, start=1):\n",
    "        experiment = {\n",
    "            \"id\": i,\n",
    "            \"parameters\": params\n",
    "        }\n",
    "        experiments.append(experiment)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(experiments, f, indent=4)\n",
    "    \n",
    "    print(f\"Experiments JSON file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert numpy types and tensors to native Python types for JSON serialization.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj.item() if obj.numel() == 1 else obj.tolist()\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def add_experiment_metrics(experiments_folder: str,                    # Path to the folder containing all experiments.\n",
    "                           params: Optional[Dict[str, Any]] = None,    # Optional dictionary of parameters identifying the experiment.\n",
    "                           experiment_id: Optional[int] = None,        # Optional ID to identify the experiment.\n",
    "                           metrics: Optional[Dict[str, Any]] = None,   # Optional dictionary of metrics to be added to the experiment.\n",
    "                           json_file: Optional[str] = None             # Optional path to the JSON file tracking experiment parameters and metrics.\n",
    "                          ) -> None:\n",
    "    \"\"\"\n",
    "    Adds metrics to an existing experiment in the JSON file based on the given parameters or ID.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(experiments_folder):\n",
    "        raise FileNotFoundError(f\"The experiments folder '{experiments_folder}' does not exist.\")\n",
    "\n",
    "    if json_file is None:\n",
    "        json_file = os.path.join(experiments_folder, 'experiments.json')\n",
    "\n",
    "    if not os.path.isfile(json_file):\n",
    "        raise FileNotFoundError(f\"The JSON file '{json_file}' does not exist.\")\n",
    "\n",
    "    if params is None and experiment_id is None:\n",
    "        raise ValueError(\"Either 'params' or 'experiment_id' must be provided to identify the experiment.\")\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = {}\n",
    "\n",
    "    with open(json_file, mode='r') as file:\n",
    "        try:\n",
    "            experiments = json.load(file)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Error reading JSON file: {e}\")\n",
    "\n",
    "    found_experiment = False\n",
    "\n",
    "    for experiment in experiments:\n",
    "        if (experiment_id is not None and experiment['id'] == experiment_id) or \\\n",
    "           (params is not None and all(experiment['parameters'].get(key) == value for key, value in params.items())):\n",
    "            experiment.update(convert_numpy_types(metrics))\n",
    "            found_experiment = True\n",
    "            break\n",
    "\n",
    "    if not found_experiment:\n",
    "        if experiment_id is not None:\n",
    "            raise ValueError(f\"Experiment with the specified ID {experiment_id} does not exist.\")\n",
    "        else:\n",
    "            raise ValueError(\"Experiment with the specified parameters does not exist.\")\n",
    "\n",
    "    # Convert the entire experiments list to ensure all nested objects are serializable\n",
    "    serializable_experiments = convert_numpy_types(experiments)\n",
    "\n",
    "    with open(json_file, mode='w') as file:\n",
    "        json.dump(serializable_experiments, file, indent=4)\n",
    "\n",
    "    if experiment_id is not None:\n",
    "        print(f'Metrics added to experiment with ID {experiment_id} in {json_file}.')\n",
    "    else:\n",
    "        experiment_id = experiment['id']\n",
    "        print(f'Metrics added to experiment with ID {experiment_id} in {json_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_experiment_parameters(experiments_folder: str,                    # Path to the folder containing all experiments.\n",
    "                              experiment_id: int,                         # ID to identify the experiment.\n",
    "                              json_file: Optional[str] = None             # Optional path to the JSON file tracking experiment parameters and metrics.\n",
    "                             ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieves the parameters of an experiment from the JSON file based on the given ID.\n",
    "    \"\"\"\n",
    "    # Ensure the experiments folder exists\n",
    "    if not os.path.exists(experiments_folder):\n",
    "        raise FileNotFoundError(f\"The experiments folder '{experiments_folder}' does not exist.\")\n",
    "\n",
    "    # Default JSON file to 'experiments.json' in the experiments_folder if not provided\n",
    "    if json_file is None:\n",
    "        json_file = os.path.join(experiments_folder, 'experiments.json')\n",
    "\n",
    "    if not os.path.isfile(json_file):\n",
    "        raise FileNotFoundError(f\"The JSON file '{json_file}' does not exist.\")\n",
    "\n",
    "    # Load existing experiments from the JSON file\n",
    "    with open(json_file, mode='r') as file:\n",
    "        experiments = json.load(file)\n",
    "\n",
    "    # Find the matching experiment and return its parameters\n",
    "    for experiment in experiments:\n",
    "        if experiment['id'] == experiment_id:\n",
    "            return experiment.get('parameters', {})\n",
    "\n",
    "    # If the experiment is not found, raise an error\n",
    "    raise ValueError(f\"Experiment with the specified ID {experiment_id} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_experiment_data(experiments_folder: str,\n",
    "                        experiment_id: int,\n",
    "                        json_file: Optional[str] = None\n",
    "                       ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieves all data for an experiment from the JSON file based on the given ID.\n",
    "    \n",
    "    Args:\n",
    "    experiments_folder (str): Path to the folder containing all experiments.\n",
    "    experiment_id (int): ID to identify the experiment.\n",
    "    json_file (Optional[str]): Optional path to the JSON file tracking experiment data.\n",
    "    \n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing all data for the specified experiment.\n",
    "    \n",
    "    Raises:\n",
    "    FileNotFoundError: If the experiments folder or JSON file doesn't exist.\n",
    "    ValueError: If the experiment with the specified ID is not found.\n",
    "    \"\"\"\n",
    "    # Ensure the experiments folder exists\n",
    "    if not os.path.exists(experiments_folder):\n",
    "        raise FileNotFoundError(f\"The experiments folder '{experiments_folder}' does not exist.\")\n",
    "\n",
    "    # Default JSON file to 'experiments.json' in the experiments_folder if not provided\n",
    "    if json_file is None:\n",
    "        json_file = os.path.join(experiments_folder, 'experiments.json')\n",
    "\n",
    "    if not os.path.isfile(json_file):\n",
    "        raise FileNotFoundError(f\"The JSON file '{json_file}' does not exist.\")\n",
    "\n",
    "    # Load existing experiments from the JSON file\n",
    "    with open(json_file, mode='r') as file:\n",
    "        experiments = json.load(file)\n",
    "\n",
    "    # Find the matching experiment and return all its data\n",
    "    for experiment in experiments:\n",
    "        if experiment['id'] == int(experiment_id):\n",
    "            return experiment\n",
    "\n",
    "    # If the experiment is not found, raise an error\n",
    "    raise ValueError(f\"Experiment with the specified ID {experiment_id} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_json_to_dataframe(json_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a JSON file containing experiment results and returns a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - json_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the experiment results.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Extract relevant information\n",
    "    records = []\n",
    "    for item in data:\n",
    "        # Assuming each item is a dictionary with an 'id' field and other details\n",
    "        record = {'id_experiment': item['id']}\n",
    "        record.update(item)\n",
    "        records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_image_paths(folder_prefix, unique_ids, file_suffix):\n",
    "    file_paths = []\n",
    "    for unique_id in unique_ids:\n",
    "        file_name = f\"exp{unique_id}{file_suffix}\"\n",
    "        file_path = f\"{folder_prefix}{unique_id}/images/{file_name}\"\n",
    "        file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def concatenate_orbits_from_experiment_folder(experiments_folder, seq_len, file_suffix='_generated_orbits'):\n",
    "    arrays = []\n",
    "    \n",
    "    for folder in os.listdir(experiments_folder):\n",
    "        if folder.startswith('experiment_') and os.path.isdir(os.path.join(experiments_folder, folder)):\n",
    "            # Extract the experiment number using regex\n",
    "            match = re.search(r'experiment_(\\d+)', folder)\n",
    "            if match:\n",
    "                experiment_id = match.group(1)\n",
    "                generated_data_path = os.path.join(experiments_folder, folder, f'exp{experiment_id}{file_suffix}.npy')\n",
    "                \n",
    "                if os.path.isfile(generated_data_path):\n",
    "                    generated_orbit = np.load(generated_data_path)\n",
    "                    \n",
    "                    if generated_orbit.shape[-1] == seq_len:\n",
    "                        arrays.append(generated_orbit)\n",
    "    \n",
    "    if arrays:\n",
    "        return np.concatenate(arrays, axis=0)\n",
    "    else:\n",
    "        return np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def concatenate_csvs_from_experiment_folder(experiments_folder, file_suffix):\n",
    "    dataframes = []\n",
    "    \n",
    "    for folder in os.listdir(experiments_folder):\n",
    "        if folder.startswith('experiment_') and os.path.isdir(os.path.join(experiments_folder, folder)):\n",
    "            # Extract the experiment number using regex\n",
    "            match = re.search(r'experiment_(\\d+)', folder)\n",
    "            if match:\n",
    "                experiment_id = match.group(1)\n",
    "                csv_file_path = os.path.join(experiments_folder, folder, f'exp{experiment_id}_{file_suffix}.csv')\n",
    "                \n",
    "                if os.path.isfile(csv_file_path):\n",
    "                    # Load the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(csv_file_path)\n",
    "                    \n",
    "                    # Add a column to identify the experiment\n",
    "                    df['experiment_id'] = experiment_id\n",
    "                    \n",
    "                    dataframes.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames along rows\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Orbit and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def concatenate_and_check_orbits_from_experiment_folder(experiments_folder, csv_file_name='_refined_orbits_df.csv', np_file_name='_refined_orbits'):\n",
    "    arrays = []\n",
    "    all_refined_orbit_dfs = []\n",
    "    \n",
    "    for folder in os.listdir(experiments_folder):\n",
    "        folder_path = os.path.join(experiments_folder, folder)  # Construct the folder path\n",
    "        \n",
    "        if folder.startswith('experiment_') and os.path.isdir(folder_path):\n",
    "            # Extract the experiment number using regex\n",
    "            match = re.search(r'experiment_(\\d+)', folder)\n",
    "            if match:\n",
    "                experiment_id = match.group(1)\n",
    "                generated_data_path = os.path.join(folder_path, f'exp{experiment_id}{np_file_name}.npy')\n",
    "                refined_orbit_path = os.path.join(folder_path, f'exp{experiment_id}{csv_file_name}')  # Path to refined orbits\n",
    "                \n",
    "                if os.path.isfile(generated_data_path):\n",
    "                    generated_orbit = np.load(generated_data_path)\n",
    "                    \n",
    "                    # Load the refined orbits DataFrame\n",
    "                    if os.path.isfile(refined_orbit_path):\n",
    "                        refined_orbit_df = pd.read_csv(refined_orbit_path)\n",
    "                        \n",
    "                        # Check if the number of orbits matches the length of refined_orbit_df\n",
    "                        if generated_orbit.shape[0] != len(refined_orbit_df):\n",
    "                            print(f\"Mismatch for experiment {experiment_id}: generated_orbit count = {generated_orbit.shape[0]}, refined_orbit_df length = {len(refined_orbit_df)}\")\n",
    "                            continue  # Skip to the next folder if there is a mismatch\n",
    "                        \n",
    "                        all_refined_orbit_dfs.append(refined_orbit_df)\n",
    "                    \n",
    "                    arrays.append(generated_orbit)\n",
    "    \n",
    "    if arrays:\n",
    "        concatenated_orbit = np.concatenate(arrays, axis=0)\n",
    "        return concatenated_orbit, pd.concat(all_refined_orbit_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return np.array([]), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_parameter_sets(params, model_specific_params):\n",
    "    keys, values = zip(*params.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*[\n",
    "        value if isinstance(value, list) else [value] for value in values\n",
    "    ])]\n",
    "    \n",
    "    final_combinations = []\n",
    "    for combo in combinations:\n",
    "        model_name = combo['model_name']\n",
    "        if model_name in model_specific_params:\n",
    "            model_kwargs = model_specific_params[model_name].copy()\n",
    "            combo['model_kwargs'] = model_kwargs\n",
    "            combo['model_kwargs']['beta'] = combo.pop('beta')\n",
    "            final_combinations.append(combo)\n",
    "    \n",
    "    return final_combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def execute_parameter_notebook(notebook_to_execute, output_dir, i, params, extra_parameters=None, checkpoint_file=None):\n",
    "    try:\n",
    "        # Mark as started\n",
    "        with open(checkpoint_file, 'r+') as f:\n",
    "            checkpoint = json.load(f)\n",
    "            if i not in checkpoint['started']:\n",
    "                checkpoint['started'].append(i)\n",
    "                f.seek(0)\n",
    "                json.dump(checkpoint, f)\n",
    "                f.truncate()\n",
    "        \n",
    "        logging.info(f\"Starting execution {i}\")\n",
    "\n",
    "        # Generate output filenames\n",
    "        base_name = os.path.splitext(os.path.basename(notebook_to_execute))[0]\n",
    "        output_notebook = os.path.join(output_dir, f\"{base_name}_execution_{i}.ipynb\")\n",
    "\n",
    "        # Read the notebook\n",
    "        with open(notebook_to_execute, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "        # Merge params and extra_parameters into a single dictionary\n",
    "        if extra_parameters is not None:\n",
    "            params.update(extra_parameters)\n",
    "\n",
    "        nb = pm.execute_notebook(\n",
    "            nb,\n",
    "            output_notebook,\n",
    "            parameters=params,\n",
    "            kernel_name='pytorch',\n",
    "            timeout=100000,\n",
    "            log_output=True\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Completed execution {i}\")\n",
    "        return i\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in execution {i}: {str(e)}\")\n",
    "        logging.error(f\"Parameters used: {params}\")\n",
    "        import traceback\n",
    "        logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def paralelize_notebook_experiment(parameter_sets, notebook_to_execute, output_dir, checkpoint_file, max_workers=3, extra_parameters=None):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "    else:\n",
    "        checkpoint = {'completed': [], 'started': []}\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f)\n",
    "\n",
    "    # Ensure checkpoint is a dictionary with 'completed' and 'started' keys\n",
    "    if not isinstance(checkpoint, dict) or 'completed' not in checkpoint or 'started' not in checkpoint:\n",
    "        checkpoint = {'completed': [], 'started': []}\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f)\n",
    "    \n",
    "    # Filter out already completed executions\n",
    "    remaining_executions = [i for i in range(1, len(parameter_sets) + 1) if i not in checkpoint['completed']]\n",
    "    \n",
    "    logging.info(f\"Starting execution. {len(remaining_executions)} executions remaining.\")\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for i in remaining_executions:\n",
    "            future = executor.submit(\n",
    "                execute_parameter_notebook,\n",
    "                notebook_to_execute=notebook_to_execute,\n",
    "                output_dir=output_dir,\n",
    "                i=i,\n",
    "                params=parameter_sets[i-1],\n",
    "                extra_parameters=extra_parameters,\n",
    "                checkpoint_file=checkpoint_file\n",
    "            )\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                logging.info(f\"Execution {result} completed successfully.\")\n",
    "                # Update checkpoint\n",
    "                with open(checkpoint_file, 'r+') as f:\n",
    "                    checkpoint = json.load(f)\n",
    "                    checkpoint['completed'].append(result)\n",
    "                    f.seek(0)\n",
    "                    json.dump(checkpoint, f)\n",
    "                    f.truncate()\n",
    "            else:\n",
    "                logging.warning(\"An execution failed.\")\n",
    "    \n",
    "    logging.info(\"All executions completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "data_path = r\"/orbit-generation/data/orbits_fix_1500/EM_N_fix_1500.h5\"\n",
    "experiments_folder = \"../experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_file_paths(experiment_id, images_folder, experiment_folder):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of file paths for an experiment.\n",
    "\n",
    "    Parameters:\n",
    "        experiment_id (int or str): The unique ID of the experiment.\n",
    "        images_folder (str): The folder path where image files are stored.\n",
    "        experiment_folder (str): The folder path where experiment-related files are stored.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing all generated file paths.\n",
    "    \"\"\"\n",
    "    paths = {\n",
    "        # Images - Orbits\n",
    "        'static_all_orbit_path': os.path.join(images_folder, f'exp{experiment_id}_generated_orbits.png'),\n",
    "        'dynamic_orbits_path': os.path.join(images_folder, f'exp{experiment_id}_generated_orbits.html'),\n",
    "        'refined_orbits_path': os.path.join(images_folder, f'exp{experiment_id}_refined_orbits.png'),\n",
    "        'dynamical_refined_orbits_path': os.path.join(images_folder, f'exp{experiment_id}_refined_orbits.html'),\n",
    "        'generated_orbits_that_converged_path': os.path.join(images_folder, f'exp{experiment_id}_generated_orbits_that_converged.png'),\n",
    "        'generated_orbits_that_did_not_converged_path': os.path.join(images_folder, f'exp{experiment_id}_generated_orbits_that_not_converged.png'),\n",
    "\n",
    "        # Images - Latent Spaces\n",
    "        'latent_space_path': os.path.join(images_folder, f'exp{experiment_id}_latent_space'),\n",
    "        'full_latent_space_path': os.path.join(images_folder, f'exp{experiment_id}_full_latent_space'),\n",
    "        'discarded_latent_space_path': os.path.join(images_folder, f'exp{experiment_id}_discarded_latent_space'),\n",
    "        'combined_latent_space_path': os.path.join(images_folder, f'exp{experiment_id}_combined_latent_space'),\n",
    "        'combined_latent_space_arrows_path': os.path.join(images_folder, f'exp{experiment_id}_combined_latent_space_arrows'),\n",
    "        'family_centroids_plot_path': os.path.join(images_folder, f'exp{experiment_id}_family_centroids'),\n",
    "        'full_family_centroids_plot_path': os.path.join(images_folder, f'exp{experiment_id}_full_family_centroids'),\n",
    "\n",
    "        # Images - Feature Spaces\n",
    "        'features_plot_path': os.path.join(images_folder, f'exp{experiment_id}_features'),\n",
    "        'family_feature_centroids_plot_path': os.path.join(images_folder, f'exp{experiment_id}_family_feature_centroids'),\n",
    "\n",
    "        # Other Images\n",
    "        'model_losses_path': os.path.join(images_folder, f'exp{experiment_id}_model_losses.png'),\n",
    "        'histogram_comparison_path': os.path.join(images_folder, f'exp{experiment_id}_histogram_comparison.png'),\n",
    "        'full_histogram_comparison_path': os.path.join(images_folder, f'exp{experiment_id}_full_histogram_comparison.png'),\n",
    "\n",
    "        # Model\n",
    "        'model_save_path': os.path.join(experiment_folder, f'exp{experiment_id}_model.pth'),\n",
    "\n",
    "        # Orbits Data\n",
    "        'generated_data_path': os.path.join(experiment_folder, f'exp{experiment_id}_generated_orbits.npy'),\n",
    "        'refined_data_path': os.path.join(experiment_folder, f'exp{experiment_id}_refined_orbits.npy'),\n",
    "\n",
    "        # Latent Representations\n",
    "        'latent_representations_path': os.path.join(experiment_folder, f'exp{experiment_id}_latent_representations.npy'),\n",
    "        'family_centroids_path': os.path.join(experiment_folder, f'exp{experiment_id}_family_centroids.npy'),\n",
    "\n",
    "        # Features Data\n",
    "        'generation_df_path': os.path.join(experiment_folder, f'exp{experiment_id}_generation_df.csv'),\n",
    "        'refined_orbits_df_path': os.path.join(experiment_folder, f'exp{experiment_id}_refined_orbits_df.csv')\n",
    "    }\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from orbit_generation.dataset import get_first_period_dataset, get_orbit_classes\n",
    "from orbit_generation.data import TSFeatureWiseScaler, discard_random_labels\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_experiment_data(params, experiments_folder, data_path, want_to_discover):\n",
    "    \"\"\"\n",
    "    Prepare the experiment data based on the provided parameters and configurations.\n",
    "\n",
    "    Parameters:\n",
    "        params (dict): A dictionary containing all the experiment parameters.\n",
    "        experiments_folder (str): The folder where experiments are stored.\n",
    "        data_path (str): Path to the dataset file.\n",
    "        want_to_discover (bool): Flag indicating whether to discover new families or use existing ones.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Processed scaled data, orbit dataframe, family labels, and additional metadata.\n",
    "    \"\"\"\n",
    "    # Step 1: Setup experiment folders\n",
    "    experiment_folder = setup_new_experiment(params, experiments_folder)\n",
    "    images_folder = os.path.join(experiment_folder, 'images')\n",
    "    if not os.path.exists(images_folder):\n",
    "        os.makedirs(images_folder)\n",
    "    \n",
    "    experiment_id = int(os.path.basename(experiment_folder).split('_')[1])\n",
    "\n",
    "    # Step 2: Generate file paths\n",
    "    file_paths = generate_file_paths(experiment_id, images_folder, experiment_folder)\n",
    "\n",
    "    # Step 3: Load full dataset\n",
    "    full_data, full_orbit_df, full_labels, system_dict = get_first_period_dataset(\n",
    "        file_path=data_path,\n",
    "        segment_length=params['seq_len']\n",
    "    )\n",
    "\n",
    "    # Adjust data shape if feature_dim is 6\n",
    "    if params['feature_dim'] == 6:\n",
    "        full_data = full_data[:, 1:, :]  # Remove the first feature dimension\n",
    "\n",
    "    # Map orbit IDs to classes\n",
    "    full_orbits_id_classes = [full_orbit_df.at[index, 'id_class'] for index in full_labels]\n",
    "\n",
    "    # Step 4: Handle discovery or reuse of discarded families\n",
    "    if want_to_discover:\n",
    "        discarded_family_ids, data, orbits_id_classes = discard_random_labels(\n",
    "            full_data,\n",
    "            np.array(full_orbits_id_classes),\n",
    "            int(params['families_to_discard'])\n",
    "        )\n",
    "        discarded_families = get_orbit_classes(discarded_family_ids)[0]\n",
    "        orbit_df = full_orbit_df[~full_orbit_df['id_class'].isin(discarded_family_ids)]\n",
    "\n",
    "        # Log metrics for the experiment\n",
    "        add_experiment_metrics(\n",
    "            experiments_folder,\n",
    "            experiment_id=experiment_id,\n",
    "            metrics={\n",
    "                'discarded_family_ids': discarded_family_ids,\n",
    "                'discarded_families': discarded_families\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # Retrieve previously discarded families for this experiment\n",
    "        experiment_data = get_experiment_data(\n",
    "            experiments_folder=experiments_folder,\n",
    "            experiment_id=experiment_id\n",
    "        )\n",
    "        discarded_family_ids = experiment_data.get('discarded_family_ids', {})\n",
    "\n",
    "        discarded_family_ids, data, orbits_id_classes = discard_random_labels(\n",
    "            full_data,\n",
    "            np.array(full_orbits_id_classes),\n",
    "            discarded_family_ids\n",
    "        )\n",
    "        discarded_families = get_orbit_classes(discarded_family_ids)[0]\n",
    "        orbit_df = full_orbit_df[~full_orbit_df['id_class'].isin(discarded_family_ids)]\n",
    "\n",
    "    # Step 5: Get family labels for remaining orbits\n",
    "    family_labels = get_orbit_classes(orbits_id_classes)[0]\n",
    "\n",
    "    # Step 6: Extract features and scale data\n",
    "    feature_names = ['jacobi', 'period', 'stability']\n",
    "    features = orbit_df[feature_names].to_numpy()\n",
    "\n",
    "    scaler = TSFeatureWiseScaler()\n",
    "    scaled_data = scaler.fit_transform(torch.tensor(data, dtype=torch.float32))\n",
    "\n",
    "    # Return processed data and metadata\n",
    "    return scaled_data, orbit_df, family_labels, discarded_families, features, file_paths, experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pytorch_lightning import Trainer\n",
    "from orbit_generation.model_factory import get_model\n",
    "from orbit_generation.data import create_dataloaders\n",
    "from orbit_generation.architectures import VAELossHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_and_train_model(params, scaled_data, experiments_folder, experiment_id, file_paths, want_to_train):\n",
    "    \"\"\"\n",
    "    Prepare the model and either train it or load a pre-trained version based on the provided parameters.\n",
    "\n",
    "    Parameters:\n",
    "        params (dict): A dictionary containing all the experiment parameters.\n",
    "        scaled_data (torch.Tensor): The scaled data to be used for training or validation.\n",
    "        experiments_folder (str): The folder where experiments are stored.\n",
    "        experiment_id (int): The unique ID of the current experiment.\n",
    "        file_paths (dict): A dictionary containing file paths for saving/loading model and metrics.\n",
    "        want_to_train (bool): Flag indicating whether to train the model or load a pre-trained one.\n",
    "\n",
    "    Returns:\n",
    "        object: The trained or loaded model.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the model\n",
    "    model = get_model(params)\n",
    "\n",
    "    # Step 2: Handle training\n",
    "    if want_to_train:\n",
    "        # Create data loaders\n",
    "        train_loader, val_loader = create_dataloaders(\n",
    "            scaled_data,\n",
    "            val_split=params.get('val_split', 0.1),\n",
    "            batch_size=params.get('batch_size', 32)\n",
    "        )\n",
    "\n",
    "        # Initialize loss history callback\n",
    "        loss_history = VAELossHistory()\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=params.get('epochs', 10),\n",
    "            log_every_n_steps=10,\n",
    "            devices=\"auto\",\n",
    "            accelerator=\"auto\",\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            callbacks=[loss_history]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in trainer.callback_metrics.items():\n",
    "            print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "        # Save metrics to experiment folder\n",
    "        add_experiment_metrics(\n",
    "            experiments_folder,\n",
    "            experiment_id=experiment_id,\n",
    "            metrics=trainer.callback_metrics\n",
    "        )\n",
    "\n",
    "        # Save model state to file\n",
    "        torch.save(model.state_dict(), file_paths['model_save_path'])\n",
    "\n",
    "        # Plot and save loss history\n",
    "        loss_history.plot_all_losses(save_path=file_paths['model_losses_path'])\n",
    "\n",
    "    else:\n",
    "        # Load pre-trained model state from file\n",
    "        model.load_state_dict(torch.load(file_paths['model_save_path'], weights_only=True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
