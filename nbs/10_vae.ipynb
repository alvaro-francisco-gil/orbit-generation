{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE\n",
    "\n",
    "> Scripts to use Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MetricCollection, MeanMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BetaVAE(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, beta=1.0, loss_fn=None, optimizer_cls=torch.optim.Adam, lr=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_fn = loss_fn if loss_fn else self.default_loss_fn\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "        self.lr = lr\n",
    "        self.metrics = MetricCollection({\n",
    "            'total_loss': MeanMetric(),\n",
    "            'reconstruction_loss': MeanMetric(),\n",
    "            'kl_loss': MeanMetric()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = self._reparameterize(z_mean, z_log_var)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def _reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def default_loss_fn(self, x, x_hat, z_mean, z_log_var):\n",
    "        # Calculate reconstruction loss between the original input x and the reconstructed input x_hat.\n",
    "        reconst_loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "        # Calculate the KL divergence to measure how much information is lost when using q(z|x) to represent p(z).\n",
    "        kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp(), dim=1).mean()\n",
    "        return reconst_loss, self.beta * kl_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = self._reparameterize(z_mean, z_log_var)\n",
    "        x_hat = self.decoder(z)\n",
    "        reconst_loss, kl_loss = self.loss_fn(x, x_hat, z_mean, z_log_var)\n",
    "        total_loss = reconst_loss + kl_loss\n",
    "        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_cls(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def generate(self, n=1):\n",
    "        z = torch.randn(n, self.latent_dim, device=self.device)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
