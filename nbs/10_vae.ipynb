{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE\n",
    "\n",
    "> Scripts to use Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import MetricCollection, MeanMetric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sampling(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch module to perform the reparameterization trick for VAEs.\n",
    "    \"\"\"\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AbstractVAE(ABC):\n",
    "    def __init__(self, seq_len, feat_dim, latent_dim):\n",
    "        \"\"\"\n",
    "        Initialize the VAE with given sequence length, feature dimension, and latent dimension.\n",
    "\n",
    "        :param seq_len: The length of the input sequences\n",
    "        :param feat_dim: The dimensionality of the input features\n",
    "        :param latent_dim: The dimensionality of the latent space\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Abstract method for the reparameterization trick.\n",
    "\n",
    "        :param mu: Mean from the encoder's latent space\n",
    "        :param logvar: Log variance from the encoder's latent space\n",
    "        :return: Sampled latent vector\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Abstract method to define the forward pass of the VAE.\n",
    "\n",
    "        :param x: Input data\n",
    "        :return: Reconstructed input, latent representation, and other necessary outputs\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BetaVAE(pl.LightningModule, AbstractVAE):\n",
    "    \"\"\"\n",
    "    Beta Variational Autoencoder implementation with configurable encoder and decoder.\n",
    "    Args:\n",
    "        encoder (nn.Module): The encoder network.\n",
    "        decoder (nn.Module): The decoder network.\n",
    "        beta (float): The beta coefficient for the KL divergence term.\n",
    "        ...\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, beta=1.0, loss_fn=None, optimizer_cls=torch.optim.Adam, lr=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_fn = loss_fn if loss_fn else self.default_loss_fn\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "        self.lr = lr\n",
    "        self.metrics = MetricCollection({\n",
    "            'total_loss': MeanMetric(),\n",
    "            'reconstruction_loss': MeanMetric(),\n",
    "            'kl_loss': MeanMetric()\n",
    "        })\n",
    "        self.latent_dim = encoder.latent_dim\n",
    "        self.feat_dim = encoder.feat_dim\n",
    "        self.seq_len = encoder.seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, z_mean, z_log_var = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "    def _reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def reconstruction_loss_by_axis(self, x, x_hat, axis):\n",
    "        # Compute mean squared error along the specified axis\n",
    "        loss = torch.mean((x - x_hat) ** 2, dim=axis)\n",
    "        # Sum the loss to get a scalar value\n",
    "        return loss.sum()\n",
    "\n",
    "    def default_loss_fn(self, x, x_hat, z_mean, z_log_var):\n",
    "        # Compute reconstruction loss over each axis and sum them\n",
    "        reconst_loss = (\n",
    "            self.reconstruction_loss_by_axis(x, x_hat, axis=0) +\n",
    "            self.reconstruction_loss_by_axis(x, x_hat, axis=1) +\n",
    "            self.reconstruction_loss_by_axis(x, x_hat, axis=2)\n",
    "        )\n",
    "        # KL Divergence Loss\n",
    "        kl_loss = -0.5 * torch.mean(\n",
    "            torch.sum(1 + z_log_var - z_mean ** 2 - torch.exp(z_log_var), dim=1)\n",
    "        )\n",
    "        return reconst_loss, self.beta * kl_loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, list) else batch\n",
    "        z, z_mean, z_log_var = self.encoder(x)  # Unpack all three values\n",
    "        x_hat = self.decoder(z)\n",
    "        reconst_loss, kl_loss = self.loss_fn(x, x_hat, z_mean, z_log_var)\n",
    "        total_loss = reconst_loss + kl_loss\n",
    "\n",
    "        # Update metrics\n",
    "        self.metrics['total_loss'].update(total_loss)\n",
    "        self.metrics['reconstruction_loss'].update(reconst_loss)\n",
    "        self.metrics['kl_loss'].update(kl_loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_reconstruction_loss', reconst_loss, on_step=True, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('train_kl_loss', kl_loss, on_step=True, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log_dict({f'train_{k}': v.compute() for k, v in self.metrics.items()}, on_step=True, on_epoch=True)\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[0] if isinstance(batch, list) else batch\n",
    "        z, z_mean, z_log_var = self.encoder(x)  # Unpack all three values\n",
    "        x_hat = self.decoder(z)\n",
    "        reconst_loss, kl_loss = self.loss_fn(x, x_hat, z_mean, z_log_var)\n",
    "        total_loss = reconst_loss + kl_loss\n",
    "        \n",
    "        # Update validation metrics\n",
    "        self.val_metrics['total_loss'].update(total_loss)\n",
    "        self.val_metrics['reconstruction_loss'].update(reconst_loss)\n",
    "        self.val_metrics['kl_loss'].update(kl_loss)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_loss', total_loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_reconstruction_loss', reconst_loss, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val_kl_loss', kl_loss, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log_dict({f'val_{k}': v.compute() for k, v in self.val_metrics.items()}, on_step=False, on_epoch=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_cls(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def generate(self, n=1):\n",
    "        z = torch.randn(n, self.latent_dim, device=self.device)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
