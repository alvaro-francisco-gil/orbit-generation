{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> Scripts to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "# from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tsgm.models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class BetaVAE(keras.Model):\n",
    "    \"\"\"\n",
    "    beta-VAE implementation for unlabeled time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: keras.Model, decoder: keras.Model, beta: float = 1.0, **kwargs) -> None:\n",
    "        super(BetaVAE, self).__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self._seq_len = self.decoder.output_shape[1]\n",
    "        self.latent_dim = self.decoder.input_shape[1]\n",
    "\n",
    "    @property\n",
    "    def metrics(self) -> list:\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, X: tsgm.types.Tensor) -> tsgm.types.Tensor:\n",
    "        z_mean, _, _ = self.encoder(X)\n",
    "        x_decoded = self.decoder(z_mean)\n",
    "        if len(x_decoded.shape) == 1:\n",
    "            x_decoded = x_decoded.reshape((1, -1))\n",
    "        return x_decoded\n",
    "\n",
    "    def _get_reconstruction_loss(self, X: tsgm.types.Tensor, Xr: tsgm.types.Tensor) -> float:\n",
    "        reconst_loss = tsgm.utils.reconstruction_loss_by_axis(X, Xr, axis=0) +\\\n",
    "            tsgm.utils.reconstruction_loss_by_axis(X, Xr, axis=1) +\\\n",
    "            tsgm.utils.reconstruction_loss_by_axis(X, Xr, axis=2)\n",
    "        return reconst_loss\n",
    "\n",
    "    def train_step(self, data: tsgm.types.Tensor) -> dict:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = self._get_reconstruction_loss(data, reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data: tsgm.types.Tensor) -> dict:\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = self._get_reconstruction_loss(data, reconstruction)\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        # Ensure losses are scalar values\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": total_loss,\n",
    "            \"val_reconstruction_loss\": reconstruction_loss,\n",
    "            \"val_kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tsgm\n",
    "\n",
    "def get_model(params, data_path):\n",
    "    model_name = params['model_name']\n",
    "    data_used = os.path.splitext(os.path.basename(data_path))[0]\n",
    "\n",
    "    if model_name == 'vae_conv5':\n",
    "        # Accessing model configuration from the zoo using parameters from the dictionary\n",
    "        architecture = tsgm.models.zoo[model_name](\n",
    "            seq_len=params['seq_len'], \n",
    "            feat_dim=params['feature_dim'], \n",
    "            latent_dim=params['latent_dim']\n",
    "        )\n",
    "\n",
    "        # Extracting encoder and decoder from the architecture\n",
    "        encoder, decoder = architecture.encoder, architecture.decoder\n",
    "\n",
    "        # Build the VAE\n",
    "        vae = tsgm.models.cvae.BetaVAE(encoder, decoder)\n",
    "        vae.compile(optimizer=params['optimizer']['name'], learning_rate=params['optimizer']['learning_rate'])\n",
    "        return vae\n",
    "\n",
    "    elif model_name == 'timeGAN':\n",
    "        model = tsgm.models.timeGAN.TimeGAN(\n",
    "            seq_len=params['seq_len'],\n",
    "            module=\"gru\",\n",
    "            hidden_dim=24,\n",
    "            n_features=params['feature_dim'],\n",
    "            n_layers=3,\n",
    "            batch_size=params['batch_size'],\n",
    "            gamma=1.0,\n",
    "        )\n",
    "        # .compile() sets all optimizers to Adam by default\n",
    "        model.compile(optimizer=params['optimizer']['name'], learning_rate=params['optimizer']['learning_rate'])\n",
    "        return model\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model(params):\n",
    "    model_name = params['model_name']\n",
    "\n",
    "    if model_name == 'vae_conv5':\n",
    "        # Accessing model configuration from the zoo using parameters from the dictionary\n",
    "        architecture = tsgm.models.zoo[model_name](\n",
    "            seq_len=params['seq_len'], \n",
    "            feat_dim=params['feature_dim'], \n",
    "            latent_dim=params['latent_dim']\n",
    "        )\n",
    "\n",
    "        # Extracting encoder and decoder from the architecture\n",
    "        encoder, decoder = architecture.encoder, architecture.decoder\n",
    "\n",
    "        # Build the VAE\n",
    "        # vae = BetaVAE(encoder, decoder)\n",
    "        vae = tsgm.models.cvae.BetaVAE(encoder, decoder)\n",
    "        return vae\n",
    "\n",
    "    elif model_name == 'timeGAN':\n",
    "        model = tsgm.models.timeGAN.TimeGAN(\n",
    "            seq_len=params['seq_len'],\n",
    "            module=\"gru\",\n",
    "            hidden_dim=24,\n",
    "            n_features=params['feature_dim'],\n",
    "            n_layers=3,\n",
    "            batch_size=params['batch_size'],\n",
    "            gamma=1.0,\n",
    "        )\n",
    "        # .compile() sets all optimizers to Adam by default\n",
    "        model.compile(optimizer=params['optimizer']['name'], learning_rate=params['optimizer']['learning_rate'])\n",
    "        return model\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_optimizer(optimizer_config):\n",
    "    name = optimizer_config['name'].lower()\n",
    "    if name == 'adam':\n",
    "        return keras.optimizers.Adam(learning_rate=optimizer_config.get('learning_rate', 0.001))\n",
    "    elif name == 'sgd':\n",
    "        return keras.optimizers.SGD(learning_rate=optimizer_config.get('learning_rate', 0.01))\n",
    "    # Add additional optimizers as needed\n",
    "    raise ValueError(\"Unsupported optimizer: {}\".format(optimizer_config['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_plot_and_return_metrics(history, validation=True):\n",
    "    \"\"\"\n",
    "    Extracts the metrics from the training history, plots the training and validation loss over epochs if validation is True, and returns the metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - history: History object returned by model.fit().\n",
    "    - validation: Boolean flag to control whether to extract and plot validation metrics.\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: Dictionary containing the final training and validation metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Extract training metrics from the training history and add to the metrics dictionary\n",
    "    metrics['loss'] = history.history['loss'][-1]\n",
    "    metrics['reconstruction_loss'] = history.history['reconstruction_loss'][-1]\n",
    "    metrics['kl_loss'] = history.history['kl_loss'][-1]\n",
    "    \n",
    "    if validation:\n",
    "        if 'val_loss' in history.history:\n",
    "            metrics['val_loss'] = history.history['val_loss'][-1]\n",
    "        if 'val_reconstruction_loss' in history.history:\n",
    "            metrics['val_reconstruction_loss'] = history.history['val_reconstruction_loss'][-1]\n",
    "        if 'val_kl_loss' in history.history:\n",
    "            metrics['val_kl_loss'] = history.history['val_kl_loss'][-1]\n",
    "\n",
    "    # Get the number of epochs\n",
    "    num_epochs = len(history.history['loss'])\n",
    "\n",
    "    # Plot metrics\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, history.history['loss'], label='Training loss')\n",
    "    if validation:\n",
    "        plt.plot(epochs, history.history['val_loss'], label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
