{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "> Scripts to perform evaluation on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, fowlkes_mallows_score, silhouette_score, jaccard_score, confusion_matrix\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.spatial.distance import squareform\n",
    "from fastdtw import fastdtw\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_metric_heatmaps(results, distance_metrics, clustering_algorithms, evaluation_metrics):\n",
    "    \"\"\"\n",
    "    Plot heatmaps for each evaluation metric.\n",
    "    \"\"\"\n",
    "    n_metrics = len(evaluation_metrics)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(evaluation_metrics):\n",
    "        data = np.zeros((len(distance_metrics), len(clustering_algorithms)))\n",
    "        for d, distance in enumerate(distance_metrics):\n",
    "            for c, clustering in enumerate(clustering_algorithms):\n",
    "                data[d, c] = results[f\"{distance}_{clustering}\"][metric]\n",
    "        \n",
    "        sns.heatmap(data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", \n",
    "                    xticklabels=clustering_algorithms, yticklabels=distance_metrics, ax=axes[i])\n",
    "        axes[i].set_title(f\"{metric} Scores\")\n",
    "        axes[i].set_xlabel(\"Clustering Algorithms\")\n",
    "        axes[i].set_ylabel(\"Distance Metrics\")\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_comparison(orbit_df, synthetic_orbit_df):\n",
    "    \"\"\"\n",
    "    Function to create a scatter plot comparing 'period' and 'calculated_jacobi'\n",
    "    between two DataFrames and plot the index of points in synthetic_orbit_df.\n",
    "    \n",
    "    Parameters:\n",
    "    - orbit_df: DataFrame containing 'period' and 'calculated_jacobi' columns\n",
    "    - synthetic_orbit_df: DataFrame containing 'period' and 'calculated_jacobi' columns\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot for orbit_df\n",
    "    plt.scatter(orbit_df['period'], orbit_df['calculated_jacobi'], color='blue', label='orbit_df', marker='o')\n",
    "\n",
    "    # Plot for synthetic_orbit_df\n",
    "    plt.scatter(synthetic_orbit_df['period'], synthetic_orbit_df['calculated_jacobi'], color='red', label='synthetic_orbit_df', marker='x')\n",
    "\n",
    "    # Annotate the points in synthetic_orbit_df with their indices\n",
    "    for i in synthetic_orbit_df.index:\n",
    "        plt.text(synthetic_orbit_df['period'].loc[i], \n",
    "                 synthetic_orbit_df['calculated_jacobi'].loc[i], \n",
    "                 str(int(i)), \n",
    "                 fontsize=9, color='black', ha='right')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Period')\n",
    "    plt.ylabel('Calculated Jacobi')\n",
    "    plt.title('Comparison of Features Between Two DataFrames')\n",
    "    plt.legend()  # Add legend to distinguish between the groups\n",
    "\n",
    "    # Display the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_closest_feature_distances(orbit_df, synthetic_orbit_df, features, plot_comparison=True):\n",
    "    \"\"\"\n",
    "    Calculate the distance from each point in synthetic_orbit_df to the closest point in orbit_df\n",
    "    based on specified features.\n",
    "    \n",
    "    Parameters:\n",
    "    - orbit_df: DataFrame containing the training data.\n",
    "    - synthetic_orbit_df: DataFrame containing the synthetic data.\n",
    "    - features: List of features (columns) to use for calculating the distances.\n",
    "    \n",
    "    Returns:\n",
    "    - distances: A NumPy array of the minimum distances from each synthetic point to the nearest orbit point.\n",
    "    \"\"\"\n",
    "    # Extract the relevant features from both DataFrames\n",
    "    orbit_points = orbit_df[features].values\n",
    "    synthetic_points = synthetic_orbit_df[features].values\n",
    "\n",
    "    # Create a KDTree for efficient nearest-neighbor search in orbit_df\n",
    "    tree = cKDTree(orbit_points)\n",
    "\n",
    "    # Query the KDTree with the synthetic points to find the distance to the nearest orbit point\n",
    "    distances, _ = tree.query(synthetic_points, k=1)\n",
    "\n",
    "    if plot_comparison:\n",
    "        plot_comparison(orbit_df, synthetic_orbit_df)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_non_matching_elements(main_array, check_array):\n",
    "    \"\"\"\n",
    "    Finds elements in check_array that are not present in main_array.\n",
    "\n",
    "    Parameters:\n",
    "    main_array (numpy.ndarray): The main array with larger set of elements.\n",
    "    check_array (numpy.ndarray): The array with elements to check against the main array.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Elements in check_array that are not in main_array.\n",
    "    \"\"\"\n",
    "    # Convert arrays to tuples to enable comparison\n",
    "    main_set = set(map(tuple, main_array))\n",
    "    check_set = set(map(tuple, check_array))\n",
    "\n",
    "    # Find elements in check_set that are not in main_set\n",
    "    non_matching_elements = check_set - main_set\n",
    "\n",
    "    # Convert the result back to a numpy array\n",
    "    non_matching_elements_array = np.array(list(non_matching_elements))\n",
    "\n",
    "    return non_matching_elements_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Clustering with Multiple Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_clustering_multiple_labels(latent_representations: np.ndarray,  # The latent space data.\n",
    "                                        list_of_labels: list,                # List of true labels or a single true labels array.\n",
    "                                        clustering_method: str = 'kmeans',   # The clustering algorithm to use ('kmeans', 'gmm', 'dbscan').\n",
    "                                        label_names: list = None,            # Optional names for the label sets.\n",
    "                                        **kwargs                             # Additional arguments for the clustering algorithm.\n",
    "                                       ) -> dict:                            # Returns a dictionary with clustering metrics.\n",
    "    \"\"\"\n",
    "    Evaluates the clustering quality of the latent representations for one or multiple sets of labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_clustering_accuracy(true_labels, pred_labels):\n",
    "        contingency = contingency_matrix(true_labels, pred_labels)\n",
    "        row_ind, col_ind = linear_sum_assignment(-contingency)\n",
    "        return contingency[row_ind, col_ind].sum() / np.sum(contingency)\n",
    "\n",
    "    # Ensure list_of_labels is a list of arrays\n",
    "    if isinstance(list_of_labels, np.ndarray):\n",
    "        list_of_labels = [list_of_labels]\n",
    "    \n",
    "    # Use default names if label_names are not provided\n",
    "    if label_names is None:\n",
    "        label_names = [f'Set_{i+1}' for i in range(len(list_of_labels))]\n",
    "    \n",
    "    combined_metrics = {}\n",
    "    average_metrics = {\n",
    "        'ARI': 0,\n",
    "        'NMI': 0,\n",
    "        'Homogeneity': 0,\n",
    "        'Completeness': 0,\n",
    "        'V-Measure': 0,\n",
    "        'FMI': 0,\n",
    "        'Purity': 0,\n",
    "        'Silhouette Score': 0,\n",
    "        'Jaccard': 0,\n",
    "        'Accuracy': 0\n",
    "    }\n",
    "    num_label_sets = len(list_of_labels)\n",
    "    \n",
    "    label_encoders = [LabelEncoder() for _ in range(num_label_sets)]\n",
    "    encoded_labels_list = [label_encoders[i].fit_transform(list_of_labels[i]) for i in range(num_label_sets)]\n",
    "    \n",
    "    for i, true_labels in enumerate(encoded_labels_list):\n",
    "        # Determine the number of clusters\n",
    "        n_clusters = len(np.unique(true_labels))\n",
    "        \n",
    "        # Apply the selected clustering algorithm\n",
    "        if clustering_method == 'kmeans':\n",
    "            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, **kwargs)\n",
    "            pred_labels = clusterer.fit_predict(latent_representations)\n",
    "        elif clustering_method == 'gmm':\n",
    "            clusterer = GaussianMixture(n_components=n_clusters, random_state=42, **kwargs)\n",
    "            pred_labels = clusterer.fit_predict(latent_representations)\n",
    "        elif clustering_method == 'dbscan':\n",
    "            clusterer = DBSCAN(**kwargs)\n",
    "            pred_labels = clusterer.fit_predict(latent_representations)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported clustering method. Choose from 'kmeans', 'gmm', 'dbscan'.\")\n",
    "        \n",
    "        # Calculate clustering metrics\n",
    "        ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "        homogeneity = homogeneity_score(true_labels, pred_labels)\n",
    "        completeness = completeness_score(true_labels, pred_labels)\n",
    "        v_measure = v_measure_score(true_labels, pred_labels)\n",
    "        fmi = fowlkes_mallows_score(true_labels, pred_labels)\n",
    "        \n",
    "        # Purity\n",
    "        cont_matrix = contingency_matrix(true_labels, pred_labels)\n",
    "        purity = np.sum(np.amax(cont_matrix, axis=0)) / np.sum(cont_matrix)\n",
    "        \n",
    "        # Silhouette Score\n",
    "        silhouette = silhouette_score(latent_representations, pred_labels)\n",
    "        \n",
    "        # Jaccard Coefficient and Accuracy\n",
    "        jaccard = jaccard_score(true_labels, pred_labels, average='macro')\n",
    "        accuracy = calculate_clustering_accuracy(true_labels, pred_labels)\n",
    "        \n",
    "        # Store the results for this set of labels\n",
    "        combined_metrics.update({\n",
    "            f'{label_names[i]}_ari': ari,\n",
    "            f'{label_names[i]}_nmi': nmi,\n",
    "            f'{label_names[i]}_homogeneity': homogeneity,\n",
    "            f'{label_names[i]}_completeness': completeness,\n",
    "            f'{label_names[i]}_v-measure': v_measure,\n",
    "            f'{label_names[i]}_fmi': fmi,\n",
    "            f'{label_names[i]}_purity': purity,\n",
    "            f'{label_names[i]}_silhouette_score': silhouette,\n",
    "            f'{label_names[i]}_jaccard': jaccard,\n",
    "            f'{label_names[i]}_accuracy': accuracy\n",
    "        })\n",
    "        \n",
    "        # Accumulate the results for averaging\n",
    "        average_metrics['ARI'] += ari\n",
    "        average_metrics['NMI'] += nmi\n",
    "        average_metrics['Homogeneity'] += homogeneity\n",
    "        average_metrics['Completeness'] += completeness\n",
    "        average_metrics['V-Measure'] += v_measure\n",
    "        average_metrics['FMI'] += fmi\n",
    "        average_metrics['Purity'] += purity\n",
    "        average_metrics['Silhouette Score'] += silhouette\n",
    "        average_metrics['Jaccard'] += jaccard\n",
    "        average_metrics['Accuracy'] += accuracy\n",
    "    \n",
    "    # Compute the average metrics if there are multiple sets of labels\n",
    "    if num_label_sets > 1:\n",
    "        for key in average_metrics:\n",
    "            combined_metrics[f'average_{key}'] = average_metrics[key] / num_label_sets\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def euclidean_distance(x):\n",
    "    n = x.shape[0]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = np.sqrt(np.sum((x[i] - x[j])**2))\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = dist\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def manhattan_distance(x):\n",
    "    n = x.shape[0]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = np.sum(np.abs(x[i] - x[j]))\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = dist\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cosine_distance(x):\n",
    "    n = x.shape[0]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dot_product = np.sum(x[i] * x[j])\n",
    "            norm_i = np.sqrt(np.sum(x[i]**2))\n",
    "            norm_j = np.sqrt(np.sum(x[j]**2))\n",
    "            similarity = dot_product / (norm_i * norm_j)\n",
    "            dist = 1 - similarity  # This ensures the distance is between 0 and 2\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = dist\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dtw_distance(x):\n",
    "    n = x.shape[0]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            distance, _ = fastdtw(x[i].T, x[j].T)\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = distance\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Closest Orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_nearest_orbits(single_orbit: np.ndarray,\n",
    "                        orbit_data: np.ndarray,\n",
    "                        n: int,\n",
    "                        distance_metric: str = 'euclidean') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Finds the n closest orbits in orbit_data to the single_orbit based on the specified distance metric.\n",
    "    \n",
    "    :param single_orbit: The reference orbit (shape: [n_features, n_time_steps]).\n",
    "    :param orbit_data: The dataset of orbits (shape: [n_samples, n_features, n_time_steps]).\n",
    "    :param n: The number of closest orbits to return.\n",
    "    :param distance_metric: The distance metric to use ('euclidean', 'manhattan', 'cosine', 'dtw').\n",
    "                            Defaults to 'euclidean'.\n",
    "    :return: Indices of the n closest orbits in orbit_data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stack the single_orbit with the orbit_data to form a new dataset\n",
    "    # This allows us to compute the distance matrix including the single_orbit\n",
    "    combined_data = np.vstack([orbit_data, single_orbit[np.newaxis, ...]])\n",
    "    \n",
    "    # Select the appropriate distance function\n",
    "    if distance_metric == 'euclidean':\n",
    "        distance_matrix = euclidean_distance(combined_data)\n",
    "    elif distance_metric == 'manhattan':\n",
    "        distance_matrix = manhattan_distance(combined_data)\n",
    "    elif distance_metric == 'cosine':\n",
    "        distance_matrix = cosine_distance(combined_data)\n",
    "    elif distance_metric == 'dtw':\n",
    "        distance_matrix = dtw_distance(combined_data)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "    \n",
    "    # The single_orbit is the last entry in combined_data\n",
    "    # Extract distances from the last row (or column) excluding the distance to itself\n",
    "    distances = distance_matrix[-1, :-1]\n",
    "    \n",
    "    # Get the indices of the n smallest distances\n",
    "    nearest_indices = np.argsort(distances)[:n]\n",
    "    \n",
    "    return nearest_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_distance_metrics_and_clustering(orbit_data: np.ndarray,\n",
    "                                    true_labels: np.ndarray,\n",
    "                                    distance_metrics: list = None,\n",
    "                                    clustering_algorithms: list = None,\n",
    "                                    evaluation_metrics: list = None,\n",
    "                                    n_clusters: int = None,\n",
    "                                    plot_results: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluates specified distance metrics and clustering algorithms on the given orbit data.\n",
    "    \n",
    "    :param orbit_data: The orbit data as a multivariate time series (shape: [n_samples, n_features, n_time_steps]).\n",
    "    :param true_labels: Array of true labels for the orbit data.\n",
    "    :param distance_metrics: List of strings specifying distance metrics to use. If None, all available metrics are used.\n",
    "    :param clustering_algorithms: List of strings specifying clustering algorithms to use. If None, all available algorithms are used.\n",
    "    :param evaluation_metrics: List of strings specifying evaluation metrics to use. If None, all available metrics are used.\n",
    "    :param n_clusters: Number of clusters for algorithms that require it. If None, it will be inferred from labels.\n",
    "    :param plot_results: If True, plot heatmaps of the results.\n",
    "    :return: A dictionary containing results for all combinations of metrics and clustering algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_clustering_accuracy(true_labels, pred_labels):\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "        return cm[row_ind, col_ind].sum() / np.sum(cm)\n",
    "\n",
    "    # Define available distance metrics\n",
    "    available_distance_metrics = {\n",
    "        'euclidean': euclidean_distance,\n",
    "        'manhattan': manhattan_distance,\n",
    "        'cosine': cosine_distance,\n",
    "        'dtw': dtw_distance\n",
    "    }\n",
    "    \n",
    "    # Define available clustering algorithms\n",
    "    available_clustering_algorithms = {\n",
    "        'kmeans': KMeans,\n",
    "        'gmm': GaussianMixture,\n",
    "        'dbscan': DBSCAN\n",
    "    }\n",
    "    \n",
    "    # Define available evaluation metrics\n",
    "    available_evaluation_metrics = {\n",
    "        'ARI': adjusted_rand_score,\n",
    "        'NMI': normalized_mutual_info_score,\n",
    "        'Homogeneity': homogeneity_score,\n",
    "        'Completeness': completeness_score,\n",
    "        'V-Measure': v_measure_score,\n",
    "        'Purity': lambda true, pred: np.sum(np.amax(confusion_matrix(true, pred), axis=0)) / np.sum(confusion_matrix(true, pred)),\n",
    "        'Accuracy': calculate_clustering_accuracy\n",
    "    }\n",
    "    \n",
    "    # If no metrics specified, use all available\n",
    "    if distance_metrics is None:\n",
    "        distance_metrics = list(available_distance_metrics.keys())\n",
    "    \n",
    "    # If no algorithms specified, use all available\n",
    "    if clustering_algorithms is None:\n",
    "        clustering_algorithms = list(available_clustering_algorithms.keys())\n",
    "    \n",
    "    # If no evaluation metrics specified, use all available\n",
    "    if evaluation_metrics is None:\n",
    "        evaluation_metrics = list(available_evaluation_metrics.keys())\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # If n_clusters is not provided, infer it from the labels\n",
    "    if n_clusters is None:\n",
    "        n_clusters = len(np.unique(true_labels))\n",
    "    \n",
    "    for distance_metric in distance_metrics:\n",
    "        if distance_metric not in available_distance_metrics:\n",
    "            print(f\"Warning: {distance_metric} is not an available distance metric. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Computing {distance_metric} distances...\")\n",
    "        metric_func = available_distance_metrics[distance_metric]\n",
    "        distance_matrix = metric_func(orbit_data)\n",
    "\n",
    "        if np.any(distance_matrix < 0):\n",
    "            distance_matrix = distance_matrix - np.min(distance_matrix)\n",
    "            np.fill_diagonal(distance_matrix, 0)\n",
    "        \n",
    "        for algo_name in clustering_algorithms:\n",
    "            if algo_name not in available_clustering_algorithms:\n",
    "                print(f\"Warning: {algo_name} is not an available clustering algorithm. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Clustering with {algo_name}...\")\n",
    "            algo_class = available_clustering_algorithms[algo_name]\n",
    "            \n",
    "            if algo_name == 'dbscan':\n",
    "                # For DBSCAN, we need to estimate eps\n",
    "                distances = squareform(distance_matrix)\n",
    "                eps = np.percentile(distances, 10)  # Use the 10th percentile of distances as eps\n",
    "                clusterer = algo_class(eps=eps, min_samples=5, metric='precomputed')\n",
    "                labels = clusterer.fit_predict(distance_matrix)\n",
    "            elif algo_name == 'kmeans':\n",
    "                clusterer = algo_class(n_clusters=n_clusters, random_state=42)\n",
    "                labels = clusterer.fit_predict(distance_matrix)\n",
    "            elif algo_name == 'gmm':\n",
    "                clusterer = algo_class(n_components=n_clusters, random_state=42)\n",
    "                labels = clusterer.fit_predict(distance_matrix)\n",
    "            else:\n",
    "                clusterer = algo_class()  # Use default parameters for other algorithms\n",
    "                labels = clusterer.fit_predict(distance_matrix)\n",
    "            \n",
    "            # Evaluate clustering\n",
    "            eval_results = {}\n",
    "            for eval_metric in evaluation_metrics:\n",
    "                if eval_metric not in available_evaluation_metrics:\n",
    "                    print(f\"Warning: {eval_metric} is not an available evaluation metric. Skipping.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    eval_results[eval_metric] = available_evaluation_metrics[eval_metric](true_labels, labels)\n",
    "            \n",
    "            # Store results\n",
    "            results[f\"{distance_metric}_{algo_name}\"] = eval_results\n",
    "    \n",
    "    if plot_results:\n",
    "        plot_metric_heatmaps(results, distance_metrics, clustering_algorithms, evaluation_metrics)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
