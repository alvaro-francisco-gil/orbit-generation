{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "\n",
    "> Scripts to get architectures for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean: Tensor, z_log_var: Tensor) -> Tensor:\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAELossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_total_losses = []\n",
    "        self.train_recon_losses = []\n",
    "        self.train_kl_losses = []\n",
    "        self.val_total_losses = []\n",
    "        self.val_recon_losses = []\n",
    "        self.val_kl_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        train_total_loss = trainer.callback_metrics.get('train_total_loss')\n",
    "        train_recon_loss = trainer.callback_metrics.get('train_reconstruction_loss')\n",
    "        train_kl_loss = trainer.callback_metrics.get('train_kl_loss')\n",
    "\n",
    "        if train_total_loss is not None:\n",
    "            self.train_total_losses.append(train_total_loss.item())\n",
    "        if train_recon_loss is not None:\n",
    "            self.train_recon_losses.append(train_recon_loss.item())\n",
    "        if train_kl_loss is not None:\n",
    "            self.train_kl_losses.append(train_kl_loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_total_loss = trainer.callback_metrics.get('val_total_loss')\n",
    "        val_recon_loss = trainer.callback_metrics.get('val_reconstruction_loss')\n",
    "        val_kl_loss = trainer.callback_metrics.get('val_kl_loss')\n",
    "\n",
    "        if val_total_loss is not None:\n",
    "            self.val_total_losses.append(val_total_loss.item())\n",
    "        if val_recon_loss is not None:\n",
    "            self.val_recon_losses.append(val_recon_loss.item())\n",
    "        if val_kl_loss is not None:\n",
    "            self.val_kl_losses.append(val_kl_loss.item())\n",
    "\n",
    "    def plot_total_losses(self, save_path=None):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_total_losses, label='Training Total Loss')\n",
    "        plt.plot(self.val_total_losses, label='Validation Total Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.title('Total Training and Validation Losses')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all_losses(self, save_path=None):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(self.train_total_losses, label='Training Total Loss')\n",
    "        plt.plot(self.val_total_losses, label='Validation Total Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.title('Total Training and Validation Losses')\n",
    "        \n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(self.train_recon_losses, label='Training Reconstruction Loss')\n",
    "        plt.plot(self.val_recon_losses, label='Validation Reconstruction Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Reconstruction Loss')\n",
    "        \n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(self.train_kl_losses, label='Training KL Divergence Loss')\n",
    "        plt.plot(self.val_kl_losses, label='Validation KL Divergence Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KL Divergence Loss')\n",
    "        \n",
    "        for i in range(1, 4):\n",
    "            plt.subplot(3, 1, i)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAEs Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEEncoder(ABC, nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input tensor into mean and log variance tensors.\n",
    "\n",
    "        :param x: Input tensor of shape (batch_size, feat_dim, seq_len)\n",
    "        :return: Tuple containing mean and log variance tensors.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEDecoder(ABC, nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decodes the latent tensor back to the original data space.\n",
    "\n",
    "        :param z: Latent tensor of shape (batch_size, latent_dim)\n",
    "        :return: Reconstructed tensor of shape (batch_size, feat_dim, seq_len)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Convolutional Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Sizes: 5, 7, 9, 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5Encoder(VAEEncoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super().__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.convo_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.feat_dim, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=7, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=9, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=13, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        x = self.convo_layers(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5Decoder(VAEDecoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super().__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64 * self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, self.seq_len))\n",
    "        )\n",
    "\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=13, stride=1, padding=6),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=self.feat_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        z = self.dense_layers(z)\n",
    "        z = self.conv_transpose_layers(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_conv5_vae_components(seq_len: int,  # Length of input sequence\n",
    "                           feat_dim: int,  # Dimensionality of input features \n",
    "                           latent_dim: int,  # Dimensionality of the latent space\n",
    "                           dropout_rate: float = 0.2,  # Dropout rate for regularization\n",
    "                           ) -> tuple[Conv5Encoder, Conv5Decoder]:\n",
    "    \"\"\"\n",
    "    Creates and returns encoder and decoder components for a convolutional VAE architecture.\n",
    "    \"\"\"\n",
    "    encoder = Conv5Encoder(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    decoder = Conv5Decoder(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legit Tsgm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5EncoderLegitTsgm(VAEEncoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super().__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.convo_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.feat_dim, out_channels=64, kernel_size=10, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=4, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)  # Output z_mean and z_log_var\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        x = self.convo_layers(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5DecoderLegitTsgm(VAEDecoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super().__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Dense layers to upscale the latent dimensions\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64 * seq_len),  # Adjust output size based on sequence length\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, seq_len))\n",
    "        )\n",
    "\n",
    "        # Transpose Convolutional layers with calculated padding for \"same\" effect\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=self.feat_dim, kernel_size=11, stride=1, padding=5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        z = self.dense_layers(z)\n",
    "        z = self.conv_transpose_layers(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_conv5_legit_tsgm_vae_components(seq_len: int,  # Length of input sequence\n",
    "                                       feat_dim: int,  # Dimensionality of input features\n",
    "                                       latent_dim: int,  # Dimensionality of the latent space\n",
    "                                       dropout_rate: float = 0.2,  # Dropout rate for regularization\n",
    "                                       ) -> tuple[Conv5EncoderLegitTsgm, Conv5DecoderLegitTsgm]:\n",
    "    \"\"\"\n",
    "    Creates and returns encoder and decoder components for a Conv5 VAE model.\n",
    "    \"\"\"\n",
    "    encoder = Conv5EncoderLegitTsgm(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    decoder = Conv5DecoderLegitTsgm(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemetation in the following cell is taken from\n",
    "https://github.com/TheMrGhostman/InceptionTime-Pytorch/blob/master/inception.py , the next cell is an adjustment for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def correct_sizes(sizes):\n",
    "\tcorrected_sizes = [s if s % 2 != 0 else s - 1 for s in sizes]\n",
    "\treturn corrected_sizes\n",
    "\n",
    "\n",
    "def pass_through(X):\n",
    "\treturn X\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU(), return_indices=False):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t: param return_indices\t\t\tIndices are needed only if we want to create decoder with InceptionTranspose with MaxUnpool1d. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Inception, self).__init__()\n",
    "\t\tself.return_indices=return_indices\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_pool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1, return_indices=return_indices)\n",
    "\t\tself.conv_from_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=4*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ_maxpool, indices = self.max_pool(X)\n",
    "\t\telse:\n",
    "\t\t\tZ_maxpool = self.max_pool(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\tZ4 = self.conv_from_maxpool(Z_maxpool)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3, Z4], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z, indices\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "\t\tsuper(InceptionBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.return_indices = return_indices\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=4*n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=4*n_filters\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ, i1 = self.inception_1(X)\n",
    "\t\t\tZ, i2 = self.inception_2(Z)\n",
    "\t\t\tZ, i3 = self.inception_3(Z)\n",
    "\t\telse:\n",
    "\t\t\tZ = self.inception_1(X)\n",
    "\t\t\tZ = self.inception_2(Z)\n",
    "\t\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z,[i1, i2, i3]\n",
    "\t\telse:\n",
    "\t\t\treturn Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionWithoutPool(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU()):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(InceptionWithoutPool, self).__init__()\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=3*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "\t\treturn Z\n",
    "\n",
    "\n",
    "class InceptionBlockWithoutPool(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "        super(InceptionBlockWithoutPool, self).__init__()\n",
    "        self.use_residual = use_residual\n",
    "        self.activation = activation\n",
    "        self.inception_1 = InceptionWithoutPool(\n",
    "            in_channels=in_channels,\n",
    "            n_filters=n_filters,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            bottleneck_channels=bottleneck_channels,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.inception_2 = InceptionWithoutPool(\n",
    "            in_channels=3 * n_filters,\n",
    "            n_filters=n_filters,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            bottleneck_channels=bottleneck_channels,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.inception_3 = InceptionWithoutPool(\n",
    "            in_channels=3 * n_filters,\n",
    "            n_filters=n_filters,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            bottleneck_channels=bottleneck_channels,\n",
    "            activation=activation\n",
    "        )   \n",
    "        if self.use_residual:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=in_channels, \n",
    "                    out_channels=3 * n_filters, \n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0\n",
    "                ),\n",
    "                nn.BatchNorm1d(\n",
    "                    num_features=3 * n_filters\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.inception_1(X)\n",
    "        Z = self.inception_2(Z)\n",
    "        Z = self.inception_3(Z)\n",
    "\n",
    "        if self.use_residual:\n",
    "            Z = Z + self.residual(X)\n",
    "            Z = self.activation(Z)\n",
    "        return Z\n",
    "\n",
    "\n",
    "class InceptionTransposeWithoutPool(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU()):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(InceptionTransposeWithoutPool, self).__init__()\n",
    "\t\tself.activation = activation\n",
    "\t\tself.conv_to_bottleneck_1 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_2 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_3 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=3*bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ1 = self.conv_to_bottleneck_1(X)\n",
    "\t\tZ2 = self.conv_to_bottleneck_2(X)\n",
    "\t\tZ3 = self.conv_to_bottleneck_3(X)\n",
    "\n",
    "\t\tZ = torch.cat([Z1, Z2, Z3], axis=1)\n",
    "\t\tBN = self.bottleneck(Z)\n",
    "\t\t\n",
    "\t\treturn self.activation(self.batch_norm(BN))\n",
    "\n",
    "\n",
    "class InceptionTransposeBlockWithoutPool(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU()):\n",
    "\t\tsuper(InceptionTransposeBlockWithoutPool, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = InceptionTransposeWithoutPool(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = InceptionTransposeWithoutPool(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = InceptionTransposeWithoutPool(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=out_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=out_channels\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.inception_1(X)\n",
    "\t\tZ = self.inception_2(Z)\n",
    "\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\treturn Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionTimeVAEEncoder(VAEEncoder):\n",
    "    def __init__(self, feat_dim=7, seq_len=100, n_filters=32, kernel_sizes=[5, 11, 23],\n",
    "                 bottleneck_channels=32, latent_dim=2):\n",
    "        super(InceptionTimeVAEEncoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.inception_blocks = nn.ModuleList([\n",
    "            InceptionBlock(\n",
    "                in_channels=feat_dim,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU(),\n",
    "                return_indices=True\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU(),\n",
    "                return_indices=True\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU(),\n",
    "                return_indices=True\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=4 * n_filters * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, list]:\n",
    "        indices_list = []\n",
    "        for block in self.inception_blocks:\n",
    "            x, indices = block(x)\n",
    "            indices_list.append(indices)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        \n",
    "        return z_mean, z_log_var, indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WPInceptionTimeVAEEncoder(VAEEncoder):\n",
    "    def __init__(self, feat_dim=7, seq_len=100, n_filters=32, kernel_sizes=[5, 11, 23],\n",
    "                 bottleneck_channels=32, latent_dim=2):\n",
    "        super(WPInceptionTimeVAEEncoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.inception_blocks = nn.ModuleList([\n",
    "            InceptionBlockWithoutPool(\n",
    "                in_channels=feat_dim,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU(),\n",
    "                return_indices=True\n",
    "            ),\n",
    "            InceptionBlockWithoutPool(\n",
    "                in_channels=3 * n_filters,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU(),\n",
    "                return_indices=True\n",
    "            ),\n",
    "            InceptionBlockWithoutPool(\n",
    "                in_channels=3 * n_filters,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU(),\n",
    "                return_indices=True\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=3 * n_filters * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, list]:\n",
    "        for block in self.inception_blocks:\n",
    "            x = block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        \n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WPInceptionTimeVAEDecoder(VAEDecoder):\n",
    "    def __init__(self, feat_dim=7, seq_len=100, n_filters=32, kernel_sizes=[5, 11, 23],\n",
    "                 bottleneck_channels=32, latent_dim=2):\n",
    "        super(WPInceptionTimeVAEDecoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=3 * n_filters * self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(3 * n_filters, self.seq_len))\n",
    "        )\n",
    "\n",
    "        self.inception_transpose_blocks = nn.ModuleList([\n",
    "            InceptionTransposeBlockWithoutPool(\n",
    "                in_channels=3 * n_filters,\n",
    "                out_channels=3 * n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionTransposeBlockWithoutPool(\n",
    "                in_channels=3 * n_filters,\n",
    "                out_channels=3 * n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionTransposeBlockWithoutPool(\n",
    "                in_channels=3 * n_filters,\n",
    "                out_channels=feat_dim,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                activation=nn.ReLU()\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.dense_layers(z)\n",
    "        for block in self.inception_transpose_blocks:\n",
    "            z = block(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_inception_time_vae_components(seq_len: int,  # Length of input sequence\n",
    "                                    feat_dim: int,  # Dimensionality of input features \n",
    "                                    latent_dim: int,  # Dimensionality of the latent space\n",
    "                                    without_pooling: bool = True,  # If True, returns WPInceptionTimeVAEEncoder instead of InceptionTimeVAEEncoder\n",
    "                                    **model_kwargs: dict  # Dictionary containing model-specific keyword arguments\n",
    "                                    ) -> tuple[Union[InceptionTimeVAEEncoder, WPInceptionTimeVAEEncoder], WPInceptionTimeVAEDecoder]:\n",
    "    \"\"\"\n",
    "    Returns encoder and decoder components for an InceptionTime-based VAE architecture.\n",
    "    \"\"\"\n",
    "    if without_pooling:\n",
    "        encoder = WPInceptionTimeVAEEncoder(\n",
    "            feat_dim=feat_dim,\n",
    "            seq_len=seq_len,\n",
    "            n_filters=model_kwargs.get('n_filters', 32),\n",
    "            kernel_sizes=model_kwargs.get('kernel_sizes', [5, 11, 23]),\n",
    "            bottleneck_channels=model_kwargs.get('bottleneck_channels', 32),\n",
    "            latent_dim=latent_dim\n",
    "        )\n",
    "    else:\n",
    "        encoder = InceptionTimeVAEEncoder(\n",
    "            feat_dim=feat_dim,\n",
    "            seq_len=seq_len,\n",
    "            n_filters=model_kwargs.get('n_filters', 32),\n",
    "            kernel_sizes=model_kwargs.get('kernel_sizes', [5, 11, 23]),\n",
    "            bottleneck_channels=model_kwargs.get('bottleneck_channels', 32),\n",
    "            latent_dim=latent_dim\n",
    "        )\n",
    "    \n",
    "    decoder = WPInceptionTimeVAEDecoder(\n",
    "        feat_dim=feat_dim,\n",
    "        seq_len=seq_len,\n",
    "        n_filters=model_kwargs.get('n_filters', 32),\n",
    "        kernel_sizes=model_kwargs.get('kernel_sizes', [5, 11, 23]),\n",
    "        bottleneck_channels=model_kwargs.get('bottleneck_channels', 32),\n",
    "        latent_dim=latent_dim\n",
    "    )\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cVAEs Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class cVAEEncoder(ABC, nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract base for a conditional VAE encoder:\n",
    "    Encodes data + condition into z_mean, z_log_var\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, x: Tensor, cond: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input tensor + condition into mean and log variance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: Tensor, cond: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        return self.encode(x, cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class cVAEDecoder(ABC, nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract base for a conditional VAE decoder:\n",
    "    Decodes z + condition into reconstructed data\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, z: Tensor, cond: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Decodes the latent tensor + condition back to the original data space.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, z: Tensor, cond: Tensor) -> Tensor:\n",
    "        return self.decode(z, cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class cConv5EncoderLegitTsgm(cVAEEncoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, cond_dim: int, dropout_rate: float):\n",
    "        super().__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        self.convo_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.feat_dim + self.cond_dim, out_channels=64, kernel_size=10, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=4, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)  # Output z_mean and z_log_var\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor, cond: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        # Concatenate condition along feature dimension\n",
    "        # cond shape: (batch_size, cond_dim). Expand to match (batch_size, cond_dim, seq_len)\n",
    "        # If cond is shape (N,), do this first:\n",
    "        if cond.ndim == 1:\n",
    "            cond = cond.unsqueeze(1)  # now shape is [N, 1]\n",
    "\n",
    "        cond = cond.unsqueeze(2).repeat(1, 1, x.shape[2])\n",
    "        x = torch.cat([x, cond], dim=1)  # (batch_size, feat_dim + cond_dim, seq_len)\n",
    "        \n",
    "        x = self.convo_layers(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class cConv5DecoderLegitTsgm(cVAEDecoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, cond_dim: int, dropout_rate: float):\n",
    "        super().__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.cond_dim = cond_dim \n",
    "        \n",
    "        # Dense layers to upscale the latent dimensions\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim + self.cond_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64 * seq_len),  # Adjust output size based on sequence length\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, seq_len))\n",
    "        )\n",
    "\n",
    "        # Transpose Convolutional layers with \"same\" padding\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=self.feat_dim, kernel_size=11, stride=1, padding=5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def decode(self, z: Tensor, cond: Tensor) -> Tensor:\n",
    "        # Concatenate condition with latent vector\n",
    "        # cond shape: (batch_size, cond_dim)\n",
    "        z = torch.cat([z, cond], dim=-1)  # (batch_size, latent_dim + cond_dim)\n",
    "        \n",
    "        z = self.dense_layers(z)\n",
    "        z = self.conv_transpose_layers(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_conditional_conv5_legit_tsgm_vae_components(\n",
    "    seq_len: int,  # Length of input sequence\n",
    "    feat_dim: int,  # Dimensionality of input features \n",
    "    latent_dim: int,  # Dimensionality of the latent space\n",
    "    dropout_rate: float = 0.2,  # Dropout rate for regularization\n",
    "    cond_dim: int = 1,  # Dimensionality of conditional input\n",
    ") -> tuple[cConv5EncoderLegitTsgm, cConv5DecoderLegitTsgm]:\n",
    "    \"\"\"\n",
    "    Creates encoder and decoder components for a conditional convolutional VAE.\n",
    "    \"\"\"\n",
    "    encoder = cConv5EncoderLegitTsgm(seq_len, feat_dim, latent_dim, cond_dim, dropout_rate)\n",
    "    decoder = cConv5DecoderLegitTsgm(seq_len, feat_dim, latent_dim, cond_dim, dropout_rate)\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
