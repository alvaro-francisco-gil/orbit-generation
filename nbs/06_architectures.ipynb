{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "\n",
    "> Scripts to get architectures for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sampling(nn.Module):\n",
    "    def forward(self, z_mean: Tensor, z_log_var: Tensor) -> Tensor:\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAELossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_total_losses = []\n",
    "        self.train_recon_losses = []\n",
    "        self.train_kl_losses = []\n",
    "        self.val_total_losses = []\n",
    "        self.val_recon_losses = []\n",
    "        self.val_kl_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        train_total_loss = trainer.callback_metrics.get('train_total_loss')\n",
    "        train_recon_loss = trainer.callback_metrics.get('train_reconstruction_loss')\n",
    "        train_kl_loss = trainer.callback_metrics.get('train_kl_loss')\n",
    "\n",
    "        if train_total_loss is not None:\n",
    "            self.train_total_losses.append(train_total_loss.item())\n",
    "        if train_recon_loss is not None:\n",
    "            self.train_recon_losses.append(train_recon_loss.item())\n",
    "        if train_kl_loss is not None:\n",
    "            self.train_kl_losses.append(train_kl_loss.item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_total_loss = trainer.callback_metrics.get('val_total_loss')\n",
    "        val_recon_loss = trainer.callback_metrics.get('val_reconstruction_loss')\n",
    "        val_kl_loss = trainer.callback_metrics.get('val_kl_loss')\n",
    "\n",
    "        if val_total_loss is not None:\n",
    "            self.val_total_losses.append(val_total_loss.item())\n",
    "        if val_recon_loss is not None:\n",
    "            self.val_recon_losses.append(val_recon_loss.item())\n",
    "        if val_kl_loss is not None:\n",
    "            self.val_kl_losses.append(val_kl_loss.item())\n",
    "\n",
    "    def plot_total_losses(self, save_path=None):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_total_losses, label='Training Total Loss')\n",
    "        plt.plot(self.val_total_losses, label='Validation Total Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.title('Total Training and Validation Losses')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all_losses(self, save_path=None):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(self.train_total_losses, label='Training Total Loss')\n",
    "        plt.plot(self.val_total_losses, label='Validation Total Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.title('Total Training and Validation Losses')\n",
    "        \n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(self.train_recon_losses, label='Training Reconstruction Loss')\n",
    "        plt.plot(self.val_recon_losses, label='Validation Reconstruction Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Reconstruction Loss')\n",
    "        \n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(self.train_kl_losses, label='Training KL Divergence Loss')\n",
    "        plt.plot(self.val_kl_losses, label='Validation KL Divergence Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('KL Divergence Loss')\n",
    "        \n",
    "        for i in range(1, 4):\n",
    "            plt.subplot(3, 1, i)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAEs Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEEncoder(ABC, nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(VAEEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input tensor into mean and log variance tensors.\n",
    "\n",
    "        :param x: Input tensor of shape (batch_size, feat_dim, seq_len)\n",
    "        :return: Tuple containing mean and log variance tensors.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEDecoder(ABC, nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(VAEDecoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decodes the latent tensor back to the original data space.\n",
    "\n",
    "        :param z: Latent tensor of shape (batch_size, latent_dim)\n",
    "        :return: Reconstructed tensor of shape (batch_size, feat_dim, seq_len)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Convolutional Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Sizes: 5, 7, 9, 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5Encoder(VAEEncoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super(VAEEncoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.convo_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.feat_dim, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=7, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=9, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=13, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        x = self.convo_layers(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5Decoder(VAEDecoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super(VAEDecoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64 * self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, self.seq_len))\n",
    "        )\n",
    "\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=13, stride=1, padding=6),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=self.feat_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z: Tensor) -> Tensor:\n",
    "        z = self.dense_layers(z)\n",
    "        z = self.conv_transpose_layers(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_conv5_vae_components(seq_len, feat_dim, latent_dim, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Returns an instance of Conv5Encoder and Conv5Decoder based on the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of input sequence.\n",
    "        feat_dim (int): Dimensionality of input features.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "        dropout_rate (float): Dropout rate to use in the model.\n",
    "    \n",
    "    Returns:\n",
    "        encoder (Conv5Encoder): The encoder part of the VAE.\n",
    "        decoder (Conv5Decoder): The decoder part of the VAE.\n",
    "    \"\"\"\n",
    "    encoder = Conv5Encoder(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    decoder = Conv5Decoder(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legit Tsgm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5EncoderLegitTsgm(VAEEncoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super(VAEEncoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.convo_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.feat_dim, out_channels=64, kernel_size=10, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=4, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)  # Output z_mean and z_log_var\n",
    "        )\n",
    "\n",
    "    def encode(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        x = self.convo_layers(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Conv5DecoderLegitTsgm(VAEDecoder):\n",
    "    def __init__(self, seq_len: int, feat_dim: int, latent_dim: int, dropout_rate: float):\n",
    "        super(VAEDecoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Dense layers to upscale the latent dimensions\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64 * seq_len),  # Adjust output size based on sequence length\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, seq_len))\n",
    "        )\n",
    "\n",
    "        # Transpose Convolutional layers with calculated padding for \"same\" effect\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=self.feat_dim, kernel_size=11, stride=1, padding=5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        z = self.dense_layers(z)\n",
    "        z = self.conv_transpose_layers(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_conv5_legit_tsgm_vae_components(seq_len, feat_dim, latent_dim, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Returns an instance of Conv5Encoder and Conv5Decoder based on the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of input sequence.\n",
    "        feat_dim (int): Dimensionality of input features.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "        dropout_rate (float): Dropout rate to use in the model.\n",
    "    \n",
    "    Returns:\n",
    "        encoder (Conv5Encoder): The encoder part of the VAE.\n",
    "        decoder (Conv5Decoder): The decoder part of the VAE.\n",
    "    \"\"\"\n",
    "    encoder = Conv5EncoderLegitTsgm(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    decoder = Conv5DecoderLegitTsgm(seq_len, feat_dim, latent_dim, dropout_rate)\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemetation in the following cell is taken from\n",
    "https://github.com/TheMrGhostman/InceptionTime-Pytorch/blob/master/inception.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def correct_sizes(sizes):\n",
    "\tcorrected_sizes = [s if s % 2 != 0 else s - 1 for s in sizes]\n",
    "\treturn corrected_sizes\n",
    "\n",
    "\n",
    "def pass_through(X):\n",
    "\treturn X\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU(), return_indices=False):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t: param return_indices\t\t\tIndices are needed only if we want to create decoder with InceptionTranspose with MaxUnpool1d. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Inception, self).__init__()\n",
    "\t\tself.return_indices=return_indices\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_pool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1, return_indices=return_indices)\n",
    "\t\tself.conv_from_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=4*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ_maxpool, indices = self.max_pool(X)\n",
    "\t\telse:\n",
    "\t\t\tZ_maxpool = self.max_pool(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\tZ4 = self.conv_from_maxpool(Z_maxpool)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3, Z4], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z, indices\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "\t\tsuper(InceptionBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.return_indices = return_indices\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=4*n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=4*n_filters\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ, i1 = self.inception_1(X)\n",
    "\t\t\tZ, i2 = self.inception_2(Z)\n",
    "\t\t\tZ, i3 = self.inception_3(Z)\n",
    "\t\telse:\n",
    "\t\t\tZ = self.inception_1(X)\n",
    "\t\t\tZ = self.inception_2(Z)\n",
    "\t\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z,[i1, i2, i3]\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "\n",
    "\n",
    "class InceptionTranspose(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU()):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(InceptionTranspose, self).__init__()\n",
    "\t\tself.activation = activation\n",
    "\t\tself.conv_to_bottleneck_1 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_2 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_bottleneck_3 = nn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_to_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_unpool = nn.MaxUnpool1d(kernel_size=3, stride=1, padding=1)\n",
    "\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=3*bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=out_channels)\n",
    "\n",
    "\t\tdef forward(self, X, indices):\n",
    "\t\t\tZ1 = self.conv_to_bottleneck_1(X)\n",
    "\t\t\tZ2 = self.conv_to_bottleneck_2(X)\n",
    "\t\t\tZ3 = self.conv_to_bottleneck_3(X)\n",
    "\t\t\tZ4 = self.conv_to_maxpool(X)\n",
    "\n",
    "\t\t\tZ = torch.cat([Z1, Z2, Z3], axis=1)\n",
    "\t\t\tMUP = self.max_unpool(Z4, indices)\n",
    "\t\t\tBN = self.bottleneck(Z)\n",
    "\t\t\t# another possibility insted of sum BN and MUP is adding 2nd bottleneck transposed convolution\n",
    "\t\t\t\n",
    "\t\t\treturn self.activation(self.batch_norm(BN + MUP))\n",
    "\n",
    "\n",
    "class InceptionTransposeBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, out_channels=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU()):\n",
    "\t\tsuper(InceptionTransposeBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=in_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = InceptionTranspose(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tout_channels=out_channels,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.ConvTranspose1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=out_channels, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=out_channels\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X, indices):\n",
    "\t\tassert len(indices)==3\n",
    "\t\tZ = self.inception_1(X, indices[2])\n",
    "\t\tZ = self.inception_2(Z, indices[1])\n",
    "\t\tZ = self.inception_3(Z, indices[0])\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\treturn Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionTimeVAEEncoder(VAEEncoder):\n",
    "    def __init__(self, feat_dim=7, seq_len=100, n_filters=32, kernel_sizes=[5, 11, 23],\n",
    "                 bottleneck_channels=32, latent_dim=2):\n",
    "        super(InceptionTimeVAEEncoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.inception_blocks = nn.Sequential(\n",
    "            InceptionBlock(\n",
    "                in_channels=feat_dim,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                n_filters=n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=4 * n_filters * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.inception_blocks(x)\n",
    "        x = self.dense_layers(x)\n",
    "        z_mean, z_log_var = torch.split(x, self.latent_dim, dim=1)\n",
    "        return z_mean, z_log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class InceptionTimeVAEDecoder(VAEDecoder):\n",
    "    def __init__(self, feat_dim=7, seq_len=100, n_filters=32, kernel_sizes=[5, 11, 23],\n",
    "                 bottleneck_channels=32, latent_dim=2):\n",
    "        super(InceptionTimeVAEDecoder, self).__init__(latent_dim=latent_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=4 * n_filters * self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, self.seq_len))\n",
    "        )\n",
    "\n",
    "        self.inception_transpose_blocks = nn.Sequential(\n",
    "            InceptionTransposeBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                out_channels=4 * n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionTransposeBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                out_channels=4 * n_filters,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionTransposeBlock(\n",
    "                in_channels=4 * n_filters,\n",
    "                out_channels=feat_dim,\n",
    "                kernel_sizes=kernel_sizes,\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            nn.ConvTranspose1d(\n",
    "                in_channels=feat_dim,\n",
    "                out_channels=feat_dim,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.dense_layers(z)\n",
    "        z = self.inception_transpose_blocks(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_inception_time_vae_components(seq_len, feat_dim, latent_dim, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Returns an instance of InceptionTimeVAEEncoder and InceptionTimeVAEDecoder based on the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): Length of input sequence.\n",
    "        feat_dim (int): Dimensionality of input features.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "        dropout_rate (float): Dropout rate to use in the model.\n",
    "    \n",
    "    Returns:\n",
    "        encoder (InceptionTimeVAEEncoder): The encoder part of the VAE.\n",
    "        decoder (InceptionTimeVAEDecoder): The decoder part of the VAE.\n",
    "    \"\"\"\n",
    "    encoder = InceptionTimeVAEEncoder(\n",
    "        feat_dim=feat_dim,\n",
    "        seq_len=seq_len,\n",
    "        n_filters=32,  # Adjust as needed\n",
    "        kernel_sizes=[5, 11, 23],  # Adjust as needed\n",
    "        bottleneck_channels=32,  # Adjust as needed\n",
    "        latent_dim=latent_dim,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    decoder = InceptionTimeVAEDecoder(\n",
    "        feat_dim=feat_dim,\n",
    "        seq_len=seq_len,\n",
    "        n_filters=32,  # Adjust as needed\n",
    "        kernel_sizes=[5, 11, 23],  # Adjust as needed\n",
    "        bottleneck_channels=32,  # Adjust as needed\n",
    "        latent_dim=latent_dim,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
