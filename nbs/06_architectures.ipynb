{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "\n",
    "> Scripts to get architectures for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sampling(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch module to perform the reparameterization trick for VAEs.\n",
    "    \"\"\"\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Architecture(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all architectures. Requires subclasses to define a type.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def arch_type(self):\n",
    "        \"\"\"\n",
    "        Returns the architectural type of the model.\n",
    "        Subclasses must provide this property.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseVAEArchitecture(Architecture):\n",
    "    \"\"\"\n",
    "    Abstract base class for Variational Autoencoder architectures.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._encoder = None\n",
    "        self._decoder = None\n",
    "\n",
    "    @property\n",
    "    def encoder(self):\n",
    "        \"\"\"\n",
    "        Property for accessing the encoder model.\n",
    "        Raises NotImplementedError if the encoder model is not implemented.\n",
    "        \"\"\"\n",
    "        if self._encoder is None:\n",
    "            raise NotImplementedError(\"Encoder model has not been implemented.\")\n",
    "        return self._encoder\n",
    "\n",
    "    @property\n",
    "    def decoder(self):\n",
    "        \"\"\"\n",
    "        Property for accessing the decoder model.\n",
    "        Raises NotImplementedError if the decoder model is not implemented.\n",
    "        \"\"\"\n",
    "        if self._decoder is None:\n",
    "            raise NotImplementedError(\"Decoder model has not been implemented.\")\n",
    "        return self._decoder\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Retrieves both encoder and decoder models as a dictionary.\n",
    "        Raises NotImplementedError if either model is not implemented.\n",
    "        \"\"\"\n",
    "        if self._encoder is None or self._decoder is None:\n",
    "            raise NotImplementedError(\"Encoder or Decoder model has not been fully implemented.\")\n",
    "        return {\"encoder\": self._encoder, \"decoder\": self._decoder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE_CONV5Architecture(BaseVAEArchitecture):\n",
    "    \"\"\"\n",
    "    This class defines the architecture for a Variational Autoencoder (VAE) with Convolutional Layers.\n",
    "    Parameters:\n",
    "        seq_len (int): Length of input sequence.\n",
    "        feat_dim (int): Dimensionality of input features.\n",
    "        latent_dim (int): Dimensionality of latent space.\n",
    "        dropout_rate (float): Dropout rate to use in dropout layers.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def arch_type(self):\n",
    "        return \"vae_conv5\"\n",
    "\n",
    "    def __init__(self, seq_len, feat_dim, latent_dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self._encoder = self._build_encoder()\n",
    "        self._decoder = self._build_decoder()\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        # Group 1: Transpose Convolution Layers to reshape and refine the spatial structure\n",
    "        convo_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.feat_dim, out_channels=64, kernel_size=10, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=4, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Group 2: Dense Layers to downscale the feature space to a lower latent dimensions\n",
    "        dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * self.seq_len, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.latent_dim * 2)  # Output z_mean and z_log_var\n",
    "        )\n",
    "        return nn.Sequential(convo_layers, dense_layers, Sampling())\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        # Group 1: Dense Layers to upscale the latent dimensions to a higher feature space\n",
    "        dense_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=64 * self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, self.seq_len))\n",
    "        )\n",
    "\n",
    "        # Group 2: Transpose Convolution Layers to reshape and refine the spatial structure\n",
    "        transpose_conv_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=64, kernel_size=10, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=self.feat_dim, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Combine both groups into the final sequential model\n",
    "        return nn.Sequential(dense_layers, transpose_conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nbdev/export.py:80: UserWarning: Notebook '/orbit-generation/nbs/10_vae.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
